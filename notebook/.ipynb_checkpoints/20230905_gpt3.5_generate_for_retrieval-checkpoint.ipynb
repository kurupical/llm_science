{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../apikey/apikey.txt\", \"r\") as f:\n",
    "    openai.api_key = f.readline().replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_prompt(prompt, max_tokens=4000):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professor at a science university and creating a exam for your students.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(fname):\n",
    "    def f(categories):\n",
    "        for cat in categories:\n",
    "            for word in [\n",
    "                \"geology\",\n",
    "                \"physics\",\n",
    "                \"chemistry\",\n",
    "                \"mathematical\",\n",
    "                \"biology\",\n",
    "                \"astronomy\",\n",
    "                \"ecology\",\n",
    "                \"genetics\",\n",
    "                \"statistics\",\n",
    "                \"theoretical\"\n",
    "            ]:\n",
    "                if word.lower() in cat.lower():\n",
    "                    return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def text_preprocess(text):\n",
    "        return text.replace(\"===\", \"\\n\").replace(\"==\", \"\\n\")\n",
    "\n",
    "    df = pd.read_parquet(fname)\n",
    "    df_science = df[df[\"categories\"].apply(f)]\n",
    "    df_science[\"text\"] = \"title: \" + df_science[\"title\"] + \"\\n\" + df_science[\"text\"].apply(text_preprocess)\n",
    "    return df_science.sample(len(df_science)//5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../data/wikipedia/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:   0%| | 1/504 [00:04<38:44,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': 'Who became the editor-in-chief of the Annals of Mathematical Statistics in 1938?', 'A': 'Samuel Wilks', 'B': 'Fisher', 'C': 'Neyman', 'D': 'Cramér', 'E': 'Hotelling', 'answer': 'A', 'basis': 'According to the given text, Samuel Wilks became the editor-in-chief of the Annals of Mathematical Statistics in 1938.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:  32%|▎| 160/504 [14:30<28:0Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 17384 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/a.parquet:  32%|▎| 161/504 [14:30<20:4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 17384 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is arrow pushing used to describe in organic chemistry reaction mechanisms?\",\n",
      "    \"A\": \"The movement of atoms\",\n",
      "    \"B\": \"The movement of electron density\",\n",
      "    \"C\": \"The formation of new bonds\",\n",
      "    \"D\": \"The breaking of covalent bonds\",\n",
      "    \"E\": \"The distribution of positive and negative charges\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"Arrow pushing is used to show the movement of electron density, which indirectly shows the movement of atoms themselves. The text states, 'The arrows illustrate the movement of electrons as bonds between atoms are broken and formed.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:  38%|▍| 189/504 [16:55<25:2Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 10 column 122 (char 386)\n",
      "../data/wikipedia/a.parquet:  38%|▍| 190/504 [17:01<26:3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 10 column 122 (char 386)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the meaning of the name 'Adeopapposaurus'?\",\n",
      "    \"A\": \"Far eating lizard\",\n",
      "    \"B\": \"Long necked dinosaur\",\n",
      "    \"C\": \"Massospondylus relative\",\n",
      "    \"D\": \"Keratinous beak dinosaur\",\n",
      "    \"E\": \"Early Jurassic dinosaur\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The name 'Adeopapposaurus' means 'far eating lizard', as mentioned in the text: 'Adeopapposaurus (meaning \"far eating lizard\", in reference to its long neck)'.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:  38%|▍| 194/504 [17:27<36:0Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17894 tokens (13894 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/a.parquet:  39%|▍| 195/504 [17:27<26:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 17894 tokens (13894 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is Alexandre Kirillov known for?\",\n",
      "    \"A\": \"His works in the field of representation theory\",\n",
      "    \"B\": \"His works in the field of topological groups\",\n",
      "    \"C\": \"His works in the field of Lie groups\",\n",
      "    \"D\": \"Introducing the orbit method into representation theory\",\n",
      "    \"E\": \"All of the above\",\n",
      "    \"answer\": \"E\",\n",
      "    \"basis\": \"The text states that Alexandre Kirillov is known for his works in the fields of representation theory, topological groups, and Lie groups. It also mentions that he introduced the orbit method into representation theory.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:  41%|▍| 205/504 [18:19<26:0Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16998 tokens (12998 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/a.parquet:  41%|▍| 206/504 [18:21<20:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 16998 tokens (12998 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the scope of the journal Astrobiology?\",\n",
      "    \"A\": \"Research on the origin and evolution of life on Earth\",\n",
      "    \"B\": \"Research on the distribution and future of life across the universe\",\n",
      "    \"C\": \"Research on the impact of climate change on ecosystems\",\n",
      "    \"D\": \"Research on the history of space exploration\",\n",
      "    \"E\": \"Research on the effects of gravity on living organisms\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The text states that the scope of the journal Astrobiology includes research on the origin, evolution, distribution, and future of life across the universe.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:  41%|▍| 208/504 [18:29<20:2Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 6 column 29 (char 298)\n",
      "../data/wikipedia/a.parquet:  41%|▍| 209/504 [18:36<23:5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 6 column 29 (char 298)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the relationship between antarafacial and suprafacial processes in organic chemistry?\",\n",
      "    \"A\": \"They involve the same face of the π system or isolated orbital.\",\n",
      "    \"B\": \"They involve opposite faces of the π system or isolated orbital.\",\n",
      "    \"C\": \"They involve two \"interior\" lobes of a σ bond.\",\n",
      "    \"D\": \"They involve two \"exterior\" lobes of a σ bond.\",\n",
      "    \"E\": \"They involve one \"interior\" lobe and one \"exterior\" lobe of a σ bond.\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The text states that the relationship is antarafacial when opposite faces of the π system or isolated orbital are involved in the process, and suprafacial when the same face of the π system or isolated orbital are involved.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:  43%|▍| 215/504 [19:05<23:1Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 18194 tokens (14194 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/a.parquet:  43%|▍| 216/504 [19:05<16:4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 18194 tokens (14194 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What was Andrew Browder's profession?\",\n",
      "    \"A\": \"Chemist\",\n",
      "    \"B\": \"Biologist\",\n",
      "    \"C\": \"Mathematician\",\n",
      "    \"D\": \"Physicist\",\n",
      "    \"E\": \"Astronomer\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"Andrew Browder was an American mathematician at Brown University.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:  56%|▌| 280/504 [25:24<22:5Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 17503 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/a.parquet:  56%|▌| 281/504 [25:25<17:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 17503 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is autophosphorylation?\",\n",
      "    \"A\": \"The phosphorylation of a kinase by another kinase\",\n",
      "    \"B\": \"The phosphorylation of a kinase by itself\",\n",
      "    \"C\": \"The phosphorylation of a kinase by a phosphatase\",\n",
      "    \"D\": \"The phosphorylation of a kinase by a ligand\",\n",
      "    \"E\": \"The phosphorylation of a kinase by a substrate\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"Autophosphorylation is generally defined as the phosphorylation of the kinase by itself.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:  74%|▋| 374/504 [35:21<13:5Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 18468 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/a.parquet:  74%|▋| 375/504 [35:32<17:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 18468 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the role of urease in the breakdown of urea?\",\n",
      "    \"A\": \"Urease catalyzes the hydrolysis of urea to ammonia and carbon dioxide.\",\n",
      "    \"B\": \"Urease prevents the hydrolysis of urea and keeps it in its original form.\",\n",
      "    \"C\": \"Urease converts urea into a stable form that can be easily absorbed by plants.\",\n",
      "    \"D\": \"Urease reacts with water to form ammonium and hydroxide ions.\",\n",
      "    \"E\": \"Urease increases the pH of the soil and promotes ammonia volatilization.\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that urease is a naturally occurring enzyme that catalyzes the hydrolysis of urea to unstable carbamic acid, which rapidly decomposes to form ammonia and carbon dioxide.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:  78%|▊| 391/504 [36:59<10:0Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16693 tokens (12693 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/a.parquet:  78%|▊| 392/504 [37:00<07:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 16693 tokens (12693 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main focus of Arturo Zychlinsky's research?\",\n",
      "    \"A\": \"Bacterial pathogens\",\n",
      "    \"B\": \"Neutrophil Extracellular Traps (NETs)\",\n",
      "    \"C\": \"Chromatin\",\n",
      "    \"D\": \"Toll Like Receptors\",\n",
      "    \"E\": \"Autoimmunity\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The text states that Arturo Zychlinsky's research focuses on Neutrophil Extracellular Traps (NETs), which he discovered together with Volker Brinkmann.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet:  90%|▉| 452/504 [46:45<05:0Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 21203 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/a.parquet:  90%|▉| 453/504 [46:46<03:3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 21203 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is Angela Dean's area of specialization?\",\n",
      "    \"A\": \"Design of experiments\",\n",
      "    \"B\": \"Mathematics\",\n",
      "    \"C\": \"Physical and Engineering Sciences\",\n",
      "    \"D\": \"Statistics\",\n",
      "    \"E\": \"Data analysis\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that Angela Dean specializes in the design of experiments.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/a.parquet: 100%|█| 504/504 [52:11<00:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass: ../data/wikipedia/all.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/b.parquet:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                | 310/367 [34:51<05:31,  5.82s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 18426 tokens (14426 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/b.parquet:  85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                               | 311/367 [34:52<04:00,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 18426 tokens (14426 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the size of Bromsgrove Road Cutting?\",\n",
      "    \"A\": \"0.2 hectare\",\n",
      "    \"B\": \"0.5 acre\",\n",
      "    \"C\": \"1 hectare\",\n",
      "    \"D\": \"1 acre\",\n",
      "    \"E\": \"2 hectares\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The text states that Bromsgrove Road Cutting is a 0.2 hectare (0.5 acre) geological site of Special Scientific Interest.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/b.parquet:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 322/367 [35:56<04:20,  5.79s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 1 column 758 (char 757)\n",
      "../data/wikipedia/b.parquet:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                        | 323/367 [36:03<04:33,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 1 column 758 (char 757)\n",
      "{\"prompt\": \"What is Bayesian inference?\", \"A\": \"A method of statistical inference that uses Bayes' theorem to update the probability for a hypothesis as more evidence becomes available.\", \"B\": \"A technique in statistics that computes the posterior probability according to Bayes' theorem.\", \"C\": \"A method of updating beliefs based on new evidence.\", \"D\": \"A mathematical model used to predict future outcomes based on past data.\", \"E\": \"None of the above\", \"answer\": \"B\", \"basis\": \"Bayesian inference is a technique in statistics that uses Bayes' theorem to update the probability for a hypothesis as more evidence becomes available. This is stated in the given text: 'Bayesian inference computes the posterior probability according to Bayes' theorem: P(H \\mid E) = \\frac{P(E \\mid H) \\cdot P(H)}{P(E)}'.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/b.parquet:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 336/367 [37:15<03:00,  5.83s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 19957 tokens (15957 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/b.parquet:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 337/367 [37:16<02:09,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 19957 tokens (15957 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main purpose of the Bayesian structural time series (BSTS) model?\",\n",
      "    \"A\": \"Feature selection\",\n",
      "    \"B\": \"Time series forecasting\",\n",
      "    \"C\": \"Nowcasting\",\n",
      "    \"D\": \"Inferring causal impact\",\n",
      "    \"E\": \"All of the above\",\n",
      "    \"answer\": \"E\",\n",
      "    \"basis\": \"The text states that the BSTS model is used for feature selection, time series forecasting, nowcasting, inferring causal impact, and other applications.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/b.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 367/367 [40:15<00:00,  6.58s/it]\n",
      "../data/wikipedia/c.parquet:   1%|███                                                                                                                                                                                                              | 9/617 [00:48<50:00,  4.93s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 10 column 73 (char 407)\n",
      "../data/wikipedia/c.parquet:   2%|███▎                                                                                                                                                                                                          | 10/617 [00:57<1:02:27,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 10 column 73 (char 407)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the Cauchy–Schwarz inequality?\",\n",
      "    \"A\": \"An inequality for sums of vectors\",\n",
      "    \"B\": \"An inequality for integrals of vectors\",\n",
      "    \"C\": \"An inequality for dot products of vectors\",\n",
      "    \"D\": \"An inequality for cross products of vectors\",\n",
      "    \"E\": \"An inequality for norms of vectors\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"The Cauchy–Schwarz inequality states that for all vectors \\mathbf{u} and \\mathbf{v} of an inner product space, the dot product of the two vectors squared is less than or equal to the product of the norms of the vectors squared. In other words, \\left(\\mathbf{u} \\cdot \\mathbf{v}\\right)^2 \\leq \\|\\mathbf{u}\\|^2 \\|\\mathbf{v}\\|^2.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/c.parquet:  32%|█████████████████████████████████████████████████████████████████▊                                                                                                                                             | 196/617 [19:18<39:59,  5.70s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 10 column 368 (char 807)\n",
      "../data/wikipedia/c.parquet:  32%|██████████████████████████████████████████████████████████████████                                                                                                                                             | 197/617 [19:28<48:45,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 10 column 368 (char 807)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is a complete set of invariants in mathematics?\",\n",
      "    \"A\": \"A collection of maps that classify objects based on their equivalence\",\n",
      "    \"B\": \"A collection of equations that define the coinvariants\",\n",
      "    \"C\": \"A set of functions that determine the image of the maps\",\n",
      "    \"D\": \"A set of matrices with their corresponding eigenvalues\",\n",
      "    \"E\": \"A set of defining equations for the coinvariants\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"A complete set of invariants for a classification problem is a collection of maps that classify objects based on their equivalence. The text states, 'In mathematics, a complete set of invariants for a classification problem is a collection of maps :f_i : X \\to Y_i (where X is the collection of objects being classified, up to some equivalence relation \\sim, and the Y_i are some sets), such that x \\sim x' if and only if f_i(x) = f_i(x') for all i.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/c.parquet:  33%|████████████████████████████████████████████████████████████████████                                                                                                                                           | 203/617 [20:07<43:06,  6.25s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 10 column 239 (char 584)\n",
      "../data/wikipedia/c.parquet:  33%|████████████████████████████████████████████████████████████████████▍                                                                                                                                          | 204/617 [20:15<46:24,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 10 column 239 (char 584)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What does the phrase 'correlation does not imply causation' mean?\",\n",
      "    \"A\": \"Correlation always implies causation\",\n",
      "    \"B\": \"Correlation sometimes implies causation\",\n",
      "    \"C\": \"Correlation never implies causation\",\n",
      "    \"D\": \"Correlation implies causation in certain cases\",\n",
      "    \"E\": \"None of the above\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"The phrase 'correlation does not imply causation' means that a correlation between two events or variables does not necessarily indicate a cause-and-effect relationship between them. This is stated in the text: 'The phrase \"correlation does not imply causation\" refers to the inability to legitimately deduce a cause-and-effect relationship between two events or variables solely on the basis of an observed association or correlation between them.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/c.parquet:  41%|███████████████████████████████████████████████████████████████████████████████████▊                                                                                                                           | 250/617 [24:55<39:05,  6.39s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 21531 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/c.parquet:  41%|████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                          | 251/617 [24:56<28:25,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 21531 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main reason for designating Chalbury Hill and Quarry as a Site of Special Scientific Interest?\",\n",
      "    \"A\": \"The presence of grassland\",\n",
      "    \"B\": \"The disused limestone quarry\",\n",
      "    \"C\": \"The area covered by Chalbury Hill Fort\",\n",
      "    \"D\": \"The geological features\",\n",
      "    \"E\": \"The biological features\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"The text mentions that Chalbury Hill and Quarry is a biological and geological Site of Special Scientific Interest. It consists of grassland and a disused limestone quarry, but the main reason for its designation is the geological features.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/c.parquet:  48%|███████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                           | 296/617 [29:13<30:49,  5.76s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 10 column 154 (char 441)\n",
      "../data/wikipedia/c.parquet:  48%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                           | 297/617 [29:19<32:17,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 10 column 154 (char 441)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main characteristic of Chernozem soil?\",\n",
      "    \"A\": \"High moisture storage capacity\",\n",
      "    \"B\": \"High percentage of phosphorus\",\n",
      "    \"C\": \"High percentage of ammonia compounds\",\n",
      "    \"D\": \"Black color\",\n",
      "    \"E\": \"High percentage of humus\",\n",
      "    \"answer\": \"E\",\n",
      "    \"basis\": \"The main characteristic of Chernozem soil is that it contains a high percentage of humus (4% to 16%). The text states, 'Chernozem (from ; \"black ground\"), also called black soil, is a black-colored soil containing a high percentage of humus (4% to 16%) and high percentages of phosphorus and ammonia compounds.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/c.parquet:  61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                | 376/617 [36:41<25:41,  6.40s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 8 column 46 (char 361)\n",
      "../data/wikipedia/c.parquet:  61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                | 377/617 [36:48<26:43,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 8 column 46 (char 361)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the purpose of the COMET experiment?\",\n",
      "    \"A\": \"To study muon decay to an electron and neutrino\",\n",
      "    \"B\": \"To look for neutrinoless muon to electron conversion\",\n",
      "    \"C\": \"To investigate neutrino oscillations\",\n",
      "    \"D\": \"To measure the branching ratio of muon to electron conversion\",\n",
      "    \"E\": \"To study the supersymmetric \\tilde{\\chi_{0} }\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The purpose of the COMET experiment is to look for neutrinoless muon to electron conversion, where the electron flies away with an energy of 104.8 MeV. This is mentioned in the given text: 'COMET seeks to look for neutrinoless muon to electron conversion, where the electron flies away with an energy of 104.8 MeV.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/c.parquet:  63%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                             | 387/617 [37:51<21:52,  5.71s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 19367 tokens (15367 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/c.parquet:  63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                            | 388/617 [37:51<15:51,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 19367 tokens (15367 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main factor that affects the behavior of chevron folds?\",\n",
      "    \"A\": \"Thickness of low competence layers\",\n",
      "    \"B\": \"Thickness of high competence layers\",\n",
      "    \"C\": \"Regular thickness in high competence layers\",\n",
      "    \"D\": \"Irregular thickness in low competence layers\",\n",
      "    \"E\": \"Length of the bed\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"According to the text, the stability of chevron folding strictly requires regular thickness in the high-competence layers. Therefore, the main factor that affects the behavior of chevron folds is the regular thickness in high competence layers.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/c.parquet:  78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                             | 483/617 [46:56<12:42,  5.69s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 20902 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/c.parquet:  78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                            | 484/617 [46:58<09:36,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 20902 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What are the two main classes of charged particle beams?\",\n",
      "    \"A\": \"Unbunched beams and bunched beams\",\n",
      "    \"B\": \"Electron beams and proton beams\",\n",
      "    \"C\": \"Ion beams and electron beams\",\n",
      "    \"D\": \"Proton beams and ion beams\",\n",
      "    \"E\": \"Coasting beams and DC beams\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"According to the text, charged particle beams can be split into two main classes: unbunched beams (coasting beams or DC beams) and bunched beams.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/c.parquet: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 617/617 [1:00:27<00:00,  5.88s/it]\n",
      "../data/wikipedia/d.parquet:  52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                   | 168/322 [15:36<11:47,  4.59s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17676 tokens (13676 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/d.parquet:  52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                  | 169/322 [15:37<08:45,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 17676 tokens (13676 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the capital of County Down, Northern Ireland?\",\n",
      "    \"A\": \"Downpatrick\",\n",
      "    \"B\": \"Belfast\",\n",
      "    \"C\": \"Newry\",\n",
      "    \"D\": \"Mourne\",\n",
      "    \"E\": \"Dundrum\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"According to the text, Downpatrick is the county town of Down and the joint headquarters of Newry, Mourne and Down District Council.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/d.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [29:41<00:00,  5.53s/it]\n",
      "../data/wikipedia/e.parquet:  26%|██████████████████████████████████████████████████████▌                                                                                                                                                         | 95/362 [08:30<26:13,  5.90s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 18176 tokens (14176 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/e.parquet:  27%|███████████████████████████████████████████████████████▏                                                                                                                                                        | 96/362 [08:30<18:56,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 18176 tokens (14176 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the primary role of dominant Xylocopa virginica females in the nest?\",\n",
      "    \"A\": \"Reproduction\",\n",
      "    \"B\": \"Foraging\",\n",
      "    \"C\": \"Nest construction\",\n",
      "    \"D\": \"All of the above\",\n",
      "    \"E\": \"None of the above\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"According to the text, dominant X. virginica females are responsible for a full gamut of activities including reproduction, foraging, and nest construction.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/e.parquet:  36%|██████████████████████████████████████████████████████████████████████████▉                                                                                                                                    | 131/362 [11:26<17:22,  4.51s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 10 column 283 (char 814)\n",
      "../data/wikipedia/e.parquet:  36%|███████████████████████████████████████████████████████████████████████████▍                                                                                                                                   | 132/362 [11:35<22:41,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 10 column 283 (char 814)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the definition of the empirical distribution function?\",\n",
      "    \"A\": \"A process that describes the proportion of objects in a system in a given state.\",\n",
      "    \"B\": \"A process which counts the number of objects in a given state.\",\n",
      "    \"C\": \"A sequence of random variables that converge to the cumulative distribution function.\",\n",
      "    \"D\": \"A map on measurable functions that converges to a normal random variable.\",\n",
      "    \"E\": \"A process that converges weakly to a certain Gaussian process.\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"The definition of the empirical distribution function is given as: For X1, X2, ... Xn independent and identically-distributed random variables in R with common cumulative distribution function F(x), the empirical distribution function is defined by :F_n(x)=\\frac{1}{n}\\sum_{i=1}^n I_{(-\\infty,x]}(X_i), where IC is the indicator function of the set C. For every (fixed) x, Fn(x) is a sequence of random variables which converge to F(x) almost surely by the strong law of large numbers.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/e.parquet:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                    | 327/362 [29:27<03:49,  6.57s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17410 tokens (13410 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/e.parquet:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                   | 328/362 [29:28<02:42,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 17410 tokens (13410 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the difference between acute ecotoxicity and chronic ecotoxicity?\",\n",
      "    \"A\": \"Acute ecotoxicity refers to the detrimental effects resulting from a hazardous exposure for no more than 15 days, while chronic ecotoxicity refers to the detrimental effects resulting from a hazardous exposure of 15 days to possibly years.\",\n",
      "    \"B\": \"Acute ecotoxicity is lethal, while chronic ecotoxicity is not lethal but decreases cellular biochemical functions.\",\n",
      "    \"C\": \"Acute ecotoxicity is caused by natural pollutants, while chronic ecotoxicity is caused by synthetic pollutants.\",\n",
      "    \"D\": \"Acute ecotoxicity affects animals, vegetation, and microbes, while chronic ecotoxicity affects only animals.\",\n",
      "    \"E\": \"Acute ecotoxicity is associated with particular drug-receptor actions, while chronic ecotoxicity is associated with cell or tissue damage or death.\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"According to the text, acute ecotoxicity refers to the detrimental effects resulting from a hazardous exposure for no more than 15 days, while chronic ecotoxicity refers to the detrimental effects resulting from a hazardous exposure of 15 days to possibly years.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/e.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 362/362 [32:54<00:00,  5.45s/it]\n",
      "../data/wikipedia/f.parquet:  52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                  | 123/235 [11:15<10:27,  5.61s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 10 column 170 (char 587)\n",
      "../data/wikipedia/f.parquet:  53%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                 | 124/235 [11:22<11:26,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 10 column 170 (char 587)\n",
      "[\n",
      "    {\n",
      "        \"prompt\": \"What is the Fekete problem?\",\n",
      "        \"A\": \"Finding the points on a 2-sphere with minimal s-energy\",\n",
      "        \"B\": \"Finding the points on a 2-sphere with maximal s-energy\",\n",
      "        \"C\": \"Finding the points on a 2-sphere with minimal logarithmic energy\",\n",
      "        \"D\": \"Finding the points on a 2-sphere with maximal logarithmic energy\",\n",
      "        \"E\": \"None of the above\",\n",
      "        \"answer\": \"A\",\n",
      "        \"basis\": \"The Fekete problem is defined as finding the points x1,...,xN on the 2-sphere for which the s-energy is minimal. The s-energy is defined as the sum of \\|x_i - x_j \\|^{-s} for s > 0 and the sum of \\log \\|x_i - x_j \\|^{-1} for s = 0.\"\n",
      "    }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/f.parquet:  69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                               | 163/235 [15:06<06:07,  5.10s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 31012 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/f.parquet:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                              | 164/235 [15:07<04:34,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 31012 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What are fault blocks?\",\n",
      "    \"A\": \"Large areas of bedrock broken up into blocks by faults\",\n",
      "    \"B\": \"Crustal blocks broken off from tectonic plates\",\n",
      "    \"C\": \"Terranes that have a specific geologic definition\",\n",
      "    \"D\": \"Blocks characterized by relatively uniform lithology\",\n",
      "    \"E\": \"Blocks created by tectonic and localized stresses in Earth's crust\",\n",
      "    \"answer\": \"E\",\n",
      "    \"basis\": \"The text states that fault blocks are very large blocks of rock created by tectonic and localized stresses in Earth's crust.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/f.parquet:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                            | 167/235 [15:25<05:44,  5.06s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 7 column 24 (char 233)\n",
      "../data/wikipedia/f.parquet:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                           | 168/235 [15:31<06:01,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 7 column 24 (char 233)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the freshman's dream?\",\n",
      "    \"A\": \"The erroneous equation (x+y)^n=x^n+y^n\",\n",
      "    \"B\": \"The correct equation (x+y)^n=x^2+2xy+y^2\",\n",
      "    \"C\": \"The theorem that says (x + y)p = xp + yp\",\n",
      "    \"D\": \"The equation \\sqrt{x^2+y^2} = \\sqrt{x^2}+\\sqrt{y^2}\",\n",
      "    \"E\": \"The equation (x + y)2 = x2 + 2xy + y2\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that the freshman's dream is a name sometimes given to the erroneous equation (x+y)^n=x^n+y^n, where n is a real number (usually a positive integer greater than 1) and x,y are non-zero real numbers.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/f.parquet:  74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                      | 173/235 [15:57<05:39,  5.48s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17467 tokens (13467 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/f.parquet:  74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                     | 174/235 [15:57<04:00,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 17467 tokens (13467 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is frequency separation within astrophysics?\",\n",
      "    \"A\": \"The spacing in frequency between adjacent modes of oscillation\",\n",
      "    \"B\": \"The time required for a sound wave to travel to the center of the Sun and return\",\n",
      "    \"C\": \"The difference in frequency between modes of different angular degree\",\n",
      "    \"D\": \"The difference in frequency between modes of different azimuthal order\",\n",
      "    \"E\": \"The difference in frequency between modes of different radial order\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that frequency separation refers to the spacing in frequency between adjacent modes of oscillation, having the same angular degree (l) but different radial order (n).\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/f.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 235/235 [21:16<00:00,  5.43s/it]\n",
      "../data/wikipedia/g.parquet:   6%|███████████▊                                                                                                                                                                                                    | 20/351 [01:46<26:21,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 35699 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"When was Geologiska föreningen founded?\",\n",
      "    \"A\": \"1871\",\n",
      "    \"B\": \"1872\",\n",
      "    \"C\": \"1873\",\n",
      "    \"D\": \"1874\",\n",
      "    \"E\": \"1875\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"Geologiska föreningen was founded in Sweden in 1871.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 35699 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/g.parquet:  16%|████████████████████████████████▌                                                                                                                                                                               | 55/351 [04:25<24:05,  4.88s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 10 column 229 (char 597)\n",
      "../data/wikipedia/g.parquet:  16%|█████████████████████████████████▏                                                                                                                                                                              | 56/351 [04:32<27:06,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 10 column 229 (char 597)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What was the main finding of Griffith's experiment?\",\n",
      "    \"A\": \"Bacteria can transfer genetic information\",\n",
      "    \"B\": \"Bacteria can synthesize a polysaccharide capsule\",\n",
      "    \"C\": \"Bacteria can be killed by heat\",\n",
      "    \"D\": \"Bacteria can be classified into different types\",\n",
      "    \"E\": \"Bacteria can defeat the host's immune system\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"Griffith's experiment suggested that bacteria are capable of transferring genetic information through a process known as transformation. This is stated in the text: 'Griffith concluded that the type II-R had been \"transformed\" into the lethal III-S strain by a \"transforming principle\" that was somehow part of the dead III-S strain bacteria.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/g.parquet:  22%|██████████████████████████████████████████████▏                                                                                                                                                                 | 78/351 [06:26<21:57,  4.83s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 22915 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/g.parquet:  23%|██████████████████████████████████████████████▊                                                                                                                                                                 | 79/351 [06:27<16:23,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 22915 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is a glow discharge?\",\n",
      "    \"A\": \"A plasma formed by the passage of electric current through a gas\",\n",
      "    \"B\": \"A radiation source such as ultraviolet light or Cosmic rays\",\n",
      "    \"C\": \"The process of ionizing gas molecules\",\n",
      "    \"D\": \"The process of exciting atoms in a gas\",\n",
      "    \"E\": \"The process of sputtering particles from a cathode\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"According to the text, a glow discharge is a plasma formed by the passage of electric current through a gas.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/g.parquet:  51%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                     | 179/351 [15:15<12:55,  4.51s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 4 column 61 (char 137)\n",
      "../data/wikipedia/g.parquet:  51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                    | 180/351 [15:24<17:03,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 4 column 61 (char 137)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the definition of a general Dirichlet series?\",\n",
      "    \"A\": \"An infinite series that takes the form of a_n e^{-\\lambda_n s}\",\n",
      "    \"B\": \"An infinite series that takes the form of a_n / n^s\",\n",
      "    \"C\": \"An infinite series that takes the form of a_n (e^{-s})^n\",\n",
      "    \"D\": \"An infinite series that takes the form of a_n e^{-\\lambda_n s} where \\lambda_n is a strictly increasing sequence of nonnegative real numbers\",\n",
      "    \"E\": \"An infinite series that takes the form of a_n e^{-\\lambda_n s} where \\lambda_n is a strictly decreasing sequence of nonnegative real numbers\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"The definition of a general Dirichlet series is given in the text as: 'In the field of mathematical analysis, a general Dirichlet series is an infinite series that takes the form of : \\sum_{n=1}^\\infty a_n e^{-\\lambda_n s}, where a_n, s are complex numbers and \\\\{\\lambda_n\\\\} is a strictly increasing sequence of nonnegative real numbers that tends to infinity.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/g.parquet:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                         | 308/351 [26:36<04:09,  5.81s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 19518 tokens (15518 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/g.parquet:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                        | 309/351 [26:36<03:00,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 19518 tokens (15518 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What was the ancient Roman treatment for uterine prolapse?\",\n",
      "    \"A\": \"Surgical removal of the uterus and bladder\",\n",
      "    \"B\": \"Suspending the patient upside down from a ladder\",\n",
      "    \"C\": \"Wrapping magic stones in the skin of sacrificed animals\",\n",
      "    \"D\": \"Administering herbs and drugs\",\n",
      "    \"E\": \"Applying ligatures to the groin and armpits\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"According to the text, ancient Roman gynecologists treated uterine prolapse by suspending the patient upside down from a ladder.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/g.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [30:19<00:00,  5.18s/it]\n",
      "../data/wikipedia/h.parquet:   5%|█████████▌                                                                                                                                                                                                      | 14/306 [01:19<29:17,  6.02s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 17119 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/h.parquet:   5%|██████████▏                                                                                                                                                                                                     | 15/306 [01:20<21:48,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 17119 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is a halo nucleus in nuclear physics?\",\n",
      "    \"A\": \"A nucleus with a core surrounded by a halo of orbiting protons or neutrons\",\n",
      "    \"B\": \"A nucleus with a core surrounded by a halo of electrons\",\n",
      "    \"C\": \"A nucleus with a core surrounded by a halo of photons\",\n",
      "    \"D\": \"A nucleus with a core surrounded by a halo of positrons\",\n",
      "    \"E\": \"A nucleus with a core surrounded by a halo of neutrinos\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"According to the given text, a halo nucleus in nuclear physics is an atomic nucleus that has a core nucleus surrounded by a 'halo' of orbiting protons or neutrons.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/h.parquet:  16%|████████████████████████████████▋                                                                                                                                                                               | 48/306 [04:22<20:37,  4.79s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16586 tokens (12586 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/h.parquet:  16%|█████████████████████████████████▎                                                                                                                                                                              | 49/306 [04:23<15:26,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 16586 tokens (12586 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Who posed the question about the stability of group homomorphisms?\",\n",
      "    \"A\": \"Stanisław Ulam\",\n",
      "    \"B\": \"Donald H. Hyers\",\n",
      "    \"C\": \"Themistocles M. Rassias\",\n",
      "    \"D\": \"S.-M. Jung\",\n",
      "    \"E\": \"T. Aoki\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The stability problem of functional equations originated from a question of Stanisław Ulam, posed in 1940.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/h.parquet:  33%|████████████████████████████████████████████████████████████████████▎                                                                                                                                          | 101/306 [08:58<16:08,  4.72s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ':' delimiter: line 5 column 10 (char 94)\n",
      "../data/wikipedia/h.parquet:  33%|█████████████████████████████████████████████████████████████████████                                                                                                                                          | 102/306 [09:02<15:19,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ':' delimiter: line 5 column 10 (char 94)\n",
      "{\n",
      "  \"prompt\": \"When did X-ray solar studies begin?\",\n",
      "  \"A\": \"1920s\",\n",
      "  \"B\": \"1940s\",\n",
      "  \"1950s\",\n",
      "  \"1960s\",\n",
      "  \"1970s\",\n",
      "  \"answer\": \"B\",\n",
      "  \"basis\": \"According to the text, X-ray solar studies began in 1949 when Herbert Friedman started studying X-ray solar emissions.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/h.parquet:  37%|█████████████████████████████████████████████████████████████████████████████                                                                                                                                  | 114/306 [10:13<17:42,  5.53s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 4 column 56 (char 113)\n",
      "../data/wikipedia/h.parquet:  38%|█████████████████████████████████████████████████████████████████████████████▊                                                                                                                                 | 115/306 [10:21<19:49,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 4 column 56 (char 113)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is a homothetic vector field?\",\n",
      "    \"A\": \"A vector field that satisfies the condition: \\(\\mathcal{L}_X g_{ab}=2c g_{ab}\\)\",\n",
      "    \"B\": \"A vector field that satisfies the condition: \\(\\mathcal{L}_X g_{ab}=c g_{ab}\\)\",\n",
      "    \"C\": \"A vector field that satisfies the condition: \\(\\mathcal{L}_X g_{ab}=0\\)\",\n",
      "    \"D\": \"A vector field that satisfies the condition: \\(\\mathcal{L}_X g_{ab}=g_{ab}\\)\",\n",
      "    \"E\": \"A vector field that satisfies the condition: \\(\\mathcal{L}_X g_{ab}=3c g_{ab}\\)\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The given text states that a homothetic vector field satisfies the condition \\(\\mathcal{L}_X g_{ab}=2c g_{ab}\\), where c is a real constant.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/h.parquet:  60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                   | 183/306 [16:16<11:15,  5.49s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 21178 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/h.parquet:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                  | 184/306 [16:17<08:27,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 21178 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What are the two main formations encompassed by the Hamilton Group?\",\n",
      "    \"A\": \"Mahantango Formation and Marcellus Shale\",\n",
      "    \"B\": \"Millboro Shale and Millboro Formation\",\n",
      "    \"C\": \"Tully Limestone and Stafford Limestone\",\n",
      "    \"D\": \"Geneseo Shale and Moscow Formation\",\n",
      "    \"E\": \"Tioga Bentonites and Needmore Shale\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that the Hamilton Group is mainly composed of marine shale with some sandstone and consists of two main formations: the Mahantango Formation and the Marcellus Shale.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/h.parquet:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊          | 291/306 [26:14<01:41,  6.77s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16868 tokens (12868 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/h.parquet:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌         | 292/306 [26:14<01:09,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 16868 tokens (12868 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the Hill limit in solid-state physics?\",\n",
      "    \"A\": \"The critical distance between actinide or rare-earth atoms\",\n",
      "    \"B\": \"The radius of the f-orbital\",\n",
      "    \"C\": \"The distance at which the overlap of f-orbitals becomes negligible\",\n",
      "    \"D\": \"The distance at which f electrons become localized on ion sites\",\n",
      "    \"E\": \"The distance at which f electrons can move through the lattice\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"The Hill limit in solid-state physics is defined as the distance at which the overlap of f-orbitals becomes negligible. This means that if two atoms of the lattice are separated by a distance greater than the Hill limit, the overlap of their f-orbitals becomes insignificant.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/h.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [27:30<00:00,  5.39s/it]\n",
      "../data/wikipedia/i.parquet:  19%|███████████████████████████████████████▉                                                                                                                                                                        | 54/281 [05:00<19:04,  5.04s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 19411 tokens (15411 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/i.parquet:  20%|████████████████████████████████████████▋                                                                                                                                                                       | 55/281 [05:01<14:34,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 19411 tokens (15411 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main objective of the International Astrostatistics Association (IAA)?\",\n",
      "    \"A\": \"To promote innovation in all aspects of academic scientific research\",\n",
      "    \"B\": \"To foster collaboration between statisticians and astrophysicists\",\n",
      "    \"C\": \"To create an interdisciplinary community around data-driven problems in Astronomy\",\n",
      "    \"D\": \"To manage the IAA Council\",\n",
      "    \"E\": \"To organize the biannual ISI World Statistics Congress\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The text states that the foremost objective of the IAA is to foster collaboration between statisticians and astrophysicists.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/i.parquet:  40%|███████████████████████████████████████████████████████████████████████████████████▏                                                                                                                           | 113/281 [10:08<16:22,  5.85s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 21097 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/i.parquet:  41%|███████████████████████████████████████████████████████████████████████████████████▉                                                                                                                           | 114/281 [10:08<12:04,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 21097 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"When was the Italian Mathematical Union founded?\",\n",
      "    \"A\": \"December 7, 1922\",\n",
      "    \"B\": \"March 31, 1922\",\n",
      "    \"C\": \"December 7, 1920\",\n",
      "    \"D\": \"December 7, 1928\",\n",
      "    \"E\": \"March 31, 1928\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"According to the text, the Italian Mathematical Union was founded on December 7, 1922, as mentioned in the sentence: 'It was founded on December 7, 1922, by Luigi Bianchi, Vito Volterra, and most notably, Salvatore Pincherle, who became the Union's first President.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/i.parquet:  41%|█████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                         | 116/281 [10:19<13:15,  4.82s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 18291 tokens (14291 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/i.parquet:  42%|██████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                        | 117/281 [10:19<09:33,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 18291 tokens (14291 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Who are the pioneers of the International Workshop on Nitride Semiconductors?\",\n",
      "    \"A\": \"Isamu Akasaki and Hiroshi Amano\",\n",
      "    \"B\": \"Hiroshi Fujioka and Alan Doolittle\",\n",
      "    \"C\": \"Tomas Palacios and Hiroshi Amano\",\n",
      "    \"D\": \"Isamu Akimoto and Alan Doolittle\",\n",
      "    \"E\": \"Hiroshi Amano and Tomas Palacios\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The pioneers of the International Workshop on Nitride Semiconductors are Isamu Akasaki and Hiroshi Amano, who are Nobel laureates in physics (2014).\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/i.parquet:  49%|████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                          | 137/281 [11:58<12:58,  5.41s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 19169 tokens (15169 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/i.parquet:  49%|█████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                         | 138/281 [11:58<09:19,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 19169 tokens (15169 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the mission of Island Conservation?\",\n",
      "    \"A\": \"To prevent extinctions by removing invasive species from islands\",\n",
      "    \"B\": \"To promote sustainable development goals\",\n",
      "    \"C\": \"To conduct field research on marine systems\",\n",
      "    \"D\": \"To protect endangered species on the IUCN's Red List\",\n",
      "    \"E\": \"To develop plans for island restoration projects\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The mission of Island Conservation is to prevent extinctions by removing invasive species from islands. This is stated in the text: 'Island Conservation is a non-profit organization with the mission to prevent extinctions by removing invasive species from islands.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/i.parquet:  61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 172/281 [15:05<09:28,  5.21s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16487 tokens (12487 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/i.parquet:  62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                               | 173/281 [15:06<07:13,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 16487 tokens (12487 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the state tree of Arizona?\",\n",
      "    \"A\": \"Blue palo verde\",\n",
      "    \"B\": \"Saguaro blossom\",\n",
      "    \"C\": \"Arizona cypress\",\n",
      "    \"D\": \"Desert spoon\",\n",
      "    \"E\": \"Ocotillo\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The state tree of Arizona is the blue palo verde. This is mentioned in the text under the section 'Symbols of the State of Arizona'.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/i.parquet:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                               | 239/281 [20:49<03:57,  5.65s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 29269 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/i.parquet:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 240/281 [20:50<02:51,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 29269 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the role of hydrocarbons in the recognition of individuals in social insects?\",\n",
      "    \"A\": \"Hydrocarbons help social insects build nests.\",\n",
      "    \"B\": \"Hydrocarbons allow social insects to communicate with other colonies.\",\n",
      "    \"C\": \"Hydrocarbons enable social insects to recognize individuals from their own colony.\",\n",
      "    \"D\": \"Hydrocarbons help social insects in reproduction.\",\n",
      "    \"E\": \"Hydrocarbons have no role in the recognition of individuals in social insects.\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"According to the text, hydrocarbons are a blend of specific signatures that emanate from the individual and are used for recognition within the same colony. The mixture of hydrocarbons, known as the insect's label, is compared with the individual's internal colony odor or template to determine recognition.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/i.parquet:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                          | 245/281 [21:14<02:46,  4.64s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16474 tokens (12474 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/i.parquet:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                         | 246/281 [21:14<02:00,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 16474 tokens (12474 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the field of isotope electrochemistry concerned with?\",\n",
      "    \"A\": \"Electrochemical separation of isotopes\",\n",
      "    \"B\": \"Estimation of isotopic exchange equilibrium constants\",\n",
      "    \"C\": \"Kinetic isotope effect\",\n",
      "    \"D\": \"Isotope sensors\",\n",
      "    \"E\": \"All of the above\",\n",
      "    \"answer\": \"E\",\n",
      "    \"basis\": \"The text states that isotope electrochemistry is concerned with various topics like electrochemical separation of isotopes, electrochemical estimation of isotopic exchange equilibrium constants, electrochemical kinetic isotope effect, and electrochemical isotope sensors.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/i.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 281/281 [24:14<00:00,  5.18s/it]\n",
      "../data/wikipedia/j.parquet:   8%|████████████████▎                                                                                                                                                                                               | 19/242 [01:24<16:39,  4.48s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 17264 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/j.parquet:   8%|█████████████████▏                                                                                                                                                                                              | 20/242 [01:24<12:05,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 17264 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What are Jerome Goldstein's main interests?\",\n",
      "    \"A\": \"Partial differential equations\",\n",
      "    \"B\": \"Operator theory\",\n",
      "    \"C\": \"Stochastic analysis\",\n",
      "    \"D\": \"Fluid dynamics\",\n",
      "    \"E\": \"All of the above\",\n",
      "    \"answer\": \"E\",\n",
      "    \"basis\": \"According to the given text, Jerome Goldstein's main interests are partial differential equations, operator theory, stochastic analysis, fluid dynamics, quantum theory, and mathematical finance.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/j.parquet:  91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                  | 221/242 [17:01<01:48,  5.18s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 33026 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/j.parquet:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 222/242 [17:02<01:19,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 33026 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is Jun-Muk Hwang's area of specialization?\",\n",
      "    \"A\": \"Algebraic geometry\",\n",
      "    \"B\": \"Complex differential geometry\",\n",
      "    \"C\": \"Physics\",\n",
      "    \"D\": \"Mathematical sciences\",\n",
      "    \"E\": \"None of the above\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The text states that Jun-Muk Hwang specializes in algebraic geometry and complex differential geometry.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/j.parquet:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 236/242 [18:05<00:28,  4.67s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 17944 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/j.parquet:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 237/242 [18:06<00:17,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 17944 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What are the research areas of Joachim von zur Gathen?\",\n",
      "    \"A\": \"Computational complexity\",\n",
      "    \"B\": \"Cryptography\",\n",
      "    \"C\": \"Finite fields\",\n",
      "    \"D\": \"Computer algebra\",\n",
      "    \"E\": \"All of the above\",\n",
      "    \"answer\": \"E\",\n",
      "    \"basis\": \"The text states that Joachim von zur Gathen's research spans several areas in mathematics and computer science, including computational complexity, cryptography, finite fields, and computer algebra.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/j.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 242/242 [18:30<00:00,  4.59s/it]\n",
      "../data/wikipedia/k.parquet:  12%|███████████████████████▉                                                                                                                                                                                        | 16/139 [01:17<09:37,  4.70s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 7 column 36 (char 307)\n",
      "../data/wikipedia/k.parquet:  12%|█████████████████████████▍                                                                                                                                                                                      | 17/139 [01:25<11:22,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 7 column 36 (char 307)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main idea behind Kaluza-Klein theory?\",\n",
      "    \"A\": \"The existence of a fifth dimension beyond the four dimensions of space and time\",\n",
      "    \"B\": \"The unification of gravitation and electromagnetism\",\n",
      "    \"C\": \"The precursor to string theory\",\n",
      "    \"D\": \"The introduction of the \"cylinder condition\" hypothesis\",\n",
      "    \"E\": \"The quantum interpretation of the fifth dimension\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The main idea behind Kaluza-Klein theory is the existence of a fifth dimension beyond the four dimensions of space and time. This is mentioned in the text: 'In physics, Kaluza–Klein theory (KK theory) is a classical unified field theory of gravitation and electromagnetism built around the idea of a fifth dimension beyond the common 4D of space and time and considered an important precursor to string theory.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/k.parquet:  21%|███████████████████████████████████████████▍                                                                                                                                                                    | 29/139 [02:31<09:36,  5.24s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 10 column 98 (char 560)\n",
      "../data/wikipedia/k.parquet:  22%|████████████████████████████████████████████▉                                                                                                                                                                   | 30/139 [02:38<10:10,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 10 column 98 (char 560)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is a kernel smoother?\",\n",
      "    \"A\": \"A statistical technique to estimate a real valued function as the weighted average of neighboring observed data\",\n",
      "    \"B\": \"A technique to fit locally linear functions\",\n",
      "    \"C\": \"A method to estimate the value of Y(X0) by averaging the values of m nearest neighbors\",\n",
      "    \"D\": \"A technique to fit polynomial functions\",\n",
      "    \"E\": \"A technique to estimate a continuous function of X\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"A kernel smoother is a statistical technique to estimate a real valued function f: \\mathbb{R}^p \\to \\mathbb{R} as the weighted average of neighboring observed data. The weight is defined by the kernel, such that closer points are given higher weights. The estimated function is smooth, and the level of smoothness is set by a single parameter.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/k.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 139/139 [11:20<00:00,  4.90s/it]\n",
      "../data/wikipedia/l.parquet:   7%|█████████████▌                                                                                                                                                                                                  | 30/458 [02:26<37:11,  5.21s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 25853 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/l.parquet:   7%|██████████████                                                                                                                                                                                                  | 31/458 [02:26<27:27,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 25853 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Which prize recognizes outstanding achievements in the field of atomic or surface physics?\",\n",
      "    \"A\": \"David Adler Lectureship Award in the Field of Materials Physics\",\n",
      "    \"B\": \"Will Allis Prize for the Study of Ionized Gases\",\n",
      "    \"C\": \"Leroy Apker Award\",\n",
      "    \"D\": \"Davisson–Germer Prize in Atomic or Surface Physics\",\n",
      "    \"E\": \"Hans A. Bethe Prize\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"The Davisson–Germer Prize in Atomic or Surface Physics is an annual prize for 'outstanding work in atomic physics or surface physics'. The prize is named after Clinton Davisson and Lester Germer, who first measured electron diffraction.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  11%|██████████████████████▋                                                                                                                                                                                         | 50/458 [03:57<31:06,  4.57s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 31698 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/l.parquet:  11%|███████████████████████▏                                                                                                                                                                                        | 51/458 [03:58<23:56,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 31698 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"When did USM Alger win their first trophy?\",\n",
      "    \"A\": \"1937\",\n",
      "    \"B\": \"1963\",\n",
      "    \"C\": \"1995\",\n",
      "    \"D\": \"2013\",\n",
      "    \"E\": \"2018\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"According to the text, USM Alger won their first trophy in 1963.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  12%|████████████████████████▌                                                                                                                                                                                       | 54/458 [04:08<23:19,  3.46s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 16464 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/l.parquet:  12%|████████████████████████▉                                                                                                                                                                                       | 55/458 [04:09<17:32,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 16464 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Where did Lewis Salter receive his undergraduate education?\",\n",
      "    \"A\": \"University of Oklahoma\",\n",
      "    \"B\": \"University of Oxford\",\n",
      "    \"C\": \"Knox College\",\n",
      "    \"D\": \"Bandung Institute of Technology\",\n",
      "    \"E\": \"Argonne National Laboratory\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"According to the text, Lewis Salter received his undergraduate education at the University of Oklahoma.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  15%|███████████████████████████████▎                                                                                                                                                                                | 69/458 [05:16<29:42,  4.58s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 19842 tokens (15842 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/l.parquet:  15%|███████████████████████████████▊                                                                                                                                                                                | 70/458 [05:17<22:16,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 19842 tokens (15842 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Which Apollo mission returned a few dozen pounds/kilos of lunar material?\",\n",
      "    \"A\": \"Apollo 11\",\n",
      "    \"B\": \"Apollo 17\",\n",
      "    \"C\": \"Apollo 15\",\n",
      "    \"D\": \"Apollo 13\",\n",
      "    \"E\": \"Apollo 12\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that the Apollo 11 mission to the surface of the Moon returned a few dozen pounds/kilos of lunar material.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  16%|████████████████████████████████▋                                                                                                                                                                               | 72/458 [05:24<23:27,  3.65s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 19064 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/l.parquet:  16%|█████████████████████████████████▏                                                                                                                                                                              | 73/458 [05:25<17:15,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 19064 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Who is the author of the book 'Silent Spring'?\",\n",
      "    \"A\": \"Edward Abbey\",\n",
      "    \"B\": \"Rachel Carson\",\n",
      "    \"C\": \"Henry Beston\",\n",
      "    \"D\": \"Marilyn A. Brown\",\n",
      "    \"E\": \"Lester R. Brown\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The author of the book 'Silent Spring' is Rachel Carson.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  17%|████████████████████████████████████▎                                                                                                                                                                           | 80/458 [06:05<33:55,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 20065 tokens (16065 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is lipophilic efficiency (LiPE) used for in drug design and drug discovery?\",\n",
      "    \"A\": \"To evaluate the quality of research compounds\",\n",
      "    \"B\": \"To estimate druglikeness\",\n",
      "    \"C\": \"To link potency and lipophilicity\",\n",
      "    \"D\": \"All of the above\",\n",
      "    \"E\": \"None of the above\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"The text states that lipophilic efficiency (LiPE) is used to evaluate the quality of research compounds, estimate druglikeness, and link potency and lipophilicity in an attempt to estimate druglikeness.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 20065 tokens (16065 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/l.parquet:  25%|████████████████████████████████████████████████████▍                                                                                                                                                          | 116/458 [08:59<30:07,  5.28s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17116 tokens (13116 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/l.parquet:  26%|████████████████████████████████████████████████████▉                                                                                                                                                          | 117/458 [09:00<21:43,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 17116 tokens (13116 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Which formation is from the Carboniferous period?\",\n",
      "    \"A\": \"Amsden Formation\",\n",
      "    \"B\": \"Bearpaw Formation\",\n",
      "    \"C\": \"Climbing Arrow Formation\",\n",
      "    \"D\": \"Deadwood Formation\",\n",
      "    \"E\": \"Fort Union Formation\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The Amsden Formation is listed as being from the Carboniferous period.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  32%|█████████████████████████████████████████████████████████████████▌                                                                                                                                             | 145/458 [11:21<28:55,  5.54s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 47987 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/l.parquet:  32%|█████████████████████████████████████████████████████████████████▉                                                                                                                                             | 146/458 [11:22<22:27,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 47987 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the purpose of the light cone in defining the concept of causality?\",\n",
      "    \"A\": \"To describe the path of light in spacetime\",\n",
      "    \"B\": \"To determine the speed of light in different frames of reference\",\n",
      "    \"C\": \"To classify events in spacetime into distinct categories\",\n",
      "    \"D\": \"To visualize the expansion of the universe\",\n",
      "    \"E\": \"To explain the behavior of black holes\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"The purpose of the light cone in defining the concept of causality is to classify events in spacetime into distinct categories. The light cone separates events that can influence each other from events that cannot. It helps determine the set of events that lie on or inside the past or future light cone of a given event, which represents the events that can send or receive a signal that would have time to reach and influence the given event.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  33%|████████████████████████████████████████████████████████████████████▏                                                                                                                                          | 151/458 [11:47<26:04,  5.10s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 20369 tokens (16369 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/l.parquet:  33%|████████████████████████████████████████████████████████████████████▋                                                                                                                                          | 152/458 [11:48<20:10,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 20369 tokens (16369 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main difference between loop-mediated isothermal amplification (LAMP) and polymerase chain reaction (PCR)?\",\n",
      "    \"A\": \"LAMP requires a thermal cycler, while PCR does not.\",\n",
      "    \"B\": \"LAMP uses a constant temperature, while PCR uses alternating temperature steps.\",\n",
      "    \"C\": \"LAMP requires a reverse transcription step, while PCR does not.\",\n",
      "    \"D\": \"LAMP uses two sets of primers, while PCR uses four primers.\",\n",
      "    \"E\": \"LAMP produces a single band on a gel, while PCR produces a ladder pattern.\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The text states that LAMP is carried out at a constant temperature, while PCR is carried out with a series of alternating temperature steps or cycles.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  36%|█████████████████████████████████████████████████████████████████████████▋                                                                                                                                     | 163/458 [12:43<20:28,  4.16s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 18785 tokens (14785 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/l.parquet:  36%|██████████████████████████████████████████████████████████████████████████                                                                                                                                     | 164/458 [12:43<14:52,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 18785 tokens (14785 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Which planet has a list of geological features that includes craters, mountains, rocks, and plains?\",\n",
      "    \"A\": \"Mercury\",\n",
      "    \"B\": \"Venus\",\n",
      "    \"C\": \"Mars\",\n",
      "    \"D\": \"Jupiter\",\n",
      "    \"E\": \"Saturn\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"The list of geological features on Mars includes craters, mountains, rocks, and plains.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                             | 251/458 [20:06<14:45,  4.28s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 22114 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/l.parquet:  55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                             | 252/458 [20:07<11:06,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 22114 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is Lisa Alvarez-Cohen's area of expertise?\",\n",
      "    \"A\": \"Engineering and applied science\",\n",
      "    \"B\": \"Environmental microbiology and ecology\",\n",
      "    \"C\": \"Chemistry\",\n",
      "    \"D\": \"Civil and environmental engineering\",\n",
      "    \"E\": \"Microbial degradation of environmental contaminants\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"In the given text, it is mentioned that Lisa Alvarez-Cohen works in environmental microbiology and ecology.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                          | 293/458 [23:25<13:17,  4.83s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 20724 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/l.parquet:  64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                          | 294/458 [23:26<09:46,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 20724 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Which textbook is recommended for advanced undergraduate and graduate level classical mechanics?\",\n",
      "    \"A\": \"Chapters 1–21\",\n",
      "    \"B\": \"Course of Theoretical Physics Volume 3 - Quantum Mechanics: Non-Relativistic Theory\",\n",
      "    \"C\": \"A Modern Approach to Quantum Mechanics\",\n",
      "    \"D\": \"Classical Mechanics\",\n",
      "    \"E\": \"None of the above\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"The textbook recommended for advanced undergraduate and graduate level classical mechanics is 'Classical Mechanics'.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                       | 336/458 [26:44<07:52,  3.88s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 30265 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/l.parquet:  74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                      | 337/458 [26:45<05:54,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 30265 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"In which year did LA Galaxy win the CONCACAF Champions' Cup?\",\n",
      "    \"A\": \"1997\",\n",
      "    \"B\": \"1999\",\n",
      "    \"C\": \"2000\",\n",
      "    \"D\": \"2003\",\n",
      "    \"E\": \"2006\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"The LA Galaxy won the CONCACAF Champions' Cup in 2000.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 30023 tokens. Please reduce the length of the messages.\n",
      "\r",
      "../data/wikipedia/l.parquet:  74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                      | 338/458 [26:46<04:41,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 30023 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"In which year did LA Galaxy win the CONCACAF Champions' Cup?\",\n",
      "    \"A\": \"1997\",\n",
      "    \"B\": \"1999\",\n",
      "    \"C\": \"2000\",\n",
      "    \"D\": \"2003\",\n",
      "    \"E\": \"2006\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"The LA Galaxy won the CONCACAF Champions' Cup in 2000.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                         | 366/458 [29:03<07:41,  5.01s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 18203 tokens (14203 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/l.parquet:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                         | 367/458 [29:04<05:29,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 18203 tokens (14203 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Which team has the highest win percentage in the Champions League Twenty20?\",\n",
      "    \"A\": \"Chennai Super Kings\",\n",
      "    \"B\": \"Mumbai Indians\",\n",
      "    \"C\": \"Trinidad and Tobago\",\n",
      "    \"D\": \"Royal Challengers Bangalore\",\n",
      "    \"E\": \"Kolkata Knight Riders\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The team with the highest win percentage is Chennai Super Kings with 61.36%. This is evident from the 'Results summary' section of the given text.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                 | 385/458 [30:38<06:02,  4.97s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 18220 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/l.parquet:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 386/458 [30:38<04:18,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 18220 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Where did Lincoln Stein complete his Doctor of Medicine and PhD?\",\n",
      "    \"A\": \"Harvard Medical School\",\n",
      "    \"B\": \"Massachusetts Institute of Technology\",\n",
      "    \"C\": \"Cold Spring Harbor Laboratory\",\n",
      "    \"D\": \"Ontario Institute for Cancer Research\",\n",
      "    \"E\": \"Whitehead Institute of Biomedical Research\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that Lincoln Stein completed his Doctor of Medicine at Harvard Medical School and a PhD in Cell Biology at Harvard University.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                | 387/458 [30:42<04:27,  3.76s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 16582 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/l.parquet:  85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                               | 388/458 [30:43<03:12,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 16582 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is Lance Fortnow known for?\",\n",
      "    \"A\": \"Results in computational complexity and interactive proof systems\",\n",
      "    \"B\": \"Discovering the P versus NP problem\",\n",
      "    \"C\": \"Developing quantum computing\",\n",
      "    \"D\": \"Contributing to game theory\",\n",
      "    \"E\": \"Studying genome sequencing\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that Lance Fortnow is known for major results in computational complexity and interactive proof systems.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                    | 413/458 [32:39<03:11,  4.25s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17283 tokens (13283 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/l.parquet:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 414/458 [32:39<02:15,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 17283 tokens (13283 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Which player has the most league appearances in the given list?\",\n",
      "    \"A\": \"Bertil Bäckvall\",\n",
      "    \"B\": \"Sven Käll\",\n",
      "    \"C\": \"Paulinho Guará\",\n",
      "    \"D\": \"Patrik Andersson\",\n",
      "    \"E\": \"Sören Mannberg\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The player with the most league appearances in the given list is Sven Käll, with 99 appearances.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/l.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 458/458 [36:21<00:00,  4.76s/it]\n",
      "../data/wikipedia/m.parquet:   4%|████████▊                                                                                                                                                                                                       | 23/543 [02:15<49:23,  5.70s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17516 tokens (13516 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/m.parquet:   4%|█████████▏                                                                                                                                                                                                      | 24/543 [02:16<35:54,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 17516 tokens (13516 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"When was the Men's Volleyball at the Games of the Small States of Europe first introduced?\",\n",
      "    \"A\": \"1987\",\n",
      "    \"B\": \"1989\",\n",
      "    \"C\": \"1991\",\n",
      "    \"D\": \"1993\",\n",
      "    \"E\": \"1995\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"According to the text, the Men's Volleyball at the Games of the Small States of Europe was first introduced in 1987.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/m.parquet:   7%|██████████████▏                                                                                                                                                                                                 | 37/543 [03:34<56:27,  6.70s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 19029 tokens (15029 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/m.parquet:   7%|██████████████▌                                                                                                                                                                                                 | 38/543 [03:35<41:31,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 19029 tokens (15029 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is Modigliani risk-adjusted performance (M2)?\",\n",
      "    \"A\": \"A measure of the risk-adjusted returns of an investment portfolio\",\n",
      "    \"B\": \"A measure of the risk-free rate of return\",\n",
      "    \"C\": \"A measure of the volatility of the market\",\n",
      "    \"D\": \"A measure of the excess return of a benchmark portfolio\",\n",
      "    \"E\": \"A measure of the average risk-free rate for a period\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that Modigliani risk-adjusted performance (M2) is a measure of the risk-adjusted returns of some investment portfolio. It measures the returns of the portfolio, adjusted for the risk of the portfolio relative to that of some benchmark (e.g., the market).\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/m.parquet:  25%|███████████████████████████████████████████████████                                                                                                                                                            | 134/543 [12:11<36:19,  5.33s/it]Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia/m.parquet:  25%|██████████████████████████████████████████████████▋                                                                                                                                                         | 135/543 [17:18<10:52:40, 95.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is Mark Alford's area of research?\",\n",
      "    \"A\": \"Quantum mechanics\",\n",
      "    \"B\": \"Astrophysics\",\n",
      "    \"C\": \"Particle physics\",\n",
      "    \"D\": \"Thermodynamics\",\n",
      "    \"E\": \"None of the above\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The text states that Mark Alford researches dense matter inside neutron stars.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/m.parquet:  32%|██████████████████████████████████████████████████████████████████▋                                                                                                                                            | 175/543 [21:31<31:54,  5.20s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 10 column 202 (char 506)\n",
      "../data/wikipedia/m.parquet:  32%|███████████████████████████████████████████████████████████████████                                                                                                                                            | 176/543 [21:36<31:38,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 10 column 202 (char 506)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the term used to describe the connection between the components of mechanically interlocked molecular architectures?\",\n",
      "    \"A\": \"Supramolecular bond\",\n",
      "    \"B\": \"Covalent bond\",\n",
      "    \"C\": \"Chemical bond\",\n",
      "    \"D\": \"Mechanical bond\",\n",
      "    \"E\": \"Residual bond\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"The term used to describe the connection between the components of mechanically interlocked molecular architectures is a mechanical bond. This is mentioned in the text: 'The terminology \"mechanical bond\" has been coined to describe the connection between the components of mechanically interlocked molecular architectures.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/m.parquet:  34%|██████████████████████████████████████████████████████████████████████▌                                                                                                                                        | 185/543 [22:20<27:33,  4.62s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 32690 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/m.parquet:  34%|██████████████████████████████████████████████████████████████████████▉                                                                                                                                        | 186/543 [22:21<20:16,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 32690 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Who was the director of MURA during its formative years?\",\n",
      "    \"A\": \"Donald Kerst\",\n",
      "    \"B\": \"Keith Symon\",\n",
      "    \"C\": \"Tihiro Ohkawa\",\n",
      "    \"D\": \"John F. Kennedy\",\n",
      "    \"E\": \"Lyndon B. Johnson\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"In its formative years, Donald Kerst was the director of MURA.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/m.parquet:  55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                             | 297/543 [32:27<18:05,  4.41s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 17684 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/m.parquet:  55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 298/543 [32:27<13:02,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 17684 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the Molucca Sea Plate classified as?\",\n",
      "    \"A\": \"A convergent plate\",\n",
      "    \"B\": \"A divergent plate\",\n",
      "    \"C\": \"A transform plate\",\n",
      "    \"D\": \"A microplate\",\n",
      "    \"E\": \"A subduction plate\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"The Molucca Sea Plate has been classified by scientists as a fully subducted microplate that is part of the Molucca Sea Collision Complex.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/m.parquet:  72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 389/543 [40:12<11:22,  4.43s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 10 column 202 (char 723)\n",
      "../data/wikipedia/m.parquet:  72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                          | 390/543 [40:20<14:06,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 10 column 202 (char 723)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the original use of the term 'modulo' in mathematics?\",\n",
      "    \"A\": \"To assert that two distinct mathematical objects can be regarded as equivalent\",\n",
      "    \"B\": \"To refer to the remainder of the numerical division of two numbers\",\n",
      "    \"C\": \"To map a functor to a category by highlighting or defining remainders\",\n",
      "    \"D\": \"To declare things equivalent that otherwise would be considered distinct\",\n",
      "    \"E\": \"To factor out a normal subgroup (or an ideal) from a group (or ring)\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The original use of the term 'modulo' in mathematics is to refer to the remainder of the numerical division of two numbers. This is stated in the text: 'Gauss originally intended to use \"modulo\" as follows: given the integers a, b and n, the expression a ≡ b (mod n) (pronounced \"a is congruent to b modulo n\") means that a − b is an integer multiple of n, or equivalently, a and b both leave the same remainder when divided by n.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/m.parquet:  83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                   | 449/543 [45:31<07:18,  4.66s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 21280 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/m.parquet:  83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                   | 450/543 [45:32<05:19,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 21280 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the function of membrane-bound polyribosomes?\",\n",
      "    \"A\": \"Produce proteins for intracellular use\",\n",
      "    \"B\": \"Produce proteins for exocytosis\",\n",
      "    \"C\": \"Produce proteins for endocytosis\",\n",
      "    \"D\": \"Produce proteins for cell division\",\n",
      "    \"E\": \"Produce proteins for DNA replication\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"According to the text, membrane-bound polyribosomes usually produce proteins that are used within the cell membrane or are expelled from the cell via exocytosis.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/m.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 543/543 [53:23<00:00,  5.90s/it]\n",
      "../data/wikipedia/n.parquet:  32%|██████████████████████████████████████████████████████████████████▉                                                                                                                                             | 84/261 [06:45<15:00,  5.09s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 10 column 241 (char 700)\n",
      "../data/wikipedia/n.parquet:  33%|███████████████████████████████████████████████████████████████████▋                                                                                                                                            | 85/261 [06:52<16:51,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 10 column 241 (char 700)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the definition of a negligible function in mathematics?\",\n",
      "    \"A\": \"A function that approaches zero as x approaches infinity\",\n",
      "    \"B\": \"A function that approaches zero as x approaches zero\",\n",
      "    \"C\": \"A function that approaches infinity as x approaches zero\",\n",
      "    \"D\": \"A function that approaches infinity as x approaches infinity\",\n",
      "    \"E\": \"A function that has a constant value regardless of the value of x\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"According to the given text, a negligible function is defined as a function that approaches zero as x approaches infinity. This is stated in the first sentence of the text: 'In mathematics, a negligible function is a function \\mu:\\mathbb{N}\\to\\mathbb{R} such that for every positive integer c there exists an integer Nc such that for all x > Nc, :|\\mu(x)|<\\frac{1}{x^c}.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/n.parquet:  54%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                               | 141/261 [11:36<09:19,  4.66s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 50112 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/n.parquet:  54%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                              | 142/261 [11:37<07:17,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 50112 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the characteristic feature of napoleonite?\",\n",
      "    \"A\": \"Rounded lumps\",\n",
      "    \"B\": \"Concentric zones of light and dark colors\",\n",
      "    \"C\": \"Radial arrangement of crystals\",\n",
      "    \"D\": \"All of the above\",\n",
      "    \"E\": \"None of the above\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"The text states that napoleonite is characterized by rounded lumps, concentric zones of light and dark colors, and a radial arrangement of crystals.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/n.parquet:  76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                 | 199/261 [16:20<05:38,  5.46s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 18024 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/n.parquet:  77%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                | 200/261 [16:21<04:04,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 18024 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main feature that distinguishes geophysics from geology?\",\n",
      "    \"A\": \"Direct access to the rock\",\n",
      "    \"B\": \"Remote sensing\",\n",
      "    \"C\": \"Data acquisition\",\n",
      "    \"D\": \"Geological interpretation\",\n",
      "    \"E\": \"Data processing\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"In the text, it is mentioned that the main feature that distinguishes geophysics from geology is that it involves remote sensing. Various physical phenomena are used to probe below the surface where scientists cannot directly access the rock.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/n.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 261/261 [21:46<00:00,  5.01s/it]\n",
      "../data/wikipedia/number.parquet:  79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 34/43 [02:58<00:45,  5.07s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 18007 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/number.parquet:  81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                      | 35/43 [02:59<00:30,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 18007 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Which player scored the most tries in the 2015 Rugby World Cup?\",\n",
      "    \"A\": \"Julian Savea\",\n",
      "    \"B\": \"Nehe Milner-Skudder\",\n",
      "    \"C\": \"Juan Imhoff\",\n",
      "    \"D\": \"Bryan Habana\",\n",
      "    \"E\": \"JP Pietersen\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The player who scored the most tries in the 2015 Rugby World Cup was Julian Savea.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/number.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [03:44<00:00,  5.22s/it]\n",
      "../data/wikipedia/o.parquet:   6%|████████████▋                                                                                                                                                                                                    | 9/148 [00:49<13:13,  5.71s/it]Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia/o.parquet:   7%|█████████████▊                                                                                                                                                                                               | 10/148 [06:05<3:53:09, 101.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the purpose of an Oncology Information System (OIS)?\",\n",
      "    \"A\": \"To manage departmental, administrative, and clinical activities in cancer care\",\n",
      "    \"B\": \"To track and manage radiology requests and workflow\",\n",
      "    \"C\": \"To manage patient records more generally\",\n",
      "    \"D\": \"To support medical information management in all healthcare departments\",\n",
      "    \"E\": \"To provide financial control and billing services for cancer patients\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The purpose of an Oncology Information System (OIS) is to manage departmental, administrative, and clinical activities in cancer care. The OIS aggregates information into a complete oncology-specific electronic health record and supports the delivery of integrated care and long-term treatment for cancer patients.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/o.parquet:  24%|█████████████████████████████████████████████████▏                                                                                                                                                              | 35/148 [08:03<09:15,  4.91s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 18728 tokens (14728 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/o.parquet:  24%|██████████████████████████████████████████████████▌                                                                                                                                                             | 36/148 [08:04<06:55,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 18728 tokens (14728 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main cause of ocean acidification in the Great Barrier Reef?\",\n",
      "    \"A\": \"Rise in sea surface temperature\",\n",
      "    \"B\": \"Decrease in aragonite levels\",\n",
      "    \"C\": \"Increase in dissolved inorganic carbon\",\n",
      "    \"D\": \"Rise in atmospheric carbon dioxide\",\n",
      "    \"E\": \"Decrease in pH of the ocean\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"The main cause of ocean acidification in the Great Barrier Reef is the rise in atmospheric carbon dioxide, which is taken up by the ocean. This process increases the acidity of the water, leading to a decrease in pH.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/o.parquet:  78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                              | 115/148 [14:33<02:43,  4.95s/it]Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia/o.parquet:  78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                            | 116/148 [19:35<50:07, 93.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"During which period were the coal seams in the Oldham Coalfield laid down?\",\n",
      "    \"A\": \"Carboniferous period\",\n",
      "    \"B\": \"Middle Ages\",\n",
      "    \"C\": \"Industrial Revolution\",\n",
      "    \"D\": \"19th century\",\n",
      "    \"E\": \"20th century\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that the coal seams in the Oldham Coalfield were laid down in the Carboniferous period.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/o.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 148/148 [22:06<00:00,  8.96s/it]\n",
      "../data/wikipedia/other.parquet: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:49<00:00,  4.92s/it]\n",
      "../data/wikipedia/p.parquet:  10%|█████████████████████▊                                                                                                                                                                                          | 53/506 [04:54<41:03,  5.44s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 17776 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/p.parquet:  11%|██████████████████████▏                                                                                                                                                                                         | 54/506 [04:54<29:43,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 17776 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the Pardy Island Formation?\",\n",
      "    \"A\": \"A formation found in Newfoundland\",\n",
      "    \"B\": \"A formation found in Labrador\",\n",
      "    \"C\": \"A formation found in both Newfoundland and Labrador\",\n",
      "    \"D\": \"A formation found in Prince Edward Island\",\n",
      "    \"E\": \"A formation found in Nova Scotia\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text explicitly states that the Pardy Island Formation is a formation cropping out in Newfoundland.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/p.parquet:  11%|███████████████████████▍                                                                                                                                                                                        | 57/506 [05:12<38:41,  5.17s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 17788 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/p.parquet:  11%|███████████████████████▊                                                                                                                                                                                        | 58/506 [05:12<28:53,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 17788 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the pEDA parameter?\",\n",
      "    \"A\": \"A scale used to measure the pi-electron donating ability of a substituent\",\n",
      "    \"B\": \"A scale used to measure the pi-electron withdrawing ability of a substituent\",\n",
      "    \"C\": \"A scale used to measure the resonance effect of a substituent\",\n",
      "    \"D\": \"A scale used to measure the mesomeric effect of a substituent\",\n",
      "    \"E\": \"A scale used to measure the substituent constants\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The pEDA parameter is described as a pi-electron substituent effect scale, specifically measuring the pi-electron donating ability of a substituent. The more positive the value of pEDA, the more pi-electron donating the substituent is.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/p.parquet:  25%|███████████████████████████████████████████████████▌                                                                                                                                                           | 126/506 [11:24<37:57,  5.99s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 10 column 226 (char 539)\n",
      "../data/wikipedia/p.parquet:  25%|███████████████████████████████████████████████████▉                                                                                                                                                           | 127/506 [11:30<38:14,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 10 column 226 (char 539)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the term used to describe an ancestral character shared by all members of a clade, which does not distinguish the clade from other clades?\",\n",
      "    \"A\": \"Plesiomorphy\",\n",
      "    \"B\": \"Symplesiomorphy\",\n",
      "    \"C\": \"Apomorphy\",\n",
      "    \"D\": \"Synapomorphy\",\n",
      "    \"E\": \"Autapomorphy\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The term used to describe an ancestral character shared by all members of a clade, which does not distinguish the clade from other clades is symplesiomorphy. The text states, 'In phylogenetics, a plesiomorphy (\"near form\") and symplesiomorphy are synonyms for an ancestral character shared by all members of a clade, which does not distinguish the clade from other clades.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/p.parquet:  34%|██████████████████████████████████████████████████████████████████████▊                                                                                                                                        | 173/506 [15:32<29:45,  5.36s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 30625 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/p.parquet:  34%|███████████████████████████████████████████████████████████████████████▏                                                                                                                                       | 174/506 [15:33<21:40,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 30625 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "    {\n",
      "        \"prompt\": \"What is the Porter Shale?\",\n",
      "        \"A\": \"A geologic formation in Washington (state)\",\n",
      "        \"B\": \"A fossil dating technique\",\n",
      "        \"C\": \"A Paleogene period artifact\",\n",
      "        \"D\": \"A stratigraphic unit in Washington (state)\",\n",
      "        \"E\": \"A Paleontology study in Washington (state)\",\n",
      "        \"answer\": \"A\",\n",
      "        \"basis\": \"The text explicitly states that the Porter Shale is a geologic formation in Washington (state).\"\n",
      "    }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/p.parquet:  35%|████████████████████████████████████████████████████████████████████████▊                                                                                                                                      | 178/506 [15:51<24:22,  4.46s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 18020 tokens (14020 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/p.parquet:  35%|█████████████████████████████████████████████████████████████████████████▏                                                                                                                                     | 179/506 [15:51<17:34,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 18020 tokens (14020 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the purpose of using potassium fluoride on alumina in organic synthesis?\",\n",
      "    \"A\": \"To induce alkylation reactions\",\n",
      "    \"B\": \"To catalyze oxidation reactions\",\n",
      "    \"C\": \"To promote reduction reactions\",\n",
      "    \"D\": \"To inhibit polymerization reactions\",\n",
      "    \"E\": \"To enhance esterification reactions\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The purpose of using potassium fluoride on alumina in organic synthesis is to induce alkylation reactions. This is mentioned in the given text: 'It is a base which is used in organic synthesis. It was originally introduced in 1979 by Ando et al. for inducing alkylation reactions.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/p.parquet:  38%|██████████████████████████████████████████████████████████████████████████████▉                                                                                                                                | 193/506 [17:03<30:15,  5.80s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16445 tokens (12445 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/p.parquet:  38%|███████████████████████████████████████████████████████████████████████████████▎                                                                                                                               | 194/506 [17:04<21:39,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 16445 tokens (12445 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the paternal age effect?\",\n",
      "    \"A\": \"The statistical relationship between the father's age at conception and biological effects on the child.\",\n",
      "    \"B\": \"The statistical relationship between the mother's age at conception and biological effects on the child.\",\n",
      "    \"C\": \"The statistical relationship between the parents' age at conception and biological effects on the child.\",\n",
      "    \"D\": \"The statistical relationship between the child's age at conception and biological effects on the father.\",\n",
      "    \"E\": \"The statistical relationship between the child's age at conception and biological effects on the mother.\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that the paternal age effect is the statistical relationship between the father's age at conception and biological effects on the child.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/p.parquet:  42%|███████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                       | 214/506 [18:45<25:40,  5.28s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 4 column 26 (char 132)\n",
      "../data/wikipedia/p.parquet:  42%|███████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                       | 215/506 [18:55<32:34,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 4 column 26 (char 132)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the formula for the ponderomotive energy in terms of the laser intensity I?\",\n",
      "    \"A\": \"U_p = {e^2 E^2 \\over 4m \\omega_0^2}\",\n",
      "    \"B\": \"U_p = {e^2 I \\over 2 c \\epsilon_0 m \\omega_0^2}\",\n",
      "    \"C\": \"U_p = {2e^2 \\over c \\epsilon_0 m} \\cdot {I \\over 4\\omega_0^2}\",\n",
      "    \"D\": \"U_p = \\frac{E^2}{4\\omega_0^2}\",\n",
      "    \"E\": \"U_p = 9.33 \\cdot I(10^{14} \\mathrm{W/cm}^2) \\cdot \\lambda(\\mathrm{\\mu m})^2\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"The formula for the ponderomotive energy in terms of the laser intensity I is U_p = {2e^2 \\over c \\epsilon_0 m} \\cdot {I \\over 4\\omega_0^2}. This can be derived from the given equation U_p={e^2 I \\over 2 c \\epsilon_0 m \\omega_0^2} by substituting I=c\\epsilon_0 E^2/2 and simplifying.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/p.parquet:  52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                   | 263/506 [22:54<24:24,  6.03s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 10 column 197 (char 739)\n",
      "../data/wikipedia/p.parquet:  52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                   | 264/506 [22:59<23:20,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 10 column 197 (char 739)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"According to the principle of maximum work, what is the relationship between pure chemical reactions and heat evolution?\",\n",
      "    \"A\": \"Pure chemical reactions are always accompanied by heat evolution.\",\n",
      "    \"B\": \"Pure chemical reactions are sometimes accompanied by heat evolution.\",\n",
      "    \"C\": \"Pure chemical reactions are never accompanied by heat evolution.\",\n",
      "    \"D\": \"Pure chemical reactions are accompanied by heat absorption.\",\n",
      "    \"E\": \"Pure chemical reactions are not affected by heat evolution.\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The principle of maximum work states that every pure chemical reaction is accompanied by the evolution of heat. This is mentioned in the text: 'Berthelot's version was essentially: \"every pure chemical reaction is accompanied by evolution of heat.\"'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/p.parquet:  55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                             | 277/506 [24:00<20:18,  5.32s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17447 tokens (13447 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/p.parquet:  55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                             | 278/506 [24:01<15:20,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 17447 tokens (13447 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is pressure?\",\n",
      "    \"A\": \"The force applied perpendicular to the surface of an object per unit area\",\n",
      "    \"B\": \"The force applied parallel to the surface of an object per unit area\",\n",
      "    \"C\": \"The force applied perpendicular to the surface of an object per unit volume\",\n",
      "    \"D\": \"The force applied parallel to the surface of an object per unit volume\",\n",
      "    \"E\": \"None of the above\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"According to the text, pressure is defined as the force applied perpendicular to the surface of an object per unit area.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/p.parquet:  66%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                     | 336/506 [28:58<14:20,  5.06s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 18808 tokens (14808 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/p.parquet:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                     | 337/506 [28:59<10:15,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 18808 tokens (14808 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "    {\n",
      "        \"prompt\": \"What is the ultimate abstract principle of actual existence according to Whitehead's Process and Reality?\",\n",
      "        \"A\": \"Creativity\",\n",
      "        \"B\": \"Efficient causality\",\n",
      "        \"C\": \"Singular causality\",\n",
      "        \"D\": \"Nomic causality\",\n",
      "        \"E\": \"Novelty\",\n",
      "        \"answer\": \"A\",\n",
      "        \"basis\": \"The ultimate abstract principle of actual existence for Whitehead is creativity. It is manifest in what can be called 'singular causality' and is a process of becoming, a creative advance into novelty.\"\n",
      "    }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/p.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 506/506 [43:02<00:00,  5.10s/it]\n",
      "../data/wikipedia/q.parquet:  40%|████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                            | 17/42 [01:30<02:28,  5.93s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 4 column 14 (char 88)\n",
      "../data/wikipedia/q.parquet:  43%|█████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                       | 18/42 [01:44<03:15,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 4 column 14 (char 88)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the Hamiltonian of the quantum clock model?\",\n",
      "    \"A\": \"-J \\left( \\sum_{ \\langle i, j \\rangle} (Z^\\dagger_i Z_j + Z_i Z^\\dagger_j ) + g \\sum_j (X_j + X^\\dagger_j) \\right)\",\n",
      "    \"B\": \"-J \\left( \\sum_{ \\langle i, j \\rangle} (Z^\\dagger_i Z_j + Z_i Z^\\dagger_j ) - g \\sum_j (X_j + X^\\dagger_j) \\right)\",\n",
      "    \"C\": \"-J \\left( \\sum_{ \\langle i, j \\rangle} (Z^\\dagger_i Z_j - Z_i Z^\\dagger_j ) + g \\sum_j (X_j + X^\\dagger_j) \\right)\",\n",
      "    \"D\": \"-J \\left( \\sum_{ \\langle i, j \\rangle} (Z^\\dagger_i Z_j - Z_i Z^\\dagger_j ) - g \\sum_j (X_j + X^\\dagger_j) \\right)\",\n",
      "    \"E\": \"-J \\left( \\sum_{ \\langle i, j \\rangle} (Z^\\dagger_i Z_j - Z_i Z^\\dagger_j ) - g \\sum_j (X_j - X^\\dagger_j) \\right)\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The Hamiltonian of the quantum clock model is given by H = -J \\left( \\sum_{ \\langle i, j \\rangle} (Z^\\dagger_i Z_j + Z_i Z^\\dagger_j ) + g \\sum_j (X_j + X^\\dagger_j) \\right). This is stated in the text: 'The Hamiltonian of this model is :H = -J \\left( \\sum_{ \\langle i, j \\rangle} (Z^\\dagger_i Z_j + Z_i Z^\\dagger_j ) + g \\sum_j (X_j + X^\\dagger_j) \\right).'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/q.parquet:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████               | 39/42 [03:32<00:13,  4.66s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 16 (char 131)\n",
      "../data/wikipedia/q.parquet:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████          | 40/42 [03:38<00:09,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 5 column 16 (char 131)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the conductance of a quantum point contact quantized in units of?\",\n",
      "    \"A\": \"2e^2/h\",\n",
      "    \"B\": \"2e^2/\\hbar\",\n",
      "    \"C\": \"2e^2\",\n",
      "    \"D\": \"2h\",\n",
      "    \"E\": \"2e\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The conductance of a quantum point contact is quantized in units of 2e^2/h, as mentioned in the text: 'The conductance of a QPC is quantized in units of 2e^2/h, the so-called conductance quantum.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/q.parquet: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [03:50<00:00,  5.49s/it]\n",
      "../data/wikipedia/r.parquet:  46%|██████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                | 155/338 [12:19<11:45,  3.86s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 64369 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/r.parquet:  46%|███████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                               | 156/338 [12:21<09:13,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 64369 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is Richard Hammond's profession?\",\n",
      "    \"A\": \"Chemist\",\n",
      "    \"B\": \"Biologist\",\n",
      "    \"C\": \"Physicist\",\n",
      "    \"D\": \"Astronomer\",\n",
      "    \"E\": \"Geologist\",\n",
      "    \"answer\": \"C\",\n",
      "    \"basis\": \"The text states that Richard Hammond is a theoretical physicist and works for the United States Army Research Laboratory.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/r.parquet:  61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                | 207/338 [16:23<11:44,  5.38s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 48632 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/r.parquet:  62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                               | 208/338 [16:25<09:12,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 48632 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What are the two chemical reactions discovered by Robin Ferrier?\",\n",
      "    \"A\": \"Ferrier rearrangement and Ferrier carbocyclization\",\n",
      "    \"B\": \"Ferrier glycosidation and Ferrier carbocyclization\",\n",
      "    \"C\": \"Ferrier rearrangement and Fischer glycosidation\",\n",
      "    \"D\": \"Ferrier rearrangement and Calvin glycosidation\",\n",
      "    \"E\": \"Ferrier glycosidation and Calvin rearrangement\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that Robin Ferrier discovered two chemical reactions, the Ferrier rearrangement and the Ferrier carbocyclization.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16777 tokens (12777 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "\r",
      "../data/wikipedia/r.parquet:  62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                               | 209/338 [16:26<06:48,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 16777 tokens (12777 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What are the two chemical reactions discovered by Robin Ferrier?\",\n",
      "    \"A\": \"Ferrier rearrangement and Ferrier carbocyclization\",\n",
      "    \"B\": \"Ferrier glycosidation and Ferrier carbocyclization\",\n",
      "    \"C\": \"Ferrier rearrangement and Fischer glycosidation\",\n",
      "    \"D\": \"Ferrier rearrangement and Calvin glycosidation\",\n",
      "    \"E\": \"Ferrier glycosidation and Calvin rearrangement\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that Robin Ferrier discovered two chemical reactions, the Ferrier rearrangement and the Ferrier carbocyclization.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/r.parquet:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 300/338 [23:43<03:11,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 5 column 23 (char 129)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is a regular numerical predicate?\",\n",
      "    \"A\": \"A kind of relation over integers\",\n",
      "    \"B\": \"A subset of \\mathbb N^r for some arity r\",\n",
      "    \"C\": \"A formal language that is regular\",\n",
      "    \"D\": \"A monadic second order formula\",\n",
      "    \"E\": \"A first order logic formula\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"According to the given text, a regular numerical predicate is defined as a subset of \\mathbb N^r for some arity r.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 23 (char 129)\n",
      "../data/wikipedia/r.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 338/338 [26:44<00:00,  4.75s/it]\n",
      "../data/wikipedia/s.parquet:   4%|████████▋                                                                                                                                                                                                       | 26/621 [02:08<45:35,  4.60s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 5 column 31 (char 177)\n",
      "../data/wikipedia/s.parquet:   4%|█████████                                                                                                                                                                                                       | 27/621 [02:13<46:29,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 5 column 31 (char 177)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the stopping rule in the Sequential Probability Ratio Test (SPRT)?\",\n",
      "    \"A\": \"Continue monitoring when a < S_i < b\",\n",
      "    \"B\": \"Accept H_1 when S_i \\geq b\",\n",
      "    \"C\": \"Accept H_0 when S_i \\leq a\",\n",
      "    \"D\": \"Both B and C\",\n",
      "    \"E\": \"None of the above\",\n",
      "    \"answer\": \"D\",\n",
      "    \"basis\": \"The stopping rule in SPRT is that if S_i \\geq b, H_1 is accepted, and if S_i \\leq a, H_0 is accepted. Therefore, the correct answer is both B and C.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/s.parquet:  22%|█████████████████████████████████████████████                                                                                                                                                                  | 135/621 [10:53<46:19,  5.72s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 17554 tokens (13554 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/s.parquet:  22%|█████████████████████████████████████████████▎                                                                                                                                                                 | 136/621 [10:54<35:23,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 17554 tokens (13554 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is a SNPSTR?\",\n",
      "    \"A\": \"A compound genetic marker composed of one or more SNPs and one microsatellite (STR)\",\n",
      "    \"B\": \"A database that contains all SNPSTRs in five model genomes\",\n",
      "    \"C\": \"A type of genetic marker suitable for drawing population genetic inferences\",\n",
      "    \"D\": \"A combination of single nucleotide polymorphisms (SNPs) and microsatellites (STRs)\",\n",
      "    \"E\": \"A type of repetitive DNA sequence\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that a SNPSTR is a compound genetic marker composed of one or more SNPs and one microsatellite (STR).\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/s.parquet:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                   | 372/621 [29:58<18:12,  4.39s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 10 column 123 (char 465)\n",
      "../data/wikipedia/s.parquet:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                  | 373/621 [30:06<22:38,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 10 column 123 (char 465)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the electronic Hamiltonian in atomic units for the 'spherium' model?\",\n",
      "    \"A\": \"-(abla_1^2/2) - (abla_2^2/2) + (1/u)\",\n",
      "    \"B\": \"-(abla_1^2/2) + (abla_2^2/2) + (1/u)\",\n",
      "    \"C\": \"(abla_1^2/2) - (abla_2^2/2) + (1/u)\",\n",
      "    \"D\": \"(abla_1^2/2) + (abla_2^2/2) + (1/u)\",\n",
      "    \"E\": \"None of the above\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The given text states that the electronic Hamiltonian in atomic units for the 'spherium' model is given by: \\hat{H} = -(abla_1^2/2) - (abla_2^2/2) + (1/u).\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/s.parquet:  78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                             | 486/621 [39:10<10:17,  4.58s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 10 column 1294 (char 1772)\n",
      "../data/wikipedia/s.parquet:  78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                            | 487/621 [39:24<16:55,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 10 column 1294 (char 1772)\n",
      "[\n",
      "    {\n",
      "        \"prompt\": \"What is the purpose of the water boundary layer in pan frying?\",\n",
      "        \"A\": \"To prevent the proteins on the surface of the meat from binding with the oil or pan surface\",\n",
      "        \"B\": \"To increase the thermal conductivity of the pan\",\n",
      "        \"C\": \"To enhance the flavor of the food\",\n",
      "        \"D\": \"To create a non-stick surface for easy cleaning\",\n",
      "        \"E\": \"To increase the heat transfer between the burner and the food\",\n",
      "        \"answer\": \"A\",\n",
      "        \"basis\": \"In pan frying, the water that exits the meat forms a barrier between the meat and the oil or the surface of the pan. This barrier is critical for the success of pan frying meat. When meat cooks, the proteins on the surface of the meat denature because of the heat. This means that many of the secondary bonds that give the proteins their shape are broken. The protein molecules want to reform those interactions to return to their most thermodynamically stable state. Two opportune locations for the surface proteins to bind are the oil and the surface of the pan. Meat sticking to the bottom of the pan is caused by the interactions between proteins on the surface of the meat binding with the molecules on the surface of the pan. The denatured protein can also bind with the oil in the pan. This is not desirable for many health and flavor reasons. Since the proteins, the oil molecules, and in some cases the surface of the pan, all have a significant polarity, the force of their interactions can be high. The presence of water reduces the force of these attractions in three ways. The water puts physical distance between the oil or pan and the proteins on the surface of the meat. This increases the D^2 value in the equation. Water also has a high permittivity value (\\varepsilon_r). Both of these increase the value of the denominator and reduce the value of the force that is possible. Water is also a polar molecule which means, in certain cases, it can bind to the denatured proteins. Water binding to the proteins on the surface of the meat has no effect on how the meat cooks.\"\n",
      "    }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/s.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 621/621 [50:51<00:00,  4.91s/it]\n",
      "../data/wikipedia/t.parquet:  76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 276/365 [24:48<07:31,  5.07s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 19543 tokens (15543 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/t.parquet:  76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                  | 277/365 [24:48<05:24,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 19543 tokens (15543 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"According to Lee Smolin's book, 'The Life of the Cosmos', what principle does he apply to the birth of universes?\",\n",
      "    \"A\": \"Natural selection\",\n",
      "    \"B\": \"Quantum mechanics\",\n",
      "    \"C\": \"Black hole collapse\",\n",
      "    \"D\": \"Heat death\",\n",
      "    \"E\": \"Darwinian selective pressures\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"In the book, Smolin details his Fecund universes which applies the principle of natural selection to the birth of universes.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/t.parquet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 365/365 [32:59<00:00,  5.42s/it]\n",
      "../data/wikipedia/u.parquet: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [06:29<00:00,  5.56s/it]\n",
      "../data/wikipedia/v.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [11:32<00:00,  5.68s/it]\n",
      "../data/wikipedia/w.parquet:  93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 155/167 [13:58<01:00,  5.03s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 43, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 10 column 222 (char 411)\n",
      "../data/wikipedia/w.parquet:  93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 156/167 [14:05<00:59,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 10 column 222 (char 411)\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"When was the Woods Hole Oceanographic Institution established?\",\n",
      "    \"A\": \"1927\",\n",
      "    \"B\": \"1930\",\n",
      "    \"C\": \"1950\",\n",
      "    \"D\": \"1977\",\n",
      "    \"E\": \"1985\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"The Woods Hole Oceanographic Institution was established in 1930, as mentioned in the text: 'The committee's recommendation for establishing a permanent independent research laboratory on the East Coast to \"prosecute oceanography in all its branches\" led to the founding in 1930 of the Woods Hole Oceanographic Institution.'\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/w.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [15:09<00:00,  5.44s/it]\n",
      "../data/wikipedia/x.parquet: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [01:11<00:00,  5.14s/it]\n",
      "../data/wikipedia/y.parquet:   3%|█████▌                                                                                                                                                                                                            | 1/38 [00:07<04:26,  7.21s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 16953 tokens (12953 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia/y.parquet:   5%|███████████                                                                                                                                                                                                       | 2/38 [00:07<01:56,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, you requested 16953 tokens (12953 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Where did Yash Pal study for his BSc Honours in Physics?\",\n",
      "    \"A\": \"Tata Institute of Fundamental Research\",\n",
      "    \"B\": \"University of the Punjab\",\n",
      "    \"C\": \"Delhi University\",\n",
      "    \"D\": \"Panjab University\",\n",
      "    \"E\": \"Massachusetts Institute of Technology\",\n",
      "    \"answer\": \"B\",\n",
      "    \"basis\": \"According to the text, Yash Pal studied for his BSc Honours in Physics at the Lahore campus of the undivided University of the Punjab.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/y.parquet: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [03:17<00:00,  5.19s/it]\n",
      "../data/wikipedia/z.parquet:  45%|██████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                  | 14/31 [01:10<01:18,  4.63s/it]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41623/2705274036.py\", line 42, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_41623/1456674105.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 17178 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia/z.parquet:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                           | 15/31 [01:11<00:55,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 17178 tokens. Please reduce the length of the messages.\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"What is the main purpose of a zenith camera?\",\n",
      "    \"A\": \"To observe Earth's gravity field\",\n",
      "    \"B\": \"To capture images of stars near the zenith\",\n",
      "    \"C\": \"To measure the tilt of the telescope axis\",\n",
      "    \"D\": \"To determine the geodetic coordinates\",\n",
      "    \"E\": \"To track and scan celestial objects\",\n",
      "    \"answer\": \"A\",\n",
      "    \"basis\": \"The text states that a zenith camera is used primarily for the local surveys of Earth's gravity field.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia/z.parquet: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [02:34<00:00,  4.98s/it]\n"
     ]
    }
   ],
   "source": [
    "import traceback \n",
    "batch_size = 1\n",
    "\n",
    "def make_prompt(series):\n",
    "    prompt = f\"\"\"\n",
    "Using the given text about science, create multiple questions based on scientific principles. Also, extract the evidence for your answer.\n",
    "The output should be an array of json format, with \"prompt\" as the question statement, \"A,\" \"B,\" \"C,\" \"D,\" and \"E\" as choices, \"answer\" as the answer choice (one of A through E), and \"basis\" as the rationale.\n",
    "\n",
    "# text\n",
    "title: {series['title']}\n",
    "\n",
    "{series['text']}\n",
    "\n",
    "# attention\n",
    "- create 1 question.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def f(series):\n",
    "    if series[\"A\"] != series[\"A\"]:\n",
    "        if type(series[\"choices\"]) == dict:\n",
    "            for key in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "                series[key] = series[\"choices\"][key]\n",
    "        elif type(series[\"choices\"] == list):\n",
    "            for i, key in enumerate([\"A\", \"B\", \"C\", \"D\", \"E\"]):\n",
    "                series[key] = series[\"choices\"][i]\n",
    "    return series\n",
    "\n",
    "now_date = dt.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "first = True\n",
    "for file in files:\n",
    "    if os.path.basename(file) in [\"all.parquet\"]:\n",
    "        print(f\"pass: {file}\")\n",
    "        continue\n",
    "    df_science = get_df(file)\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(df_science)), desc=file):\n",
    "        try:\n",
    "            series = df_science.iloc[i]\n",
    "            prompt = make_prompt(series)\n",
    "            text = query_prompt(prompt)\n",
    "            texts_json = json.loads(text)\n",
    "            if first:\n",
    "                print(texts_json)\n",
    "                first = False\n",
    "            if type(texts_json) == dict:\n",
    "                text_json[\"wiki_id\"] = series[\"id\"]\n",
    "                text_json[\"original_text\"] = series[\"text\"]\n",
    "                texts.append(text_json)\n",
    "            else:\n",
    "                for text_json in texts_json:\n",
    "                    text_json[\"wiki_id\"] = series[\"id\"]\n",
    "                    text_json[\"original_text\"] = series[\"text\"]\n",
    "                    texts.append(text_json)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            traceback.print_exc()\n",
    "            print(text)\n",
    "        if i % 20 == 0:\n",
    "            df_texts = pd.DataFrame(texts)\n",
    "            df_texts = df_texts.apply(f, axis=1)\n",
    "\n",
    "            df_texts.to_csv(f\"output_gpt3.5_generate/{now_date}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_texts.to_csv(f\"output_gpt3.5_generate/{now_date}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
