{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../apikey/apikey.txt\", \"r\") as f:\n",
    "    openai.api_key = f.readline().replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_prompt(prompt, max_tokens=1000):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(fname):\n",
    "    def f(categories):\n",
    "        for cat in categories:\n",
    "            for word in [\n",
    "                \"geology\",\n",
    "                \"physics\",\n",
    "                \"chemistry\",\n",
    "                \"mathematical\",\n",
    "                \"biology\",\n",
    "                \"astronomy\",\n",
    "                \"ecology\",\n",
    "                \"genetics\",\n",
    "                \"statistics\",\n",
    "                \"theoretical\"\n",
    "            ]:\n",
    "                if word.lower() in cat.lower():\n",
    "                    return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def text_preprocess(text):\n",
    "        return text.replace(\"====\", \"\\n\\n\").replace(\"===\", \"\\n\\n\").replace(\"==\", \"\\n\\n\")\n",
    "\n",
    "    def sep_n(text, n=80):\n",
    "        try:\n",
    "            text = text.split(\".\")\n",
    "            return \".\".join(text[:n])\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    df = pd.read_parquet(fname)\n",
    "    df_science = df[df[\"category\"].apply(f)]\n",
    "    df_science[\"text\"] = df_science[\"text\"].apply(text_preprocess)\n",
    "    df_science[\"text\"] = df_science[\"text\"].apply(sep_n)\n",
    "    df_science = df_science[df_science[\"text\"].notnull()]\n",
    "    return df_science.sample(len(df_science)//5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../data/wikipedia_fixed/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia_fixed/a.parquet:   0%|                                           | 1/614 [00:05<59:47,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- text ------------------: \n",
      "Accelerated solvent extraction (ASE) is a method for extracting various chemicals from a complex solid or semisolid sample matrix. The process uses high temperature and pressure, which results in the extraction taking less time and requiring less solvent, and possibly also giving better analyte recovery, than traditional methods that use less extreme conditions. The elevated temperature is employed to increase extraction efficiency of the analyte of interest and the elevated pressure is used to keep the solvent in a liquid state as the temperature is increased above its boiling point. An automated system for the process was developed by Dionex, a company owned by Thermo Fisher Scientific. The extraction cell is filled with the solid sample to be examined and placed in a temperature-controllable oven. After adding the solvent, the cell is heated at constant pressure (adjustable between 0.3 and 20 MPa) up to a maximum temperature of 200°C and kept at constant conditions for a while so that equilibrium can be established. The extract is then transferred to a sample tube. A sample often goes through several extraction cycles. Finally, the extraction cell is rinsed with solvent, the rinsing valve is opened and the cell and all lines are rinsed with nitrogen and the apparatus is prepared for further extractions. Accelerated solvent extraction has found many applications in the food industry, including in: The testing of dietary seafoods for arsenic content Extraction of catechins from green tea samples Authentication of natural vanilla flavors The analysis of terpenoids and sterols in tobacco Supercritical fluid extraction Superheated water Richter, Bruce E.; Jones, Brian A.; Ezzell, John L.; Porter, Nathan L.; Avdalovic, Nebojsa; Pohl, Chris (1996). \"Accelerated Solvent Extraction: A Technique for Sample Preparation\". Anal. Chem. 68 (6): 1033–1039. doi:10.1021/ac9508199. Murphy, B.J.; Carlson, R.E.; Peterson, J.H.; Richter, Bruce E. (December 2, 2007). \"Accelerated Solvent Extraction: Sample Preparation for Determination of Brominated Flame Retardants\". Retrieved 2016-05-09. \"Accelerated Solvent Extraction - Dionex Solvent Extractors\". analytica-world.com. Retrieved 11 September 2020. \"Accelerated Solvent Extraction (ASE) - US\". Thermo Fisher Scientific. 25 February 2019. Retrieved 11 September 2020. \"Präanalytische Methoden\". Instrumentelle Analytik und Bioanalytik. Springer-Lehrbuch (in German). Berlin, Heidelberg: Springer Berlin Heidelberg. 2008. pp. 57–90. doi:10.1007/978-3-540-73804-6_3. ISBN 978-3-540-73803-9. ISSN 0937-7433. Gallagher, P. A.; Shoemaker, J. A.; Wei, Xinyi; Brockhoff-Schwegel, C. A.; Creed, J. T. (8 January 2001). \"Extraction and detection of arsenicals in seaweed via accelerated solvent extraction with ion chromatographic separation and ICP-MS detection\". Fresenius' Journal of Analytical Chemistry. Springer Science and Business Media LLC. 369 (1): 71–80. doi:10.1007/s002160000585. ISSN 0937-0633. PMID 11210234. S2CID 46609876. Kellogg, Joshua J.; Wallace, Emily D.; Graf, Tyler N.; Oberlies, Nicholas H.; Cech, Nadja B. (2017). \"Conventional and accelerated-solvent extractions of green tea (camellia sinensis) for metabolomics-based chemometrics\". Journal of Pharmaceutical and Biomedical Analysis. Elsevier BV. 145: 604–610. doi:10.1016/j\n",
      "--------------- json ------------------: \n",
      "{'prompt': 'What is the purpose of using high temperature and pressure in accelerated solvent extraction?', 'answer': 'C', 'A': 'To decrease extraction efficiency of the analyte of interest', 'B': 'To increase the amount of solvent required for extraction', 'C': 'To increase extraction efficiency of the analyte of interest', 'D': 'To decrease the boiling point of the solvent', 'E': 'To decrease the time required for equilibrium to be established'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data/wikipedia_fixed/a.parquet:   5%|█▉                                      | 30/614 [03:34<1:03:04,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the theory of animal magnetism?\", \"answer\": \"C\", \"A\": \"Animal magnetism is a theory that posits the existence of an invisible natural force possessed by all living things.\", \"B\": \"Animal magnetism is a theory that claims the force could have physical effects, including healing.\", \"C\": \"Animal magnetism is a theory invented by German doctor Franz Mesmer in the 18th century.\", \"D\": \"Animal magnetism is a theory that attracted numerous followers in Europe and the United States.\", \"E\": \"Animal magnetism is a theory that is no longer practiced today except as a form of alternative medicine in some places.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/a.parquet:   9%|███▌                                    | 54/614 [06:23<1:02:29,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4615 tokens (3615 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is Alberto Vecchio's current position and affiliation?\", \"answer\": \"B\", \"A\": \"Physicist at the University of Milan\", \"B\": \"Professor at University of Birmingham in the School of Physics and Astronomy and Director of the Institute of Gravitational Wave Astronomy\", \"C\": \"Contributor in the upgrade of LIGO to Advanced LIGO\", \"D\": \"Graduate of Ghislieri College and the University of Pavia\", \"E\": \"Collaborator with Bruno Bertotti\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4615 tokens (3615 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/a.parquet:  10%|████▏                                   | 64/614 [07:47<1:25:30,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4690 tokens (3690 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the Andersen thermostat and how does it work?\", \"answer\": \"C\", \"A\": \"The Andersen thermostat is a proposal in molecular dynamics simulation that maintains constant temperature conditions by adjusting the velocity of a chosen atom or molecule based on Maxwell–Boltzmann statistics.\", \"B\": \"The Andersen thermostat is a proposal in molecular dynamics simulation that maintains constant pressure conditions by adjusting the velocity of a chosen atom or molecule based on Maxwell–Boltzmann statistics.\", \"C\": \"The Andersen thermostat is a proposal in molecular dynamics simulation that maintains constant temperature conditions by adjusting the velocity of a chosen atom or molecule based on Maxwell–Boltzmann statistics for the given temperature.\", \"D\": \"The Andersen thermostat is a proposal in molecular dynamics simulation that maintains constant temperature conditions by adjusting the position of a chosen atom or molecule based on Maxwell–Boltzmann statistics for the given temperature.\", \"E\": \"The Andersen thermostat is a proposal in molecular dynamics simulation that maintains constant pressure conditions by adjusting the position of a chosen atom or molecule based on Maxwell–Boltzmann statistics for the given temperature.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4690 tokens (3690 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/a.parquet:  27%|██████████▉                              | 163/614 [18:56<52:00,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the purpose of the Arizona Geological Society?\", \"answer\": \"B\", \"A\": \"To publish geologic maps and guidebooks\", \"B\": \"To promote and encourage interest in the science of geology and in the geology of the State of Arizona\", \"C\": \"To provide scholarships for graduate students studying field geology projects\", \"D\": \"To organize symposia on ore deposits and structural geology\", \"E\": \"To lead field trips for professional geologists\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/a.parquet:  30%|████████████▎                            | 184/614 [26:59<50:11,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4682 tokens (3682 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes androgen conjugates?\", \"answer\": \"C\", \"A\": \"Androgen conjugates are hormonally active and bind to the androgen receptor.\", \"B\": \"Androgen conjugates are formed by steroid sulfatase and glucuronosyltransferase enzymes.\", \"C\": \"Androgen conjugates do not bind to the androgen receptor and are hormonally inactive.\", \"D\": \"Androgen conjugates can only be converted back into active androgens through sulfotransferase enzymes.\", \"E\": \"Androgen conjugates are primarily conjugated at the C3 and/or C17β positions.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4682 tokens (3682 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/a.parquet:  52%|█████████████████████▎                   | 319/614 [42:26<34:43,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 1 column 508 (char 507)\n",
      "{\"prompt\": \"What is the Altiplano–Puna volcanic complex?\", \"answer\": \"C\", \"A\": \"A complex of volcanic systems located in the Puna of the Andes\", \"B\": \"A highland bounded by the Bolivian Cordillera Real and the Western Cordillera\", \"C\": \"A volcanic complex resulting from the subduction of the Nazca Plate beneath the South American Plate\", \"D\": \"A volcanic province located between 21° S–24° S latitude\", \"E\": \"A surface plateau 300 kilometres wide and 2,000 kilometres long at an altitude of 4,000 metres\"},\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 340, in decode\n",
      "    raise JSONDecodeError(\"Extra data\", s, end)\n",
      "json.decoder.JSONDecodeError: Extra data: line 1 column 508 (char 507)\n",
      "../data/wikipedia_fixed/a.parquet:  61%|████████████████████████▉                | 373/614 [48:46<31:15,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4320 tokens (3320 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is Artemis Corona?\", \"answer\": \"C\", \"A\": \"A corona found in the Aphrodite Terra continent on the planet Venus.\", \"B\": \"The largest corona on Venus with a diameter of 2,600 kilometers.\", \"C\": \"A corona on Venus that is largely enclosed by the near circular Artemis Chasma and is believed to be the site of plate tectonics operating on a regional scale.\", \"D\": \"A corona on Venus that is elevated like other coronae.\", \"E\": \"A corona on Venus that has grabens and compressional arcs which rise above the surrounding plains.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4320 tokens (3320 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/a.parquet:  84%|████████████████████████████████▊      | 517/614 [1:05:36<38:07, 23.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4589 tokens (3589 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is Adisa Azapagic's academic background and current position?\", \"answer\": \"B\", \"A\": \"Adisa Azapagic has a bachelor's degree in Chemical Engineering and is currently a professor at the University of Manchester.\", \"B\": \"Adisa Azapagic has a bachelor's degree in Chemical Engineering, completed her doctoral studies at the University of Surrey, and is currently a professor at the University of Manchester.\", \"C\": \"Adisa Azapagic has a bachelor's degree in Chemical Engineering and completed her doctoral studies at the University of Manchester. She is currently a professor at the University of Surrey.\", \"D\": \"Adisa Azapagic has a bachelor's degree in Chemical Engineering and completed her doctoral studies at the University of Surrey. She is currently a professor at the University of Surrey.\", \"E\": \"Adisa Azapagic has a bachelor's degree in Chemical Engineering and completed her doctoral studies at the University of Manchester. She is currently a professor at the University of Manchester.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4589 tokens (3589 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/a.parquet:  86%|█████████████████████████████████▋     | 531/614 [1:07:17<10:06,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4670 tokens (3670 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the process of abortive initiation in genetic transcription?\", \"answer\": \"C\", \"A\": \"RNA polymerase escapes the promoter and enters into the elongation step of transcription\", \"B\": \"RNA polymerase binds to promoter DNA to form an RNA polymerase-promoter closed complex\", \"C\": \"RNA polymerase enters into abortive cycles of synthesis and releases short RNA products\", \"D\": \"RNA polymerase re-winds and ejects the downstream portion of the unwound DNA, releasing the RNA\", \"E\": \"RNA polymerase remains stationary while it unwinds and pulls downstream DNA into the transcription complex to pass the nucleotides through the polymerase active site\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4670 tokens (3670 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/a.parquet:  93%|████████████████████████████████████▎  | 572/614 [1:11:59<04:32,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the purpose of using aluminon in qualitative inorganic analysis?\", \"answer\": \"C\", \"A\": \"To detect the presence of sodium nitrite in an aqueous solution\", \"B\": \"To form a red complex salt with sodium nitrite\", \"C\": \"To detect the presence of the aluminium ion in an aqueous solution\", \"D\": \"To produce lake pigments with many metals\", \"E\": \"To change the color of a particular pigment in acidic solutions\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/a.parquet:  96%|█████████████████████████████████████▎ | 587/614 [1:19:05<02:46,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4958 tokens (3958 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements is true about asexual reproduction in starfish?\", \"answer\": \"B\", \"A\": \"Asexual reproduction in starfish occurs only through fission and autotomy.\", \"B\": \"Fissiparity in starfish is limited to certain species such as Coscinasterias, Stephanasterias, and Sclerasterias.\", \"C\": \"Asterinidae is the only family in which asexual reproduction by fission has been observed.\", \"D\": \"Fission in starfish occurs throughout the year, but is more common during the winter.\", \"E\": \"Asexual reproduction in starfish is correlated with the presence of multiple madreporites.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4958 tokens (3958 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/a.parquet:  99%|██████████████████████████████████████▋| 609/614 [1:22:02<00:39,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"Which of the following statements is true about the Avian HBV RNA encapsidation signal epsilon (AHBV epsilon)?\", \"answer\": \"C\", \"A\": \"DHBV epsilon has more base pairing in the upper stem than HHBV epsilon.\", \"B\": \"HHBV epsilon is used as a model to understand human Hepatitis B virus.\", \"C\": \"Recent NMR analysis revealed that both HHBV and DHBV epsilon have a stable well-structured UGUU tetraloop at the apical loop.\", \"D\": \"The RNA structure of HHBV epsilon was determined by chemical probing, NMR analysis, and mutagenesis.\", \"E\": \"The Rfam database contains multiple covariation models for AHBV epsilon.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/a.parquet: 100%|███████████████████████████████████████| 614/614 [1:28:01<00:00,  8.60s/it]\n",
      "../data/wikipedia_fixed/b.parquet:   0%| | 0/461 [00:00<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"Which of the following statements accurately describes annulate lamella?\", \"answer\": \"C\", \"A\": \"Annulate lamella is found only in oocytes and spermatocytes.\", \"B\": \"Annulate lamella is a type of cell membrane that is irregular and short-lived.\", \"C\": \"Annulate lamella is a set of parallel elements with double-walled membranes, similar to the nuclear envelope, and is characteristic of actively growing cells.\", \"D\": \"Annulate lamella is formed from the cytoplasmic membrane.\", \"E\": \"Annulate lamella is not involved in genetic information transfer and storage.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/b.parquet:  10%| | 44/461 [09:25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4680 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What are some of the challenges raised by the advent of biobanks?\", \"answer\": \"D\", \"A\": \"The lack of storage facilities for biobanks\", \"B\": \"The difficulty in obtaining consent for future research purposes\", \"C\": \"The limited availability of genetic data for researchers\", \"D\": \"Ethical, legal, and social issues pertaining to their existence\", \"E\": \"The high cost of collecting and storing biological samples\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4680 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/b.parquet:  10%| | 45/461 [09:36"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 1 column 316 (char 315)\n",
      "{\"prompt\": \"When was the British Geological Survey founded and what was its original name?\", \"answer\": \"B\", \"A\": \"1835, Geological Survey of Scotland\", \"B\": \"1835, Geological Survey of Great Britain\", \"C\": \"1965, Institute of Geological Sciences\", \"D\": \"1984, British Geological Survey\", \"E\": \"2019, Karen Hanghøj\"},\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 340, in decode\n",
      "    raise JSONDecodeError(\"Extra data\", s, end)\n",
      "json.decoder.JSONDecodeError: Extra data: line 1 column 316 (char 315)\n",
      "../data/wikipedia_fixed/b.parquet:  23%|▏| 105/461 [15:5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6283 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Bacton Group?\", \"answer\": \"C\", \"A\": \"The Bacton Group is a sequence of rock strata found in northeast England.\", \"B\": \"The Bacton Group is a Triassic lithostratigraphic group located in Bacton on the Norfolk coast.\", \"C\": \"The Bacton Group is the offshore equivalent of the Roxby Formation and Sherwood Sandstone Group.\", \"D\": \"The Bacton Group comprises a lower Bunter Shale Formation of red and brown to grey mudstones.\", \"E\": \"The Bacton Group is primarily composed of dolomitic sandstones with mudstone interlayers.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6283 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/b.parquet:  44%|▍| 202/461 [24:2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4877 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the purpose of bacterial oxidation in the BIOX® process?\", \"answer\": \"C\", \"A\": \"To remove organic substances from the ore or concentrate\", \"B\": \"To produce iron(III) sulfate and sulfuric acid\", \"C\": \"To liberate occluded gold for subsequent recovery via cyanidation\", \"D\": \"To create a suitable environment for the growth of other microbes\", \"E\": \"To neutralize the arsenic produced during the oxidation process\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4877 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/b.parquet:  61%|▌| 282/461 [32:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4342 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the primary food source for honeybees and how is it stored?\", \"answer\": \"C\", \"A\": \"Bee pollen is the primary food source for honeybees and it is stored in brood cells.\", \"B\": \"Bee pollen is the primary food source for honeybees and it is stored in open cells located at the interface between the brood and stored honey.\", \"C\": \"Bee pollen is the primary food source for honeybees and it is stored in brood cells, mixed with saliva, and sealed with a drop of honey.\", \"D\": \"Bee pollen is the primary food source for honeybees and it is stored in chambers of the hives.\", \"E\": \"Bee pollen is the primary food source for honeybees and it is stored in brood cells, mixed with nectar and bee salivary secretions.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4342 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/b.parquet:  68%|▋| 312/461 [34:4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4896 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the significance of Babylonian astronomy in the development of Western astronomy?\", \"answer\": \"D\", \"A\": \"Babylonian astronomy had no influence on the development of Western astronomy.\", \"B\": \"Babylonian astronomy was the first scientific revolution in the field of astronomy.\", \"C\": \"Babylonian astronomy focused primarily on astrology and divination, rather than scientific observation.\", \"D\": \"Babylonian astronomy provided the foundation for subsequent scientific astronomy in the Hellenistic world, India, Islam, and the West.\", \"E\": \"Babylonian astronomy was solely concerned with the recording of celestial phenomena, without any mathematical or theoretical contributions.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4896 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/b.parquet:  68%|▋| 315/461 [35:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 1 column 277 (char 276)\n",
      "{\"prompt\": \"What is the Bogoliubov causality condition in axiomatic quantum field theory?\", \"answer\": \"C\", \"A\": \"The Bogoliubov causality condition states that the scattering matrix (S-matrix) in axiomatic quantum field theory is a functional of a function g : M → [ 0 , 1 ] {\\displaystyle g:M\\to [0,1]} defined on the Minkowski space M {\\displaystyle M} , which characterizes the intensity of the interaction in different space-time regions.\", \"B\": \"The Bogoliubov causality condition states that the scattering matrix (S-matrix) in axiomatic quantum field theory is a functional of a function g : M → [ 0 , 1 ] {\\displaystyle g:M\\to [0,1]} defined on the Minkowski space M {\\displaystyle M} , which characterizes the absence of interaction in a point x {\\displaystyle x} when g ( x ) = 0 {\\displaystyle g(x)=0} .\", \"C\": \"The Bogoliubov causality condition in axiomatic quantum field theory, as formulated by Nikolay Bogolyubov in 1955, states that for two points x , y ∈ M {\\displaystyle x,y\\in M} , x ≤ y {\\displaystyle x\\leq y} implies that the variational derivatives of the scattering matrix S ( g ) {\\displaystyle S(g)} with respect to the function g ( x ) {\\displaystyle g(x)} and g ( y ) {\\displaystyle g(y)} , multiplied by the adjoint of the scattering matrix S † ( g ) {\\displaystyle S^{\\dagger }(g)} , must be equal to zero.\", \"D\": \"The Bogoliubov causality condition states that the scattering matrix (S-matrix) in axiomatic quantum field theory is a functional of a function g : M → [ 0 , 1 ] {\\displaystyle g:M\\to [0,1]} defined on the Minkowski space M {\\displaystyle M} , which characterizes the most intense interaction when g ( x ) = 1 {\\displaystyle g(x)=1} .\", \"E\": \"The Bogoliubov causality condition states that the scattering matrix (S-matrix) in axiomatic quantum field theory is a functional of a function g : M → [ 0 , 1 ] {\\displaystyle g:M\\to [0,1]} defined on the Minkowski space M {\\displaystyle M} , which characterizes incomplete interaction at a point x {\\displaystyle x} when g ( x ) {\\displaystyle g(x)} is between 0 and 1.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 1 column 277 (char 276)\n",
      "../data/wikipedia_fixed/b.parquet:  70%|▋| 321/461 [36:2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4162 tokens (3162 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"When was the Belgian Physical Society founded and who was its first president?\", \"answer\": \"A\", \"A\": \"The Belgian Physical Society was founded on 16 February 1929 at the University Foundation, and its first president was the spectroscopist A. de Hemptinne.\", \"B\": \"The Belgian Physical Society was founded on 26 December 1962, and its first president was Charles Manneback.\", \"C\": \"The Belgian Physical Society was founded in 1949 under the presidency of Charles Manneback, and its first president was Th. De Donder.\", \"D\": \"The Belgian Physical Society was founded in 1950, and its first president was the spectroscopist A. de Hemptinne.\", \"E\": \"The Belgian Physical Society was founded in 1970 under the presidency of Lieven Van Gerven, and its first president was Th. De Donder.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4162 tokens (3162 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/b.parquet:  89%|▉| 409/461 [46:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What was Beryl May Dent's role at the University of Bristol after being appointed a demonstrator in the Department of Physics?\", \"answer\": \"C\", \"A\": \"She became a researcher in the Physics Department\", \"B\": \"She joined the Women's Engineering Society\", \"C\": \"She became John Lennard-Jones' research assistant in theoretical physics\", \"D\": \"She became the leader of the computation section at Metropolitan-Vickers\", \"E\": \"She became a technical librarian for the scientific and technical staff\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/b.parquet:  91%|▉| 420/461 [52:1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4155 tokens (3155 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the purpose of BGS Groundhog Desktop software?\", \"answer\": \"C\", \"A\": \"To develop 2D geological models\", \"B\": \"To import and analyze AGS borehole data\", \"C\": \"To visualize, interpret, and create 3D geological models\", \"D\": \"To digitize and edit geologic map linework\", \"E\": \"To integrate and publish 3D geological outputs\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4155 tokens (3155 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/b.parquet:  95%|▉| 436/461 [53:5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6186 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the main goal of the Brazilian Astronomy and Astronautics Olympiad?\", \"answer\": \"C\", \"A\": \"To organize a national astronomy competition in Brazilian schools\", \"B\": \"To spread the knowledge of astronautics throughout all of Brazilian society\", \"C\": \"To spread the knowledge of astronomy throughout all of Brazilian society and encourage youths' interest in astronomy and astronautics and in scientific thought in general\", \"D\": \"To encourage youths' interest in astronomy and astronautics\", \"E\": \"To participate in the organizing committee of the olympiad\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6186 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/b.parquet: 100%|█| 461/461 [56:4\n",
      "../data/wikipedia_fixed/c.parquet:   3%|█▎                                      | 26/774 [02:49<1:16:51,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 10401 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What did Carolyn Widney Greider discover in 1984?\", \"answer\": \"A\", \"A\": \"The enzyme telomerase\", \"B\": \"The structure of telomeres\", \"C\": \"The mechanism of telomere addition\", \"D\": \"The role of RNA in telomerase\", \"E\": \"The process of telomere elongation\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 10401 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/c.parquet:   4%|█▊                                      | 34/774 [03:36<1:10:46,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4874 tokens (3874 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Where did Christina Sormani receive her Ph.D.?\", \"answer\": \"A\", \"A\": \"New York University\", \"B\": \"City University of New York\", \"C\": \"Lehman College\", \"D\": \"CUNY Graduate Center\", \"E\": \"Johns Hopkins University\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4874 tokens (3874 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:   5%|█▉                                      | 37/774 [03:56<1:15:04,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 691 (char 690)\n",
      "{\"prompt\": \"Which of the following statements accurately describes C.D. Broad's contributions to philosophy and his personal life?\", \"answer\": \"C\", \"A\": \"C.D. Broad was primarily known for his work in the field of psychical research and his support for the acceptance of homosexual acts.\", \"B\": \"C.D. Broad's essay on determinism, indeterminism, and libertarianism introduced the philosophical terms occurrent causation and non-occurrent causation.\", \"C\": \"C.D. Broad was a professor at Bristol University and later at Cambridge University, where he held the position of Knightbridge Professor of Moral Philosophy for twenty years.\", \"D\": \"C.D. Broad argued for non-occurrent causation as \"literally determined by the agent or self\" in his essay on determinism, indeterminism, and libertarianism.\", \"E\": \"C.D. Broad's essay on determinism, indeterminism, and libertarianism challenged the belief that physical and non-physical things cannot interact.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 1 column 691 (char 690)\n",
      "../data/wikipedia_fixed/c.parquet:   7%|██▊                                     | 54/774 [06:07<1:02:02,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4400 tokens (3400 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Who first described the geological setting of the Coalbrookdale Formation and named it Silurian?\", \"answer\": \"A\", \"A\": \"Roderick Murchison\", \"B\": \"Robert J. King\", \"C\": \"Adam Sedgwick\", \"D\": \"John M. Hurst\", \"E\": \"Derek E. G. Briggs\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4400 tokens (3400 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  10%|████                                    | 78/774 [08:52<1:00:41,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4827 tokens (3827 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which geologic formation in New York preserves fossils dating back to the Devonian period?\", \"answer\": \"C\", \"A\": \"The Cattaraugus Formation\", \"B\": \"The Earth sciences portal\", \"C\": \"The Cattaraugus Formation\", \"D\": \"The Paleontology portal\", \"E\": \"The List of fossiliferous stratigraphic units in New York\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4827 tokens (3827 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  19%|███████▍                               | 148/774 [16:05<1:05:37,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 1 column 400 (char 399)\n",
      "{\"prompt\": \"According to physical causal closure, what is the assertion made about the causes of physical events?\", \"answer\": \"A\", \"A\": \"Physical events have only physical causes\", \"B\": \"Physical events have causes outside the physical domain\", \"C\": \"Physical events have both physical and mental causes\", \"D\": \"Physical events have multiple redundant causes\", \"E\": \"Physical events have no causes\"},\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 340, in decode\n",
      "    raise JSONDecodeError(\"Extra data\", s, end)\n",
      "json.decoder.JSONDecodeError: Extra data: line 1 column 400 (char 399)\n",
      "../data/wikipedia_fixed/c.parquet:  20%|███████▊                               | 156/774 [17:05<1:05:12,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4737 tokens (3737 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the formula for cyclooctadiene rhodium chloride dimer?\", \"answer\": \"B\", \"A\": \"RhCl2(C8H12)\", \"B\": \"Rh2Cl2(C8H12)2\", \"C\": \"RhCl(COD)\", \"D\": \"Rh2Cl2(COD)2\", \"E\": \"RhCl(L)(COD)\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4737 tokens (3737 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  29%|███████████▍                           | 228/774 [25:07<1:13:47,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5312 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"According to the text, the concept of the clockwork universe compares the universe to a mechanical clock, which operates perfectly and predictably due to the laws of physics governing its gears. This idea was popular among deists during the Enlightenment, particularly when Isaac Newton formulated his laws of motion and universal gravitation, allowing for the prediction of the behavior of both terrestrial objects and the Solar System. Additionally, the notion of the universe as a machine can be traced back to John of Sacrobosco's medieval text, where he referred to it as the machina mundi, suggesting that the reported eclipse of the Sun during the crucifixion of Jesus disrupted the order of this machine. Samuel Clarke, in response to Gottfried Leibniz, argued that viewing the world as a clockwork machine without the intervention of God leads to materialism and fate, ultimately excluding the concept of Providence and God's governance from reality. Furthermore, in 2009, artist Tim Wetherell created a steel artwork for Questacon in Australia, representing the clockwork universe with moving gears, a functioning clock, and a lunar terminator movie. This concept of the clockwork universe is related to mechanical philosophy, determinism, eternalism, the philosophy of space and time, orrery, and superdeterminism.\", \"answer\": \"C\", \"A\": \"Mechanical philosophy\", \"B\": \"Determinism\", \"C\": \"Eternalism (philosophy of time)\", \"D\": \"Orrery\", \"E\": \"Philosophy of space and time\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5312 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/c.parquet:  43%|█████████████████▋                       | 334/774 [36:58<52:50,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 5019 tokens (4019 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes chaotic rotation?\", \"answer\": \"C\", \"A\": \"Chaotic rotation is only observed in objects that are spherically symmetric or well isolated from gravitational interaction.\", \"B\": \"Chaotic rotation is a result of the interactions within a system of orbiting bodies.\", \"C\": \"Chaotic rotation may not have a fixed axis or period and is caused by interactions within a system of orbiting bodies.\", \"D\": \"Chaotic rotation is a phenomenon observed only in galaxies.\", \"E\": \"Chaotic rotation is a predictable and regular rotation of an astronomical body.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 5019 tokens (4019 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  50%|████████████████████▍                    | 385/774 [42:45<34:30,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4866 tokens (3866 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What determines the leading-order orbital evolution of a compact binary system in astrophysics?\", \"answer\": \"C\", \"A\": \"The total mass of the system\", \"B\": \"The frequency evolution of the gravitational wave signal\", \"C\": \"The chirp mass of the system\", \"D\": \"The component masses of the system\", \"E\": \"The symmetric mass ratio of the system\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4866 tokens (3866 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  52%|█████████████████████▍                   | 405/774 [45:28<36:14,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4515 tokens (3515 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the primary focus of the China Geological Survey's work?\", \"answer\": \"C\", \"A\": \"Resource exploration\", \"B\": \"Environmental protection\", \"C\": \"Geological surveys and research\", \"D\": \"Geological hazard prevention\", \"E\": \"Geological disaster reduction\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4515 tokens (3515 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  53%|█████████████████████▋                   | 409/774 [45:54<34:44,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4452 tokens (3452 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the name of the railway station that is located near St John's Hill in south-west Battersea in the London Borough of Wandsworth?\", \"answer\": \"C\", \"A\": \"London Victoria\", \"B\": \"London Waterloo\", \"C\": \"Clapham Junction\", \"D\": \"Nine Elms\", \"E\": \"Richmond\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4452 tokens (3452 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  55%|██████████████████████▌                  | 427/774 [47:42<31:34,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5350 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which property states that changing the order of the operands in a binary operation does not change the result?\", \"answer\": \"A\", \"A\": \"Commutative property\", \"B\": \"Associative property\", \"C\": \"Distributive property\", \"D\": \"Symmetric property\", \"E\": \"Identity property\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5350 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/c.parquet:  61%|█████████████████████████                | 473/774 [52:28<30:12,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4707 tokens (3707 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which two quadrants symbolize the modern constellation Cetus in traditional Chinese uranography?\", \"answer\": \"B\", \"A\": \"The Black Tortoise of the North and the White Tiger of the West\", \"B\": \"The Black Tortoise of the North and the White Tiger of the West\", \"C\": \"The Black Tortoise of the South and the White Tiger of the East\", \"D\": \"The Azure Dragon of the East and the Vermilion Bird of the South\", \"E\": \"The Azure Dragon of the East and the Vermilion Bird of the West\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4707 tokens (3707 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  63%|█████████████████████████▊               | 487/774 [53:56<23:55,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the purpose of combining forecasts?\", \"answer\": \"C\", \"A\": \"To identify the best forecasting method\", \"B\": \"To reduce the forecast errors of individual forecasts\", \"C\": \"To improve forecast accuracy by capturing diverse information and minimizing biases\", \"D\": \"To create a consensus forecast based on equal weights\", \"E\": \"To combine interval forecasts and density forecasts\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/c.parquet:  74%|████████████████████████████▊          | 573/774 [1:08:35<23:24,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 5047 tokens (4047 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which chemical compound is commonly referred to as calcium oxychloride?\", \"answer\": \"B\", \"A\": \"Calcium hypochlorite is a chemical compound with the formula Ca(OCl)2.\", \"B\": \"Calcium oxychloride may refer to either calcium hypochlorite (Ca(OCl)2) or calcium hydroxychloride (CaOHCl2).\", \"C\": \"Calcium hydroxychloride is a chemical compound with the formula CaOHCl2.\", \"D\": \"Calcium oxychloride is a set index article that lists chemical compounds associated with the same name.\", \"E\": \"An internal link may lead to calcium oxychloride, but it can be changed to point directly to the intended article.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 5047 tokens (4047 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  76%|█████████████████████████████▋         | 590/774 [1:10:30<18:46,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4378 tokens (3378 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"When was the first kimberlite discovered in the Chidliak Kimberlite Province?\", \"answer\": \"B\", \"A\": \"July 2008\", \"B\": \"Between 138 and 156 Ma\", \"C\": \"2012\", \"D\": \"In 2018\", \"E\": \"During exploration conducted by BHP Billiton and Peregrine Diamonds Ltd.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4378 tokens (3378 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  77%|█████████████████████████████▉         | 594/774 [1:11:00<21:34,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5538 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What are some features and functionalities of ChemWindow?\", \"answer\": \"C\", \"A\": \"ChemWindow is a chemical structure drawing molecule editor and publishing program.\", \"B\": \"ChemWindow was first developed by SoftShell International in the 1990s.\", \"C\": \"ChemWindow offers 2D chemical structure drawing, scientific publishing/reporting tools, 3D visualization, database searching, data plotting/visualization, and various collections and calculators.\", \"D\": \"ChemWindow was acquired by Bio-Rad Laboratories, Inc. in 1996 and incorporated into their KnowItAll software product line.\", \"E\": \"ChemWindow has multiple interface languages and features DEA Controlled Substance Prediction and Structure Classification.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5538 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/c.parquet:  84%|████████████████████████████████▌      | 647/774 [1:17:05<11:02,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4149 tokens (3149 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Where did Christopher Bishop receive his bachelor's degree in mathematics?\", \"answer\": \"A\", \"A\": \"Michigan State University\", \"B\": \"Cambridge University\", \"C\": \"Yale University\", \"D\": \"University of Chicago\", \"E\": \"Stony Brook University\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4149 tokens (3149 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  94%|████████████████████████████████████▋  | 727/774 [1:26:30<04:35,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the purpose of CO stripping in electrochemistry?\", \"answer\": \"C\", \"A\": \"To electrochemically oxidize carbon monoxide on the surface of an electrocatalyst\", \"B\": \"To measure the charge expended in oxidizing CO on the surface of an electrocatalyst\", \"C\": \"To indirectly measure the electrochemically active surface area of an electrocatalyst\", \"D\": \"To determine the structural properties of an electrocatalyst\", \"E\": \"To remove carbon monoxide from the surface of an electrocatalyst\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/c.parquet:  95%|█████████████████████████████████████▏ | 737/774 [1:32:36<05:40,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4378 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Who published the Canon of Eclipses at the Imperial Academy of Sciences of Vienna in 1887?\", \"answer\": \"A\", \"A\": \"Theodor Ritter von Oppolzer\", \"B\": \"Donald H. Menzel\", \"C\": \"Owen Gingerich\", \"D\": \"The Imperial Academy of Sciences of Vienna\", \"E\": \"Dover Publications\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4378 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/c.parquet:  97%|█████████████████████████████████████▉ | 753/774 [1:34:49<02:30,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4540 tokens (3540 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the main advantage of cell-based models in computational biology?\", \"answer\": \"C\", \"A\": \"They allow for continuous movement of cells in space.\", \"B\": \"They represent biological cells as discrete entities.\", \"C\": \"They integrate cell level processes within a cell population.\", \"D\": \"They capture spatial distributions of both intracellular and extracellular processes.\", \"E\": \"They are easily extended to a large number of cells in both 2D and 3D.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4540 tokens (3540 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet:  97%|█████████████████████████████████████▉ | 754/774 [1:34:59<02:42,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the main advantage of cell-based models in computational biology?\", \"answer\": \"C\", \"A\": \"They allow for continuous movement of cells in space.\", \"B\": \"They represent biological cells as discrete entities.\", \"C\": \"They integrate cell level processes within a cell population.\", \"D\": \"They capture spatial distributions of both intracellular and extracellular processes.\", \"E\": \"They are easily extended to a large number of cells in both 2D and 3D.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/c.parquet:  99%|██████████████████████████████████████▍| 764/774 [1:36:22<01:09,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4529 tokens (3529 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What are the three mechanisms proposed to account for the core-mantle differentiation following the stage of nebular formation of the solar system?\", \"answer\": \"C\", \"A\": \"Percolation of iron alloy through silicate crystals\", \"B\": \"Separation of metal from rock in a primordial magma ocean\", \"C\": \"Migration of iron diapirs or dikes through the mantle\", \"D\": \"Formation of iron ponds in the mantle\", \"E\": \"Deposition of iron blobs at the lithosphere-asthenosphere boundary\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4529 tokens (3529 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/c.parquet: 100%|███████████████████████████████████████| 774/774 [1:37:28<00:00,  7.56s/it]\n",
      "../data/wikipedia_fixed/d.parquet:  22%|█████████▍                                | 91/406 [10:38<31:24,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4485 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which university has David Edwin Logan been a professor at since 2005?\", \"answer\": \"B\", \"A\": \"Trinity College, Cambridge\", \"B\": \"University of Oxford\", \"C\": \"Christ's College, Cambridge\", \"D\": \"Balliol College, Oxford\", \"E\": \"University College, Oxford\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4485 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/d.parquet:  23%|█████████▌                                | 92/406 [10:48<38:13,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4527 tokens (3527 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which university has David Edwin Logan been a professor at since 2005?\", \"answer\": \"B\", \"A\": \"Trinity College, Cambridge\", \"B\": \"University of Oxford\", \"C\": \"Christ's College, Cambridge\", \"D\": \"Balliol College, Oxford\", \"E\": \"University College, Oxford\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4527 tokens (3527 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/d.parquet:  26%|██████████▋                              | 106/406 [12:33<32:37,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5789 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"When was Daresbury Laboratory officially opened and who inaugurated it?\", \"answer\": \"B\", \"A\": \"1962 by Harold Wilson\", \"B\": \"16 June 1967 by Harold Wilson\", \"C\": \"1967 by the British National Institute for Research in Nuclear Science\", \"D\": \"16 June 1962 by the Science and Technology Facilities Council\", \"E\": \"16 June 1967 by the Science and Technology Facilities Council\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5789 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/d.parquet:  33%|█████████████▌                           | 134/406 [16:09<31:06,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4210 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the definition of a drag count?\", \"answer\": \"C\", \"A\": \"A drag count is a unit used to measure the drag force of an object relative to the fluid.\", \"B\": \"A drag count is a unit used to measure the speed of an object relative to the fluid.\", \"C\": \"A drag count is a dimensionless unit used to compare the solutions of different geometries by means of a dimensionless number.\", \"D\": \"A drag count is a unit used to measure the mass density of the fluid.\", \"E\": \"A drag count is a unit used to measure the reference area of an object.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4210 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/d.parquet:  37%|███████████████                          | 149/406 [18:03<28:39,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which of the following statements accurately describes David Chaim Rubinsztein?\", \"answer\": \"C\", \"A\": \"David Chaim Rubinsztein is a professor of Molecular Neurogenetics at the University of Cambridge.\", \"B\": \"David Chaim Rubinsztein completed his Bachelor of Medicine, Bachelor of Surgery (MB ChB) in 1986.\", \"C\": \"David Chaim Rubinsztein has made major contributions to the field of neurodegeneration with his laboratory's discovery that autophagy regulates the levels of intracytoplasmic aggregate-prone proteins that cause many neurodegenerative diseases.\", \"D\": \"David Chaim Rubinsztein has been an author on more than 300 scientific papers.\", \"E\": \"David Chaim Rubinsztein has won numerous awards including the Graham Bull Prize for Clinical Science by the Royal College of Physicians in 2007.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/d.parquet:  59%|████████████████████████                 | 238/406 [27:40<17:10,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4544 tokens (3544 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which statement accurately describes dense regular connective tissue (DRCT)?\", \"answer\": \"C\", \"A\": \"DRCT is composed of white fibrous connective tissue and yellow fibrous connective tissue.\", \"B\": \"DRCT is characterized by the presence of elastic fibers.\", \"C\": \"DRCT has great tensile strength and resists pulling forces well in one direction.\", \"D\": \"DRCT has a rich blood supply, allowing for quick healing of damaged tendons and ligaments.\", \"E\": \"DRCT is primarily found in the form of irregular patterns and networks.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4544 tokens (3544 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/d.parquet:  65%|██████████████████████████▋              | 264/406 [30:49<18:11,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"According to the book Dogs: Their Fossil Relatives and Evolutionary History by Xiaoming Wang and Richard H. Tedford, what does the book provide?\", \"answer\": \"C\", \"A\": \"A detailed analysis of the domestication and evolution of domestic dogs from their wolf ancestors\", \"B\": \"A comprehensive overview of existing and extinct taxa of canids\", \"C\": \"A holistic picture of canid evolution\", \"D\": \"An in-depth coverage of Chinese canids\", \"E\": \"All of the above\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/d.parquet:  74%|██████████████████████████████▎          | 300/406 [40:44<13:05,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4112 tokens (3112 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which statement accurately describes the displaced Poisson distribution?\", \"answer\": \"C\", \"A\": \"The displaced Poisson distribution is a generalization of the normal distribution.\", \"B\": \"The probability mass function of the displaced Poisson distribution is given by P(X=n) = e^(-λ) * (λ^(n+r)) / ((n+r)! * I(r,λ)), where λ > 0 and r is a new parameter.\", \"C\": \"The ratio of successive probabilities in the Poisson distribution is λ/n, while in the displaced Poisson distribution it is λ/(n+r).\", \"D\": \"The displaced Poisson distribution is a special case of the exponential distribution.\", \"E\": \"The Pearson's incomplete gamma function is used to calculate the probability mass function of the displaced Poisson distribution.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4112 tokens (3112 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/d.parquet:  79%|████████████████████████████████▌        | 322/406 [43:22<09:35,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4716 tokens (3716 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the deviance information criterion (DIC)?\", \"answer\": \"B\", \"A\": \"DIC is a measure of how well a model fits the data, but does not penalize for model complexity.\", \"B\": \"DIC is a hierarchical modeling generalization of the Akaike information criterion (AIC) and is particularly useful in Bayesian model selection problems.\", \"C\": \"DIC is only valid when the posterior distribution is exactly multivariate normal.\", \"D\": \"DIC requires calculating the likelihood at its maximum over θ, which is readily available from the MCMC simulation.\", \"E\": \"DIC is easily calculated from the samples generated by a Markov chain Monte Carlo simulation.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4716 tokens (3716 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/d.parquet:  82%|█████████████████████████████████▌       | 332/406 [44:30<07:53,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which legislation and regulation does the Department of Plant Protection in Pakistan work under?\", \"answer\": \"C\", \"A\": \"Plant Quarantine Act 1976\", \"B\": \"Agricultural Pesticide Rules\", \"C\": \"Agricultural Pesticide Ordinance\", \"D\": \"Central Plant Quarantine Lab\", \"E\": \"Federal Pesticide Testing and Reference Lab\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/d.parquet:  84%|██████████████████████████████████▌      | 342/406 [45:50<08:51,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4511 tokens (3511 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes dibromoethane?\", \"answer\": \"B\", \"A\": \"Dibromoethane is a compound with the molecular formula C2H4Br2.\", \"B\": \"Dibromoethane can refer to either 1,1-dibromoethane or 1,2-dibromoethane, which are isomers.\", \"C\": \"Dibromoethane is commonly known as ethylidene dibromide.\", \"D\": \"Dibromoethane is a chemical compound associated with dibromoethene.\", \"E\": \"Dibromoethane is a set index article that lists various chemical compounds.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4511 tokens (3511 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/d.parquet:  89%|████████████████████████████████████▎    | 360/406 [47:54<04:06,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5206 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following compounds is associated with the name 'Dimethylpentane'?\", \"answer\": \"C\", \"A\": \"2,2-Dimethylpentane\", \"B\": \"2,3-Dimethylpentane\", \"C\": \"2,4-Dimethylpentane\", \"D\": \"3,3-Dimethylpentane\", \"E\": \"None of the above\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5206 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/d.parquet:  94%|██████████████████████████████████████▋  | 383/406 [50:50<02:57,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5049 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"According to J. B. S. Haldane in his book Daedalus; or, Science and the Future, what does he argue about the nature of biological inventions?\", \"answer\": \"C\", \"A\": \"Biological inventions are always hailed as insults to some god.\", \"B\": \"Biological inventions are always perceived as indecent and unnatural by observers from nations that had not previously heard of them.\", \"C\": \"Biological inventions are considered perversions by observers from nations that had not previously heard of them.\", \"D\": \"Biological inventions bring progress and benefits to mankind.\", \"E\": \"Biological inventions should be accompanied by advances in ethics to prevent grief.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5049 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/d.parquet:  99%|████████████████████████████████████████▌| 402/406 [53:02<00:28,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4654 tokens (3654 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What was Donald Paul Merrifield's educational background?\", \"answer\": \"D\", \"A\": \"He received his bachelor's degree in physics from California Institute of Technology in 1950.\", \"B\": \"He obtained a master's degree in physics from University of Notre Dame in 1951.\", \"C\": \"He earned his doctorate in physics from the Massachusetts Institute of Technology (M.I.T.) in 1962.\", \"D\": \"All of the above.\", \"E\": \"None of the above.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4654 tokens (3654 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/d.parquet: 100%|█████████████████████████████████████████| 406/406 [53:32<00:00,  7.91s/it]\n",
      "../data/wikipedia_fixed/e.parquet:   3%|█▍                                        | 15/429 [01:45<55:41,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4520 tokens (3520 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the difference between explicit symmetry breaking and spontaneous symmetry breaking?\", \"answer\": \"C\", \"A\": \"Explicit symmetry breaking occurs when the defining equations of motion do not respect the symmetry, while spontaneous symmetry breaking occurs when the ground state of the theory breaks the symmetry.\", \"B\": \"Explicit symmetry breaking is associated with electromagnetic radiation, while spontaneous symmetry breaking is not.\", \"C\": \"Explicit symmetry breaking is caused by terms in the defining equations of motion that do not respect the symmetry, while spontaneous symmetry breaking is caused by the ground state of the theory breaking the symmetry.\", \"D\": \"Explicit symmetry breaking is commonly observed in perturbation theory in quantum mechanics, while spontaneous symmetry breaking is not.\", \"E\": \"Explicit symmetry breaking is characterized by a small interaction parameter in the Hamiltonian, while spontaneous symmetry breaking is not.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4520 tokens (3520 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/e.parquet:   7%|██▉                                       | 30/429 [03:42<49:32,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4921 tokens (3921 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the purpose of the EcoProIT project?\", \"answer\": \"C\", \"A\": \"To provide production engineers with a tool for detailed ecological footprint analyses\", \"B\": \"To design a tool for industrial applications used for detailed environmental footprint analyses\", \"C\": \"To simulate production and analyze the environmental footprint of products in a standardized way\", \"D\": \"To benchmark different sites using standardized methods and data management\", \"E\": \"To connect different companies' tools and upload information to the central EcoProIT Database\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4921 tokens (3921 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/e.parquet:   7%|███                                       | 31/429 [03:52<55:01,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the purpose of the EcoProIT project?\", \"answer\": \"C\", \"A\": \"To provide production engineers with a tool for detailed ecological footprint analyses\", \"B\": \"To design a tool for industrial applications used for detailed environmental footprint analyses\", \"C\": \"To simulate production and analyze the environmental footprint of products in a standardized way\", \"D\": \"To benchmark different sites using standardized methods and data management\", \"E\": \"To connect different companies' tools and upload information to the central EcoProIT Database\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/e.parquet:  10%|████▏                                     | 43/429 [05:10<41:19,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is Eric Schechter best known for?\", \"answer\": \"C\", \"A\": \"Authoring articles in analysis, differential equations, mathematical logic, and set theory.\", \"B\": \"Being involved in political activism of the democratic socialist variety.\", \"C\": \"Writing the book Handbook of Analysis and its Foundations.\", \"D\": \"Maintaining two frequently cited webpages: Common Errors in Undergraduate Mathematics and A Home Page for the Axiom of Choice.\", \"E\": \"Running for Tennessee's 5th Congressional District seat against incumbent congressman Jim Cooper.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/e.parquet:  11%|████▍                                   | 47/429 [11:04<4:19:14, 40.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4500 tokens (3500 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What are some features of an expression vector?\", \"answer\": \"C\", \"A\": \"Expression vectors do not contain regulatory sequences.\", \"B\": \"Expression vectors are only used for gene expression in prokaryotic cells.\", \"C\": \"Expression vectors contain regulatory sequences, an origin of replication, a selectable marker, and a suitable site for gene insertion.\", \"D\": \"Expression vectors are primarily used for the production of insulin.\", \"E\": \"Expression vectors do not require elements necessary for gene expression.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4500 tokens (3500 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/e.parquet:  15%|██████▎                                   | 64/429 [13:01<33:22,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4513 tokens (3513 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which university did Elizabeth Turtle earn her Ph.D. from?\", \"answer\": \"C\", \"A\": \"Massachusetts Institute of Technology\", \"B\": \"Johns Hopkins University\", \"C\": \"University of Arizona\", \"D\": \"Planetary Science Institute\", \"E\": \"Applied Physics Laboratory\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4513 tokens (3513 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/e.parquet:  18%|███████▋                                  | 78/429 [15:06<48:38,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 5077 tokens (4077 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the purpose of embryonic diapause in animal species?\", \"answer\": \"C\", \"A\": \"To ensure the blastocyst immediately implants in the uterus after sexual reproduction\", \"B\": \"To extend the normal gestation period for a species-specific time\", \"C\": \"To synchronize the birth of offspring with the most favorable conditions for reproductive success\", \"D\": \"To induce reversible developmental arrest in species that do not naturally undergo embryonic diapause\", \"E\": \"To regulate the cell cycle and promote DNA synthesis in the blastocyst\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 5077 tokens (4077 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/e.parquet:  27%|███████████                              | 116/429 [18:54<35:59,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6282 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the purpose of a statistical ensemble in physics?\", \"answer\": \"C\", \"A\": \"To represent all possible states of a system at a given time\", \"B\": \"To calculate averages directly over the whole of the thermodynamic ensemble\", \"C\": \"To describe a single system by considering a large number of virtual copies\", \"D\": \"To determine the macroscopic properties of a system based on its microscopic details\", \"E\": \"To generate equivalence classes of statistical ensembles\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6282 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/e.parquet:  32%|█████████████▎                           | 139/429 [21:33<24:46,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 5050 tokens (4050 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which company acquired Landes Bioscience in 2014?\", \"answer\": \"D\", \"A\": \"Taylor & Francis\", \"B\": \"Epigenetics Society\", \"C\": \"Manel Esteller\", \"D\": \"Taylor & Francis\", \"E\": \"University of Barcelona\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 5050 tokens (4050 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/e.parquet:  34%|█████████████▊                           | 144/429 [22:35<48:24, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4441 tokens (3441 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the behavior of the chemolithoautotrophic bacterium Acidithiobacillus ferrooxidans in experiments?\", \"answer\": \"C\", \"A\": \"The bacterium Acidithiobacillus ferrooxidans can receive electrons necessary for growth from an electrode terminal.\", \"B\": \"The bacterium Acidithiobacillus ferrooxidans can receive electrons necessary for growth from diffusible Fe2+ ions.\", \"C\": \"The bacterium Acidithiobacillus ferrooxidans switches the electron source for carbon assimilation from diffusible Fe2+ ions to an electrode under the condition that electrical current is the only source of energy and electrons.\", \"D\": \"The bacterium Acidithiobacillus ferrooxidans can only grow in ocean thermal vents.\", \"E\": \"The bacterium Acidithiobacillus ferrooxidans does not exhibit electrotrophic behavior.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4441 tokens (3441 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/e.parquet:  37%|███████████████▏                         | 159/429 [24:13<28:47,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the purpose of the European Space Weather Portal (ESWeP)?\", \"answer\": \"D\", \"A\": \"To provide a centralized access point for space weather professionals to upload and share technical documents\", \"B\": \"To develop space weather products and services in Europe\", \"C\": \"To host and maintain models for space weather research\", \"D\": \"To provide a platform for the space weather community to share knowledge and results\", \"E\": \"To disseminate reference documents that are space weather-related\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/e.parquet:  43%|█████████████████▋                       | 185/429 [32:48<26:24,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 26 Sep 2023 14:23:04 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '80cc29926a7b0aba-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "{\"prompt\": \"What is the relationship between electrets and magnets?\", \"answer\": \"C\", \"A\": \"Electrets and magnets have no relationship.\", \"B\": \"Electrets and magnets have a similar effect on each other when stationary.\", \"C\": \"Electrets and magnets have no effect on each other when stationary, but when an electret is moved with respect to a magnetic pole, a force is felt pushing the electret in a direction perpendicular to the magnetic field.\", \"D\": \"Electrets and magnets have the same type of electric field around their perimeter.\", \"E\": \"Electrets and magnets have the same type of magnetic field around their perimeter.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 26 Sep 2023 14:23:04 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '80cc29926a7b0aba-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "../data/wikipedia_fixed/e.parquet:  44%|██████████████████▏                      | 190/429 [33:39<38:05,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6742 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the focus and subject areas of the journal Environmental Earth Sciences?\", \"answer\": \"C\", \"A\": \"The journal Environmental Earth Sciences focuses on the interaction between humans, natural resources, ecosystems, special climates or unique geographic zones, and the earth.\", \"B\": \"The journal Environmental Earth Sciences focuses on water and soil contamination caused by waste management.\", \"C\": \"The journal Environmental Earth Sciences focuses on all aspects of interaction between humans, natural resources, ecosystems, special climates or unique geographic zones, and the earth. Its subject areas include water and soil contamination caused by waste management; environmental problems associated with transportation by land, air, and water; and geological processes that may impact biosystems or humans.\", \"D\": \"The journal Environmental Earth Sciences focuses on geological processes that may impact biosystems or humans.\", \"E\": \"The journal Environmental Earth Sciences focuses on water and soil contamination caused by waste management and geological processes that may impact biosystems or humans.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6742 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/e.parquet:  62%|█████████████████████████▍               | 266/429 [42:01<17:33,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4513 tokens (3513 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which scientific journal covers all areas of ecology, evolution, and conservation?\", \"answer\": \"C\", \"A\": \"Journal Citation Reports\", \"B\": \"Web of Science\", \"C\": \"Ecology and Evolution\", \"D\": \"Wiley Online Library\", \"E\": \"Wikimedia Commons\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4513 tokens (3513 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/e.parquet:  71%|█████████████████████████████▏           | 306/429 [46:48<13:57,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"Which of the following statements accurately describes the electromeric effect?\", \"answer\": \"C\", \"A\": \"The electromeric effect is a type of electron displacement caused by the presence of a reagent like an electrophile or a nucleophile.\", \"B\": \"The electromeric effect is a molecular polarization occurring by an intermolecular electron displacement characterized by the substitution of one electron pair for another within the same atomic octet of electrons.\", \"C\": \"The electromeric effect is absorbed in the term resonance effect and is no longer used in standard texts.\", \"D\": \"The electromeric effect is classified into two types: the +E effect and the -E effect, based on the direction of electron pair transfer.\", \"E\": \"The electromeric effect is often referred to as the conjugative mechanism and is considered a type of inductive effect.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/e.parquet:  86%|███████████████████████████████████▎     | 369/429 [59:02<05:47,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4246 tokens (3246 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the current win-loss record of Ekaterina Makarova against players ranked in the top 10?\", \"answer\": \"D\", \"A\": \"Ekaterina Makarova has a win-loss record of 10-5 against players ranked in the top 10.\", \"B\": \"Ekaterina Makarova has a win-loss record of 5-10 against players ranked in the top 10.\", \"C\": \"Ekaterina Makarova has a win-loss record of 15-5 against players ranked in the top 10.\", \"D\": \"Ekaterina Makarova has a win-loss record of 5-15 against players ranked in the top 10.\", \"E\": \"Ekaterina Makarova has a win-loss record of 10-10 against players ranked in the top 10.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4246 tokens (3246 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/e.parquet: 100%|███████████████████████████████████████| 429/429 [1:05:40<00:00,  9.19s/it]\n",
      "../data/wikipedia_fixed/f.parquet:   9%|████▋                                                 | 25/287 [02:39<27:31,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"Which of the following statements accurately describes facultative parasites?\", \"answer\": \"C\", \"A\": \"Facultative parasites rely on a host for completion of their life cycle.\", \"B\": \"Facultative parasites cannot survive without engaging in parasitic activity.\", \"C\": \"Facultative parasites may resort to parasitic activity but do not absolutely rely on a host for completion of their life cycle.\", \"D\": \"Facultative parasites only parasitize living organisms and cannot survive on dead matter.\", \"E\": \"Facultative parasites always act as destructive agents of wood rot.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/f.parquet:  21%|███████████▎                                          | 60/287 [11:07<18:30,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4310 tokens (3310 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the role of the fusome in germline cyst development?\", \"answer\": \"D\", \"A\": \"Maintaining germline cysts\", \"B\": \"Coordinating the number of mitotic divisions\", \"C\": \"Serving as a structure for intercellular communication\", \"D\": \"All of the above\", \"E\": \"None of the above\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4310 tokens (3310 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/f.parquet:  25%|█████████████▎                                        | 71/287 [12:20<22:19,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5772 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the predicted tertiary structure of FAM110A?\", \"answer\": \"C\", \"A\": \"FAM110A is predicted to be hydrophilic and soluble.\", \"B\": \"FAM110A is not predicted to be myristolated, indicating that it is not membrane-associated.\", \"C\": \"The tertiary structure of FAM110A is predicted to be 80% disordered.\", \"D\": \"FAM110A is not predicted to contain sulfation of tyrosine residues, suggesting that it is not secreted.\", \"E\": \"FAM110A has been observed to be localized in the nucleoplasm, cytosol, and vesicles.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5772 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/f.parquet:  26%|██████████████                                        | 75/287 [12:42<17:56,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is Firat Güder's position at Imperial College London?\", \"answer\": \"C\", \"A\": \"Chief Engineer and Principal Investigator\", \"B\": \"Professor of Bioengineering\", \"C\": \"Professor of Intelligent Interfaces\", \"D\": \"Research Fellow\", \"E\": \"Scientist and Innovator\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/f.parquet:  34%|██████████████████▎                                   | 97/287 [20:20<15:36,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4521 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the primary fabric in a rock that is characterized by a preferred orientation of clast long axes parallel to the flow direction, deposited by a fast waning current?\", \"answer\": \"A\", \"A\": \"Primary fabric\", \"B\": \"Shape fabric\", \"C\": \"Crystallographic preferred orientation\", \"D\": \"S-fabric\", \"E\": \"L-fabric\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4521 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/f.parquet:  37%|███████████████████▊                                 | 107/287 [21:27<16:49,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4463 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements about free-radical reactions is most accurate?\", \"answer\": \"C\", \"A\": \"Free-radical reactions are only found in organic synthesis.\", \"B\": \"Free-radical reactions are always acid or base catalyzed.\", \"C\": \"Free-radical reactions occur frequently in the gas phase and are often initiated by light.\", \"D\": \"Free-radical reactions are dependent on the polarity of the reaction medium.\", \"E\": \"Free-radical reactions have a broken-order dependence of 1.5 with respect to the initiating species.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4463 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/f.parquet:  56%|█████████████████████████████▋                       | 161/287 [27:29<22:56, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4969 tokens (3969 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"According to Faraday's law of induction, the flux linkage in a multi-turn inductor is defined as Ψ = n Φ, where n is the number of turns and Φ is the flux passing through all the turns. However, due to the physical limitations of the coil and the configuration of the magnetic field, some flux leaks between the turns, resulting in the formation of leakage flux and a reduction in the linkage. The flux linkage is measured in webers (Wb) and is used to describe the interaction between the inductor and the magnetic flux. In circuit theory, flux linkage is a property of a two-terminal element and is defined as the time integral of the voltage across the device. Faraday's law of induction states that the magnitude of the electromotive force (EMF) generated in a closed loop conductor is proportional to the rate of change of the total magnetic flux passing through the loop. The flux linkage is equivalent to the magnetic flux passing through the surface formed by a closed conducting loop coil and is determined by the number of turns in the coil and the magnetic field. The distinction between flux linkage and flux is mainly used in engineering disciplines, although theoretically, the concept of flux linkage can be explained rigorously with Riemann surfaces. However, for a memristor, which is a fourth fundamental circuit element, the flux linkage is not equivalent to magnetic flux, as the electric field in the element is not negligible and the energy related to the flux linkage is dissipated as Joule heating instead of being stored in the magnetic field.\", \"answer\": \"C\", \"A\": \"Flux linkage is the same as magnetic flux in all cases.\", \"B\": \"The flux linkage in a multi-turn inductor is given by Ψ = nM/MF*R.\", \"C\": \"Flux linkage is a property of a two-terminal element and is defined as the time integral of the voltage across the device.\", \"D\": \"The flux linkage is determined by the number of turns in the coil and the magnetic field.\", \"E\": \"The concept of flux linkage can be explained rigorously with Riemann surfaces.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4969 tokens (3969 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/f.parquet:  67%|███████████████████████████████████▋                 | 193/287 [30:43<11:13,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Fourier–Bros–Iagolnitzer (FBI) transform?\", \"answer\": \"C\", \"A\": \"The FBI transform is a generalization of the Fourier transform developed by Jacques Bros and Daniel Iagolnitzer to characterize the local analyticity of functions on Rn.\", \"B\": \"The FBI transform is a technique used to prove the analyticity of solutions of analytic elliptic partial differential equations.\", \"C\": \"The FBI transform can be used to define the Fourier and FBI transforms of tempered distributions in S'(Rn).\", \"D\": \"The FBI transform is defined by (Faf)(t,y) = (2π)-n/2 ∫ Rn f(x) e^(-a|x-y|^2/2) e^(-ix⋅t) dx.\", \"E\": \"The FBI transform satisfies the inequality |(F|ξ|f)(ξ,y)| ≤ Ce^(-ε|ξ|) for |ξ| sufficiently large to characterize local analyticity.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/f.parquet:  75%|███████████████████████████████████████▌             | 214/287 [38:44<08:21,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4113 tokens (3113 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What was Fumiko Yonezawa's contribution to the field of physics?\", \"answer\": \"C\", \"A\": \"She conducted research on semi-conductors and liquid metals.\", \"B\": \"She obtained BSc, MSc, and Ph.D. degrees from Kyoto University.\", \"C\": \"She pioneered theory and computer simulations on amorphous semiconductors and liquid metals.\", \"D\": \"She was the first woman to be President of the Physics Society of Japan.\", \"E\": \"She received the Saruhashi Prize in 1984 and the L'Oréal-UNESCO Award for Women in Science in 2005.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4113 tokens (3113 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/f.parquet:  87%|██████████████████████████████████████████████▏      | 250/287 [42:31<03:50,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5222 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Faraday cup electrometer?\", \"answer\": \"C\", \"A\": \"The Faraday cup electrometer is used to measure particles of bipolar charge.\", \"B\": \"The Faraday cup electrometer consists of a conductor filter inside a Faraday cage.\", \"C\": \"Charged particles collected by the filter generate an electric current which is measured by the electrometer.\", \"D\": \"The Faraday cup electrometer is primarily used in mass spectrometers.\", \"E\": \"The sensitiveness of the Faraday cup electrometer is mass-dependent.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5222 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/f.parquet:  87%|██████████████████████████████████████████████▎      | 251/287 [42:41<04:29,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4247 tokens (3247 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Faraday cup electrometer?\", \"answer\": \"C\", \"A\": \"The Faraday cup electrometer is used to measure particles of bipolar charge.\", \"B\": \"The Faraday cup electrometer consists of a conductor filter inside a Faraday cage.\", \"C\": \"Charged particles collected by the filter generate an electric current which is measured by the electrometer.\", \"D\": \"The Faraday cup electrometer is primarily used in mass spectrometers.\", \"E\": \"The sensitiveness of the Faraday cup electrometer is mass-dependent.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4247 tokens (3247 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/f.parquet:  97%|███████████████████████████████████████████████████▏ | 277/287 [45:22<00:53,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the field of food physical chemistry concerned with?\", \"answer\": \"C\", \"A\": \"The study of physical and chemical interactions in food systems\", \"B\": \"The applications of physical/chemical techniques and instrumentation for the study of foods\", \"C\": \"Both A and B\", \"D\": \"The manufacture, handling, and storage of foods\", \"E\": \"The physiochemical principles of reactions and conversions in food processes\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/f.parquet: 100%|█████████████████████████████████████████████████████| 287/287 [51:41<00:00, 10.81s/it]\n",
      "../data/wikipedia_fixed/g.parquet:   9%|████▉                                                 | 46/499 [05:16<54:23,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6239 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which scientists have advocated for the concept of gravitational decoherence?\", \"answer\": \"C\", \"A\": \"Frigyes Károlyházy\", \"B\": \"Roger Penrose\", \"C\": \"Lajos Diósi\", \"D\": \"Dmitriy Podolskiy\", \"E\": \"Robert Lanza\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6239 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/g.parquet:  10%|█████▌                                                | 51/499 [05:49<46:21,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4298 tokens (3298 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What was the subject of Sir Gordon Brims Black McIvor Sutherland's research work during his time at Trinity College, Cambridge?\", \"answer\": \"B\", \"A\": \"Raman spectra\", \"B\": \"Infrared spectroscopy\", \"C\": \"Mathematical ability\", \"D\": \"Hydrazine and hydrogen peroxide\", \"E\": \"N2O4 and ozone\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4298 tokens (3298 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/g.parquet:  14%|███████▋                                              | 71/499 [07:48<41:09,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4732 tokens (3732 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"According to recent genetic studies, what is the most well-regarded theory about the genetic origins of modern Japanese people?\", \"answer\": \"C\", \"A\": \"Japanese people are genetically most similar to Ryukyuans, Ainu people, Koreans as well as other East Asian people.\", \"B\": \"Japanese people descend solely from the Yayoi people and not the Jōmon population.\", \"C\": \"Present-day Japanese are descendants of both the indigenous Jōmon people and the immigrant Yayoi people.\", \"D\": \"The genetic origins of modern Japanese people are still a subject of dispute and no clear consensus has been reached.\", \"E\": \"Modern Japanese (Yamato) have a significant amount of Jōmon ancestry.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4732 tokens (3732 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/g.parquet:  25%|█████████████▎                                       | 125/499 [13:04<43:55,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4621 tokens (3621 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Who co-founded GF Biochemicals and what is the significance of this company?\", \"answer\": \"B\", \"A\": \"Pasquale Granata and Mathieu Flamini co-founded GF Biochemicals, which is the first company in the world able to mass-produce levulinic acid.\", \"B\": \"Pasquale Granata and Mathieu Flamini co-founded GF Biochemicals, the first company in the world able to mass-produce levulinic acid, and in 2015, the company won the John Sime Award for Most Innovative New Technology.\", \"C\": \"Pasquale Granata and Mathieu Flamini co-founded GF Biochemicals, a biochemical company that has a plant in Caserta employing around 80 people.\", \"D\": \"Pasquale Granata and Mathieu Flamini co-founded GF Biochemicals, which acquired the American company Segetis in 2016.\", \"E\": \"Pasquale Granata and Mathieu Flamini co-founded GF Biochemicals, which has offices in Milan and the Netherlands.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4621 tokens (3621 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/g.parquet:  31%|████████████████▎                                    | 154/499 [15:47<28:45,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4914 tokens (3914 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the estimated volume of groundwater in the Great Artesian Basin?\", \"answer\": \"C\", \"A\": \"64,900 cubic kilometres\", \"B\": \"1,700,000 square kilometres\", \"C\": \"15,600 cu mi\", \"D\": \"3,000 metres deep\", \"E\": \"30 to 100 °C\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4914 tokens (3914 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/g.parquet:  34%|█████████████████▉                                   | 169/499 [17:06<27:15,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the composition of the Gallic acid reagent used for presumptively identifying drug precursor chemicals?\", \"answer\": \"C\", \"A\": \"A mixture of gallic acid and concentrated sulfuric acid\", \"B\": \"A mixture of gallic acid and ethanol\", \"C\": \"A mixture of gallic acid and concentrated sulfuric acid, with a ratio of 0.05 g of gallic acid for every 10 mls of sulfuric acid\", \"D\": \"A mixture of gallic acid n-propyl ester and concentrated sulfuric acid\", \"E\": \"A mixture of gallic acid n-propyl ester and ethanol\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/g.parquet:  55%|█████████████████████████████                        | 274/499 [27:25<24:14,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4198 tokens (3198 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes a gravitational anomaly?\", \"answer\": \"B\", \"A\": \"A gravitational anomaly is a result of the violation of general covariance in a theory of general relativity combined with other fields.\", \"B\": \"A gravitational anomaly is an effect of quantum mechanics, usually a one-loop diagram, that invalidates the general covariance of a theory of general relativity combined with other fields.\", \"C\": \"A gravitational anomaly is a term used to describe the violation of general covariance in a theory of general relativity without considering other fields.\", \"D\": \"A gravitational anomaly is a consequence of the violation of diffeomorphism invariance in a theory of general relativity combined with other fields.\", \"E\": \"A gravitational anomaly is a phenomenon that occurs when the symmetry of a gravitational theory, known as general covariance, is broken.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4198 tokens (3198 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/g.parquet:  60%|███████████████████████████████▊                     | 300/499 [29:38<19:51,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4189 tokens (3189 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What were some of Gottfried Wilhelm Leibniz's major contributions to various fields?\", \"answer\": \"D\", \"A\": \"Leibniz made major contributions to physics and technology, and anticipated notions that surfaced much later in probability theory, biology, medicine, geology, psychology, linguistics and computer science.\", \"B\": \"Leibniz's major achievement was the development of the main ideas of differential and integral calculus, independently of Isaac Newton's contemporaneous developments.\", \"C\": \"Leibniz refined the binary number system, which is the foundation of nearly all digital (electronic, solid-state, discrete logic) computers.\", \"D\": \"All of the above.\", \"E\": \"None of the above.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4189 tokens (3189 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/g.parquet:  68%|████████████████████████████████████                 | 340/499 [32:57<12:30,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4237 tokens (3237 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What position did Giuliano Volpe hold at the University of Foggia?\", \"answer\": \"B\", \"A\": \"President of the Superior Council for Cultural and Scenic Goods\", \"B\": \"Rector\", \"C\": \"Minister of Cultural Heritage and Activities and Tourism\", \"D\": \"Italian academic and politician\", \"E\": \"None of the above\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4237 tokens (3237 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/g.parquet:  71%|█████████████████████████████████████▊               | 356/499 [34:31<15:21,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4107 tokens (3107 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What geological events occurred in the region of East Timor?\", \"answer\": \"B\", \"A\": \"The region experienced rifting between the Permian and early Cretaceous.\", \"B\": \"During the mid-Eocene, the Australian Plate collided with a subduction zone, generating folds and thrusts plus emplacing sheets of ophiolite and continental rock on top of Mesozoic sedimentary rocks.\", \"C\": \"Shallow water sediments shifted to deep water sediments by the Triassic.\", \"D\": \"The region was a subsiding passive margin from the Early Cretaceous through the Eocene, experiencing deep water carbonate and shale deposition.\", \"E\": \"Slow subsidence and carbonate deposition were typical of the region from the Eocene through the Miocene, until a second phase of thrusting uplifted onshore structures up to one kilometer.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4107 tokens (3107 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/g.parquet:  83%|███████████████████████████████████████████▉         | 414/499 [40:00<09:31,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 7309 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"According to the book Gulp: Adventures on the Alimentary Canal by Mary Roach, which topic explores the relationship between thorough chewing and the national debt?\", \"answer\": \"C\", \"A\": \"Nose Job: Tasting has little to do with taste\", \"B\": \"I'll Have the Putrescine: Your pet is not like you\", \"C\": \"The Longest Meal: Can thorough chewing lower the national debt?\", \"D\": \"Hard to Stomach: The acid relationship of William Beaumont and Alexis St. Martin\", \"E\": \"Spit Gets a Polish: Someone ought to bottle the stuff\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 7309 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/g.parquet:  84%|████████████████████████████████████████████▌        | 419/499 [40:37<09:15,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4757 tokens (3757 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"According to the genetic studies mentioned in the text, what conclusions can be drawn about the genetic makeup of ancient Egyptians and their geographic origins?\", \"answer\": \"C\", \"A\": \"The genetic studies have provided firm conclusions about the population of ancient Egypt and their geographic origins.\", \"B\": \"The genetic studies have led to a consensus on the genetic makeup of the ancient Egyptians and their geographic origins.\", \"C\": \"Conflicting DNA analysis on recent genetic samples has resulted in a lack of consensus on the genetic makeup of the ancient Egyptians and their geographic origins.\", \"D\": \"The genetic studies have revealed a significant African component in the ancient Egyptian population.\", \"E\": \"The genetic studies have shown that modern Egyptians share identical haplotypes with ancient Egyptians.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4757 tokens (3757 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/g.parquet:  84%|████████████████████████████████████████████▋        | 421/499 [40:54<09:56,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4868 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the purpose of gene trapping and how does it work?\", \"answer\": \"C\", \"A\": \"To introduce insertional mutations in an organism's genome by using gene trap vectors with a promoterless reporter gene and/or selectable genetic marker.\", \"B\": \"To terminate transcription prematurely at the inserted polyadenylation site, resulting in a truncated and nonfunctional version of the cellular protein.\", \"C\": \"To simultaneously inactivate and report the expression of a trapped gene at the insertion site, while providing a DNA tag for the rapid identification of the disrupted gene.\", \"D\": \"To centralize the data and supply modified cell lines through the International Gene Trap Consortium.\", \"E\": \"To detect developmentally regulated genes using mouse embryonic stem cells and reporter constructs.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4868 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/g.parquet:  89%|██████████████████████████████████████████████▉      | 442/499 [42:36<03:41,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the primary rock type found on The Wrekin?\", \"answer\": \"C\", \"A\": \"Rhyolite\", \"B\": \"Granophyre\", \"C\": \"Schists and gneisses\", \"D\": \"Limestone\", \"E\": \"Sandstone\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/g.parquet:  96%|██████████████████████████████████████████████████▉  | 479/499 [50:46<01:26,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4394 tokens (3394 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which geological formation on the Isle of Man is associated with the highest point on the island?\", \"answer\": \"C\", \"A\": \"Glen Rushen Formation\", \"B\": \"Injebreck Formation\", \"C\": \"Barrule Formation\", \"D\": \"Maughold Formation\", \"E\": \"Creg Agneash Formation\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4394 tokens (3394 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/g.parquet: 100%|█████████████████████████████████████████████████████| 499/499 [52:39<00:00,  6.33s/it]\n",
      "../data/wikipedia_fixed/h.parquet:   1%|▋                                                      | 5/389 [00:32<38:08,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4407 tokens (3407 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements is the most accurate regarding hydroxyeicosatetraenoic acid?\", \"answer\": \"C\", \"A\": \"Hydroxyeicosatetraenoic acid is a single compound with multiple isomers.\", \"B\": \"Hydroxyeicosatetraenoic acid is a type of fatty acid found in plants.\", \"C\": \"Hydroxyeicosatetraenoic acid refers to a group of chemical compounds with different isomers.\", \"D\": \"Hydroxyeicosatetraenoic acid is primarily produced in the liver.\", \"E\": \"Hydroxyeicosatetraenoic acid is commonly used in the production of pharmaceutical drugs.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4407 tokens (3407 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/h.parquet:  10%|█████▏                                                | 37/389 [03:24<28:47,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4382 tokens (3382 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the branch of biology that studies the microscopic anatomy of biological tissues?\", \"answer\": \"B\", \"A\": \"Histopathology\", \"B\": \"Histology\", \"C\": \"Cytology\", \"D\": \"Organology\", \"E\": \"Paleohistology\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4382 tokens (3382 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/h.parquet:  17%|█████████▏                                            | 66/389 [06:02<24:28,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Tue, 26 Sep 2023 16:47:15 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '160000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '155904', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '1.536s', 'x-request-id': '6f94b82f94cd6462eaeb91901a11e350', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80ccfc866ea78314-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "{\"prompt\": \"Who was the first scientist to experimentally determine that a Penicillium mould secretes an antibacterial substance?\", \"answer\": \"B\", \"A\": \"Louis Pasteur\", \"B\": \"Alexander Fleming\", \"C\": \"Ernest Duchesne\", \"D\": \"Howard Florey\", \"E\": \"Joseph Lister\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Tue, 26 Sep 2023 16:47:15 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '160000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '155904', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '1.536s', 'x-request-id': '6f94b82f94cd6462eaeb91901a11e350', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80ccfc866ea78314-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "../data/wikipedia_fixed/h.parquet:  18%|█████████▋                                            | 70/389 [06:44<42:11,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Tue, 26 Sep 2023 16:48:04 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '160000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '156064', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '1.476s', 'x-request-id': '1a1f90db127578ca09fab4e5c972e517', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80ccfd89a81a8314-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "{\"prompt\": \"When was the Swiss Chemical Society founded and what was the reason for IUPAC refusing SCG as a member?\", \"answer\": \"B\", \"A\": \"August 6, 1901: Founding of the Swiss Chemical Society and IUPAC refused SCG as a member because they didn't have their own journal.\", \"B\": \"August 6, 1901: Founding of the Swiss Chemical Society and IUPAC refused SCG as a member, no own journal.\", \"C\": \"August 6, 1901: Founding of the Swiss Chemical Society and IUPAC refused SCG as a member due to political reasons.\", \"D\": \"August 6, 1901: Founding of the Swiss Chemical Society and IUPAC refused SCG as a member because of their low impact factor.\", \"E\": \"August 6, 1901: Founding of the Swiss Chemical Society and IUPAC refused SCG as a member, no specific reason.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Tue, 26 Sep 2023 16:48:04 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '160000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '156064', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '1.476s', 'x-request-id': '1a1f90db127578ca09fab4e5c972e517', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80ccfd89a81a8314-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "../data/wikipedia_fixed/h.parquet:  21%|███████████                                           | 80/389 [08:04<33:53,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Tue, 26 Sep 2023 16:49:35 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '160000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '157911', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '783ms', 'x-request-id': 'a3d1a15fd70ab87b28717113a55566b4', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80ccff7efe098d13-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "{\"prompt\": \"According to the Hájek–Le Cam convolution theorem, what is the asymptotic equivalence of any regular estimator in a parametric model?\", \"answer\": \"A\", \"A\": \"A sum of two independent random variables, one of which is normal with asymptotic variance equal to the inverse of Fisher information, and the other having arbitrary distribution.\", \"B\": \"A sum of two dependent random variables, both of which are normal with asymptotic variance equal to the inverse of Fisher information.\", \"C\": \"A sum of two independent random variables, both of which have arbitrary distributions.\", \"D\": \"A sum of two dependent random variables, one of which is normal with asymptotic variance equal to the inverse of Fisher information, and the other having arbitrary distribution.\", \"E\": \"A sum of two dependent random variables, one of which is normal with asymptotic variance equal to the inverse of Fisher information, and the other having a distribution determined by the model.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Tue, 26 Sep 2023 16:49:35 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '160000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '157911', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '783ms', 'x-request-id': 'a3d1a15fd70ab87b28717113a55566b4', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80ccff7efe098d13-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "../data/wikipedia_fixed/h.parquet:  22%|███████████▊                                          | 85/389 [09:09<44:50,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4300 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Huber loss function?\", \"answer\": \"C\", \"A\": \"The Huber loss function is a loss function used in robust regression that is more sensitive to outliers than the squared error loss.\", \"B\": \"The Huber loss function is a loss function used in robust regression that is less sensitive to outliers than the squared error loss.\", \"C\": \"The Huber loss function is a loss function used in robust regression that is less sensitive to outliers than the squared error loss and is quadratic for small values of a and linear for large values.\", \"D\": \"The Huber loss function is a loss function used in robust regression that is more sensitive to outliers than the absolute loss.\", \"E\": \"The Huber loss function is a loss function used in robust regression that is less sensitive to outliers than the absolute loss.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4300 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/h.parquet:  22%|███████████▉                                          | 86/389 [09:20<47:06,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Tue, 26 Sep 2023 16:50:34 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '160000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '158168', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '687ms', 'x-request-id': 'cabd20e5b1c4bb6355f39044b4bea6d7', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80cd015a8a668d13-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Huber loss function?\", \"answer\": \"C\", \"A\": \"The Huber loss function is a loss function used in robust regression that is more sensitive to outliers than the squared error loss.\", \"B\": \"The Huber loss function is a loss function used in robust regression that is less sensitive to outliers than the squared error loss.\", \"C\": \"The Huber loss function is a loss function used in robust regression that is less sensitive to outliers than the squared error loss and is quadratic for small values of a and linear for large values.\", \"D\": \"The Huber loss function is a loss function used in robust regression that is more sensitive to outliers than the absolute loss.\", \"E\": \"The Huber loss function is a loss function used in robust regression that is less sensitive to outliers than the absolute loss.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Tue, 26 Sep 2023 16:50:34 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '160000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '158168', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '687ms', 'x-request-id': 'cabd20e5b1c4bb6355f39044b4bea6d7', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80cd015a8a668d13-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "../data/wikipedia_fixed/h.parquet:  35%|██████████████████▋                                  | 137/389 [14:17<24:56,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Tue, 26 Sep 2023 16:56:00 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '160000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '156542', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '1.296s', 'x-request-id': 'd45de51519b6e58e2faa835eaba7012b', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80cd089a8df1831c-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "{\"prompt\": \"Which of the following statements accurately describes the inheritance of autosomal dominant traits?\", \"answer\": \"C\", \"A\": \"Autosomal dominant traits can only be inherited from the mother.\", \"B\": \"Autosomal dominant traits require two copies of the trait to be displayed.\", \"C\": \"Autosomal dominant traits can be inherited from either parent and a single copy is enough to cause the trait to appear.\", \"D\": \"Autosomal dominant traits are rarely seen in females.\", \"E\": \"Autosomal dominant traits can remain hidden for several generations before displaying the phenotype.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Tue, 26 Sep 2023 16:56:00 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '160000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '156542', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '1.296s', 'x-request-id': 'd45de51519b6e58e2faa835eaba7012b', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80cd089a8df1831c-KIX', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "../data/wikipedia_fixed/h.parquet:  64%|█████████████████████████████████▊                   | 248/389 [24:57<12:06,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4793 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which two quadrants does the modern constellation Hydra lie across in traditional Chinese uranography, and what are the symbols of these quadrants?\", \"answer\": \"C\", \"A\": \"The modern constellation Hydra lies across the Azure Dragon of the East and the Vermilion Bird of the South.\", \"B\": \"The modern constellation Hydra lies across the Azure Dragon of the East and the Vermilion Bird of the North.\", \"C\": \"The modern constellation Hydra lies across the Azure Dragon of the East and the Vermilion Bird of the South.\", \"D\": \"The modern constellation Hydra lies across the Azure Dragon of the West and the Vermilion Bird of the South.\", \"E\": \"The modern constellation Hydra lies across the Azure Dragon of the West and the Vermilion Bird of the North.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4793 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/h.parquet:  72%|██████████████████████████████████████▍              | 282/389 [28:14<09:27,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5179 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the typical off-peak service at Headstone Lane railway station?\", \"answer\": \"C\", \"A\": \"Two trains per hour to London Euston and two trains to Watford Junction\", \"B\": \"Three trains per hour to London Euston and three trains to Watford Junction\", \"C\": \"Four trains per hour to London Euston and four trains to Watford Junction\", \"D\": \"Five trains per hour to London Euston and five trains to Watford Junction\", \"E\": \"Six trains per hour to London Euston and six trains to Watford Junction\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5179 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/h.parquet:  75%|███████████████████████████████████████▉             | 293/389 [29:37<09:39,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4415 tokens (3415 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What are the two important sequences of chemical reactions discovered by Sir Hans Adolf Krebs?\", \"answer\": \"C\", \"A\": \"The glyoxylate cycle and the ornithine cycle\", \"B\": \"The glycolysis cycle and the citric acid cycle\", \"C\": \"The citric acid cycle and the urea cycle\", \"D\": \"The Krebs cycle and the glycolysis cycle\", \"E\": \"The urea cycle and the ornithine cycle\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4415 tokens (3415 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/h.parquet:  77%|████████████████████████████████████████▊            | 300/389 [30:20<08:02,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4240 tokens (3240 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Who conducted experiments with dispersive spectrometers that enabled spectroscopy to become a more precise and quantitative scientific technique?\", \"answer\": \"C\", \"A\": \"Isaac Newton\", \"B\": \"Joseph von Fraunhofer\", \"C\": \"Jan Marek Marci\", \"D\": \"Robert Boyle\", \"E\": \"Francesco Maria Grimaldi\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4240 tokens (3240 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/h.parquet:  96%|██████████████████████████████████████████████████▋  | 372/389 [36:32<01:26,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4979 tokens (3979 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is HUMHOT?\", \"answer\": \"C\", \"A\": \"HUMHOT is a database of human meiotic recombination hot spot DNA sequences.\", \"B\": \"HUMHOT is a scientific study on human meiotic recombination hot spots.\", \"C\": \"HUMHOT is a database of human meiotic recombination hot spots.\", \"D\": \"HUMHOT is a research paper on human meiotic recombination hot spots.\", \"E\": \"HUMHOT is a website about human meiotic recombination hot spots.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4979 tokens (3979 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/h.parquet:  99%|████████████████████████████████████████████████████▎| 384/389 [37:26<00:17,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4781 tokens (3781 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the estimated annual oblique slip rate of the Hollywood fault?\", \"answer\": \"C\", \"A\": \"0.3 mm\", \"B\": \"0.5 mm\", \"C\": \"0.5-0.6 mm\", \"D\": \"0.6 mm\", \"E\": \"0.3-0.6 mm\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4781 tokens (3781 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/h.parquet: 100%|█████████████████████████████████████████████████████| 389/389 [37:55<00:00,  5.85s/it]\n",
      "../data/wikipedia_fixed/i.parquet:   6%|███▏                                                  | 20/336 [01:46<23:48,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4636 tokens (3636 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the term used to describe the differences in morphology, ecology, physiology, and behavior of insular species compared to their continental counterparts?\", \"answer\": \"A\", \"A\": \"Island syndrome\", \"B\": \"Insular gigantism\", \"C\": \"Insular dwarfism\", \"D\": \"Foster's rule\", \"E\": \"Reversed island syndrome\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4636 tokens (3636 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/i.parquet:  14%|███████▋                                              | 48/336 [04:32<19:24,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4692 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"How many times has the Italy women's national water polo team participated in the Summer Olympics?\", \"answer\": \"C\", \"A\": \"0\", \"B\": \"1\", \"C\": \"4\", \"D\": \"5\", \"E\": \"6\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4692 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/i.parquet:  15%|███████▉                                              | 49/336 [04:42<28:17,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4113 tokens (3113 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"How many times has the Italy women's national water polo team participated in the Summer Olympics?\", \"answer\": \"C\", \"A\": \"0\", \"B\": \"1\", \"C\": \"4\", \"D\": \"5\", \"E\": \"6\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4113 tokens (3113 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/i.parquet:  21%|███████████▍                                          | 71/336 [06:47<22:48,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4666 tokens (3666 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the goal of the Illinois long-term selection experiment on maize?\", \"answer\": \"B\", \"A\": \"To study the genetic architecture of response to long-term artificial selection for oil concentration in the maize kernel\", \"B\": \"To alter the levels of oil and protein in the kernel through artificial selection\", \"C\": \"To investigate the limits of selection for low oil and protein levels in the maize kernel\", \"D\": \"To analyze the progress and outcomes of long-term artificial selection in maize\", \"E\": \"To explore the genetic resources and genomics of maize selection\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4666 tokens (3666 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/i.parquet:  30%|███████████████▉                                     | 101/336 [09:30<21:27,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the purpose of an item-total correlation test in psychometrics?\", \"answer\": \"E\", \"A\": \"To determine the range of scores for different tests across individuals.\", \"B\": \"To evaluate the Pearson correlation between the scores for a given test and the average of the scores of the remaining tests.\", \"C\": \"To construct a useful single quantity for each individual that can be used to compare that individual with others in a given population.\", \"D\": \"To eliminate 'garbage' items prior to determining the factors that represent the construct.\", \"E\": \"To check whether any item in the set of tests is inconsistent with the averaged behavior of the others and can be discarded.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/i.parquet:  41%|█████████████████████▌                               | 137/336 [12:54<16:51,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the physical property that rules interface processes involving liquids?\", \"answer\": \"C\", \"A\": \"Surface tension\", \"B\": \"Capillary pressure\", \"C\": \"Disjoining pressure\", \"D\": \"Refraction\", \"E\": \"Optical lenses\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/i.parquet:  41%|█████████████████████▊                               | 138/336 [13:04<21:52,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4852 tokens (3852 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the physical property that rules interface processes involving liquids?\", \"answer\": \"C\", \"A\": \"Surface tension\", \"B\": \"Capillary pressure\", \"C\": \"Disjoining pressure\", \"D\": \"Refraction\", \"E\": \"Optical lenses\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4852 tokens (3852 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/i.parquet:  42%|██████████████████████                               | 140/336 [13:20<22:46,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4756 tokens (3756 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"According to Avi Loeb, what is the significance of searching for physical evidence of alien technology within our solar system?\", \"answer\": \"E\", \"A\": \"It is a scientific pursuit that will lead to the discovery of more advanced cosmic civilizations.\", \"B\": \"It is an arrogant assumption to think that humans are alone in the universe.\", \"C\": \"It will bring humanity together and improve our future.\", \"D\": \"It is a disorganized and rambling affair that lacks coherence.\", \"E\": \"It will elevate our species and potentially connect us to more advanced cosmic civilizations.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4756 tokens (3756 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/i.parquet:  47%|█████████████████████████                            | 159/336 [14:54<12:56,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4343 tokens (3343 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which political party did Ioannis Banias belong to from the beginning of the 1960s until its split in 1968?\", \"answer\": \"B\", \"A\": \"Communist Party of Greece\", \"B\": \"Communist Party of Greece (Interior)\", \"C\": \"Coalition of Radical Left\", \"D\": \"Renewing Communist Ecological Left\", \"E\": \"None of the above\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4343 tokens (3343 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/i.parquet:  52%|███████████████████████████▍                         | 174/336 [16:25<18:32,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4137 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the purpose of isobaric tags for relative and absolute quantitation (iTRAQ) in quantitative proteomics?\", \"answer\": \"C\", \"A\": \"To determine the amount of proteins from different sources in a single experiment\", \"B\": \"To fractionate peptides by liquid chromatography\", \"C\": \"To covalently label peptides from protein digestions with tags of varying mass\", \"D\": \"To perform a database search using fragmentation data\", \"E\": \"To analyze MS/MS spectra using software\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4137 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/i.parquet:  53%|████████████████████████████                         | 178/336 [16:47<13:54,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"Who is the editor-in-chief of Industrial & Engineering Chemistry Research?\", \"answer\": \"A\", \"A\": \"Phillip E. Savage\", \"B\": \"Milton C. Whitaker\", \"C\": \"Harrison E. Howe\", \"D\": \"F. J. van Antwerpen\", \"E\": \"None of the above\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/i.parquet:  58%|██████████████████████████████▉                      | 196/336 [24:01<12:04,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4403 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which organization is responsible for organizing the International Symposium for Personal and Commercial Spaceflight (ISPCS) and is a member of the National Space Grant College and Fellowship Program?\", \"answer\": \"C\", \"A\": \"The New Mexico Space Grant Consortium\", \"B\": \"NASA\", \"C\": \"The International Symposium for Personal and Commercial Spaceflight (ISPCS)\", \"D\": \"The congressionally funded National Space Grant College and Fellowship Program\", \"E\": \"Spaceport America\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4403 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/i.parquet:  60%|████████████████████████████████                     | 203/336 [24:45<12:23,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5147 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the purpose of the Inca complex at Pisac?\", \"answer\": \"C\", \"A\": \"To protect Cusco from attacks by the Antis\", \"B\": \"To serve as a refuge in times of danger\", \"C\": \"To function as a multi-purpose residence, citadel, observatory, and religious site\", \"D\": \"To commemorate the victories over the Cuyos\", \"E\": \"To defend the southern entrance to the Sacred Valley\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5147 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/i.parquet:  68%|███████████████████████████████████▉                 | 228/336 [27:08<08:49,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is Igor Mikhailovich Ternov known for?\", \"answer\": \"C\", \"A\": \"Discovery of new quantum effects in microscopic particle motion\", \"B\": \"Development of a new field, the theory of quantum processes in strong external fields\", \"C\": \"Both A and B\", \"D\": \"Chairmanship of MSU Physical Society\", \"E\": \"Authorship of more than 300 scientific papers\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/i.parquet:  77%|█████████████████████████████████████████            | 260/336 [35:42<06:39,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4187 tokens (3187 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which model in nuclear physics treats nucleons as pairs of particles with boson properties, and can be used to predict vibrational and rotational modes of non-spherical nuclei?\", \"answer\": \"C\", \"A\": \"Liquid drop model\", \"B\": \"Nuclear shell model\", \"C\": \"Interacting boson model\", \"D\": \"IBM1/IBM-I model\", \"E\": \"IBM2/IBM-II model\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4187 tokens (3187 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/i.parquet:  81%|██████████████████████████████████████████▋          | 271/336 [36:42<05:23,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4962 tokens (3962 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes insertion sequences?\", \"answer\": \"C\", \"A\": \"Insertion sequences are large DNA sequences that code for multiple proteins.\", \"B\": \"Insertion sequences are only found in prokaryotic genomes.\", \"C\": \"Insertion sequences are short DNA sequences that code for proteins implicated in transposition activity.\", \"D\": \"Insertion sequences are always flanked by direct repeats.\", \"E\": \"Insertion sequences are named according to the form ISn, where n is a letter.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4962 tokens (3962 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/i.parquet:  82%|███████████████████████████████████████████▌         | 276/336 [37:12<05:55,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 14948 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is insertional mutagenesis and how is it used in molecular biology?\", \"answer\": \"C\", \"A\": \"Insertional mutagenesis is the creation of mutations in DNA by the addition of one or more base pairs, and it is used to study the function of genes.\", \"B\": \"Insertional mutagenesis is the process of deleting base pairs from DNA, and it is used to create genetically modified organisms.\", \"C\": \"Insertional mutagenesis is the creation of mutations in DNA by the addition of one or more base pairs, and it is used to study the function of genes by randomly integrating transposons or viruses into the genome of an organism being studied.\", \"D\": \"Insertional mutagenesis is the process of rearranging the order of base pairs in DNA, and it is used to identify oncogenes.\", \"E\": \"Insertional mutagenesis is the process of repairing mutations in DNA, and it is used to treat genetic diseases.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 14948 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/i.parquet:  84%|████████████████████████████████████████████▋        | 283/336 [37:48<04:00,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5237 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What are the main fields of interest in the research activities of the Institute of Chemistry of the Slovak Academy of Sciences?\", \"answer\": \"C\", \"A\": \"Synthesis and structure of biologically important mono- and oligosaccharides and their derivatives\", \"B\": \"Structure and functional properties of polysaccharides, their derivatives, and conjugates with other polymers\", \"C\": \"Structure, function, and mechanism of action of glycanases\", \"D\": \"Development of physicochemical methods for structural analysis of carbohydrates\", \"E\": \"Gene engineering and nutritional and biologically active proteins\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5237 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/i.parquet:  92%|████████████████████████████████████████████████▋    | 309/336 [40:04<02:13,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4851 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is Idun Reiten's research area?\", \"answer\": \"C\", \"A\": \"Representation theory for Artinian algebras\", \"B\": \"Commutative algebra\", \"C\": \"Representation theory for Artinian algebras, commutative algebra, and homological algebra\", \"D\": \"Homological algebra\", \"E\": \"Artinian algebras\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4851 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/i.parquet: 100%|█████████████████████████████████████████████████████| 336/336 [42:45<00:00,  7.63s/it]\n",
      "../data/wikipedia_fixed/j.parquet:   7%|███▊                                                  | 19/273 [01:29<19:54,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4436 tokens (3436 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is Joanne P. Webster's current position at Imperial College London?\", \"answer\": \"C\", \"A\": \"Director of the Centre for Emerging, Endemic and Exotic Diseases\", \"B\": \"Professor of Infectious Diseases\", \"C\": \"Chair in Parasitic Diseases\", \"D\": \"Co-director of the Schistosomiasis Control Initiative\", \"E\": \"Associate director of the Imperial College London, London School of Hygiene and Tropical Medicine and Royal Veterinary College London Centre for Neglected Tropical Disease Research\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4436 tokens (3436 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/j.parquet:  14%|███████▎                                              | 37/273 [03:11<23:43,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the major difference between Journal of Chromatography A and Journal of Chromatography B?\", \"answer\": \"C\", \"A\": \"Journal of Chromatography A focuses on analytical chromatography, while Journal of Chromatography B focuses on preparative chromatography.\", \"B\": \"Journal of Chromatography A focuses on preparative chromatography, while Journal of Chromatography B focuses on analytical chromatography.\", \"C\": \"The major difference between Journal of Chromatography A and Journal of Chromatography B is the focus being on preparative chromatography instead of analytical chromatography.\", \"D\": \"Journal of Chromatography A and Journal of Chromatography B have the same focus on preparative chromatography.\", \"E\": \"Journal of Chromatography A and Journal of Chromatography B have the same focus on analytical chromatography.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/j.parquet:  23%|████████████▎                                         | 62/273 [10:48<18:13,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What contributions did John Nash make to the field of mathematics?\", \"answer\": \"C\", \"A\": \"John Nash made fundamental contributions to game theory, real algebraic geometry, differential geometry, and partial differential equations.\", \"B\": \"John Nash made fundamental contributions to game theory, real algebraic geometry, and partial differential equations.\", \"C\": \"John Nash made fundamental contributions to game theory, real algebraic geometry, differential geometry, and partial differential equations.\", \"D\": \"John Nash made fundamental contributions to game theory, differential geometry, and partial differential equations.\", \"E\": \"John Nash made fundamental contributions to game theory and partial differential equations.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/j.parquet:  53%|████████████████████████████▎                        | 146/273 [17:46<08:48,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4489 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What was Leo James Rainwater's contribution to the scientific understanding of atomic nuclei?\", \"answer\": \"C\", \"A\": \"He developed the first nuclear reactor\", \"B\": \"He discovered the connection between collective motion and particle motion in atomic nuclei\", \"C\": \"He determined the asymmetrical shapes of certain atomic nuclei\", \"D\": \"He developed the theory of nuclear fission\", \"E\": \"He developed the nuclear shell model\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4489 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/j.parquet:  85%|████████████████████████████████████████████▊        | 231/273 [24:31<02:38,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which university is G. Ian Alsop affiliated with?\", \"answer\": \"C\", \"A\": \"University of Aberdeen\", \"B\": \"Elsevier\", \"C\": \"University of Aberdeen\", \"D\": \"Thomson Reuters\", \"E\": \"Web of Science\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/j.parquet:  92%|████████████████████████████████████████████████▌    | 250/273 [25:59<01:42,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the aim of the Journal of Biological Systems?\", \"answer\": \"C\", \"A\": \"To publish articles on complex systems studies\", \"B\": \"To promote interdisciplinary approaches in Biology and in Medicine\", \"C\": \"To promote interdisciplinary approaches in Biology and in Medicine, and the study of biological situations with a variety of tools, including mathematical and general systems methods\", \"D\": \"To publish articles on environmental studies\", \"E\": \"To publish articles on epidemiology\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/j.parquet: 100%|█████████████████████████████████████████████████████| 273/273 [28:10<00:00,  6.19s/it]\n",
      "../data/wikipedia_fixed/k.parquet:  22%|███████████▊                                          | 38/173 [03:22<12:44,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5046 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Kattsund-Koster dyke swarm?\", \"answer\": \"C\", \"A\": \"The Kattsund-Koster dyke swarm is primarily composed of tholeiitic diabase and some dykes of intermediate composition.\", \"B\": \"The Kattsund-Koster dyke swarm is located in southeastern Norway and the West Coast of Sweden.\", \"C\": \"Radiometric dating has shown that the Kattsund-Koster dyke swarm is approximately 1421 million years old.\", \"D\": \"Geologists have suggested that the Kattsund-Koster dyke swarm is related to extensional tectonics.\", \"E\": \"Some dykes in the Kattsund-Koster dyke swarm have been deformed and metamorphosed into amphibolite.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5046 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/k.parquet:  24%|█████████████                                         | 42/173 [03:51<14:21,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 1046 (char 1045)\n",
      "{\"prompt\": \"What is the main difference between single chain cyclized/knotted polymers and other polymer architectures?\", \"answer\": \"C. Single chain cyclized/knotted polymers consist of intramolecular cyclized links within a single polymer chain, while other polymer architectures are formed by two or more polymer chains in combination.\", \"A\": \"A. Single chain cyclized/knotted polymers have a higher density and lower intrinsic viscosity compared to other polymer architectures.\", \"B\": \"B. Single chain cyclized/knotted polymers have a higher glass transition temperature and better elasticity than other polymer architectures.\", \"C\": \"C. Single chain cyclized/knotted polymers consist of intramolecular cyclized links within a single polymer chain, while other polymer architectures are formed by two or more polymer chains in combination.\", \"D\": \"D. Single chain cyclized/knotted polymers have a different method of interaction with plasmid DNA compared to other polymer architectures.\", \"E\": \"E. Single chain cyclized/knotted polymers can \"untie\" to form linear chains under reducing conditions, unlike other polymer architectures.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 1 column 1046 (char 1045)\n",
      "../data/wikipedia_fixed/k.parquet:  51%|███████████████████████████▍                          | 88/173 [08:03<08:15,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4966 tokens (3966 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is karyolysis?\", \"answer\": \"A\", \"A\": \"Karyolysis is the complete dissolution of the chromatin of a dying cell due to the enzymatic degradation by endonucleases.\", \"B\": \"Karyolysis is the disintegration of the cytoplasm and pyknosis of the nuclei in urine from healthy individuals.\", \"C\": \"Karyolysis is the contraction band necrosis observed in individuals who have had a myocardial infarction.\", \"D\": \"Karyolysis is the dissolution of the nucleus into apoptotic bodies in apoptosis.\", \"E\": \"Karyolysis is the presence of comet or decoy cells in urine containing malignant cells.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4966 tokens (3966 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/k.parquet:  91%|████████████████████████████████████████████████     | 157/173 [14:02<01:44,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 1 column 91 (char 90)\n",
      "{\"prompt\": \"Which version of Kellogg's theorem states that for domains which are C k , α {\\displaystyle C^{k,\\alpha }} , if the boundary data is of class C k , α {\\displaystyle C^{k,\\alpha }} , then so is the harmonic function itself?\", \"answer\": \"B\", \"A\": \"The first version of Kellogg's theorem\", \"B\": \"The second version of Kellogg's theorem\", \"C\": \"Kellogg's method of proof\", \"D\": \"The Poisson kernel\", \"E\": \"The boundary Schauder estimates for elliptic partial differential equations\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 1 column 91 (char 90)\n",
      "../data/wikipedia_fixed/k.parquet:  92%|█████████████████████████████████████████████████    | 160/173 [14:26<01:25,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"When did the Kent Coalfield experience its maximum coal output?\", \"answer\": \"D\", \"A\": \"1911\", \"B\": \"1924\", \"C\": \"1945\", \"D\": \"1936\", \"E\": \"1989\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/k.parquet: 100%|█████████████████████████████████████████████████████| 173/173 [20:58<00:00,  7.28s/it]\n",
      "../data/wikipedia_fixed/l.parquet:   1%|▋                                                    | 9/701 [00:40<1:00:23,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"Which of the following statements accurately describes the LEAPER genetic engineering technique?\", \"answer\": \"C\", \"A\": \"LEAPER targets DNA gene editing techniques to modify defective genes directly.\", \"B\": \"LEAPER utilizes CRISPR-Cas proteins to edit messenger RNA (mRNA) for protein synthesis.\", \"C\": \"LEAPER recruits native ADAR enzymes to edit RNA by using engineered strands of RNA.\", \"D\": \"LEAPER achieves targeted RNA editing through the use of short engineered ADAR-recruiting DNAs (arDNAs).\", \"E\": \"LEAPER was discovered by a team of researchers at Harvard University in the United States.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/l.parquet:   3%|█▍                                                  | 20/701 [07:04<1:28:46,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4681 tokens (3681 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the list of sequenced plant genomes?\", \"answer\": \"B\", \"A\": \"The list includes all plant species with publicly available complete genome sequences.\", \"B\": \"The list only includes plant species with complete genome sequences that have been assembled, annotated, and published.\", \"C\": \"The list includes both assembled and unassembled plant genomes.\", \"D\": \"The list includes plant species with organelle-only sequences.\", \"E\": \"The list includes plant species with sequences announced in press releases or websites.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4681 tokens (3681 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:   3%|█▋                                                  | 22/701 [07:19<1:22:12,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4400 tokens (3400 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is Laurence Chisholm Young known for?\", \"answer\": \"C\", \"A\": \"Contributions to measure theory\", \"B\": \"Contributions to potential theory\", \"C\": \"Contributions to measure theory, the calculus of variations, optimal control theory, and potential theory\", \"D\": \"Contributions to the calculus of variations\", \"E\": \"Contributions to optimal control theory\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4400 tokens (3400 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:   4%|█▉                                                  | 26/701 [07:42<1:02:14,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 3681 (char 3680)\n",
      "{\"prompt\": \"What is Luciana Castellina's background and career?\", \"answer\": \"B\", \"A\": \"Luciana Castellina was born in Rome on 9 August 1929 and graduated in law from Sapienza University of Rome. She joined the Italian Communist Party in 1947 and started her career in journalism in the 1950s. She worked for several newspapers, including Paese Sera, L'Unità, Il Manifesto, and Liberazione. Castellina focused her writings on workers' rights, feminism, and communism. She co-founded the Proletarian Unity Party for Communism in 1974 and served four terms in the Italian Chamber of Deputies and twenty years in the European Parliament. She also served as chair of the Committee on Culture, Youth, Education and the Media and of the Committee on External Economic Relations in the European Parliament. Castellina was president of Italia cinema from 1998 to 2003 and served as editor of Nuova Generazione, Liberazione, and played an important role at Il Manifesto. She received honors from France and Argentina. Left Ecology Freedom supported her as a possible successor to Giorgio Napolitano as President of Italy in the 2015 presidential elections, but Sergio Mattarella was ultimately elected. She is married to communist leader Alfredo Reichlin and has two economist children.\", \"B\": \"Luciana Castellina was born in Rome on 9 August 1929 and graduated in law from Sapienza University of Rome. She joined the Italian Communist Party in 1947 and started her career in journalism in the 1950s. She worked for several newspapers, including Paese Sera, L'Unità, Il Manifesto, and Liberazione. Castellina focused her writings on workers' rights, feminism, and communism. She co-founded the Proletarian Unity Party for Communism in 1974 and served four terms in the Italian Chamber of Deputies and twenty years in the European Parliament. She also served as chair of the Committee on Culture, Youth, Education and the Media and of the Committee on External Economic Relations in the European Parliament. Castellina was president of Italia cinema from 1998 to 2003 and served as editor of Nuova Generazione, Liberazione, and played an important role at Il Manifesto. She received honors from France and Argentina. Left Ecology Freedom supported her as a possible successor to Giorgio Napolitano as President of Italy in the 2015 presidential elections, but Sergio Mattarella was ultimately elected. She is married to communist leader Alfredo Reichlin and has two economist children.\", \"C\": \"Luciana Castellina was born in Rome on 9 August 1929 and graduated in law from Sapienza University of Rome. She joined the Italian Communist Party in 1947 and started her career in journalism in the 1950s. She worked for several newspapers, including Paese Sera, L'Unità, Il Manifesto, and Liberazione. Castellina focused her writings on workers' rights, feminism, and communism. She co-founded the Proletarian Unity Party for Communism in 1974 and served four terms in the Italian Chamber of Deputies and twenty years in the European Parliament. She also served as chair of the Committee on Culture, Youth, Education and the Media and of the Committee on External Economic Relations in the European Parliament. Castellina was president of Italia cinema from 1998 to 2003 and served as editor of Nuova Generazione, Liberazione, and played an important role at Il Manifesto. She received honors from France and Argentina. Left Ecology Freedom supported her as a possible successor to Giorgio Napolitano as President of Italy in the 2015 presidential elections, but Sergio Mattarella was ultimately elected. She is married to communist leader Alfredo Reichlin and has two economist children.\", \"D\": \"Luciana Castellina was born in Rome on 9 August 1929 and graduated in law from Sapienza University of Rome. She joined the Italian Communist Party in 1947 and started her career in journalism in the 1950s. She worked for several newspapers, including Paese Sera, L'Unità, Il Manifesto, and Liberazione. Castellina focused her writings on workers' rights, feminism, and communism. She co-founded the Proletarian Unity Party for Communism in 1974 and served four terms in the Italian Chamber of Deputies and twenty years in the European Parliament. She also served as chair of the Committee on Culture, Youth, Education and the Media and of the Committee on External Economic Relations in the European Parliament. Castellina was president of Italia cinema from 1998 to 2003 and served as editor of Nuova Generazione, Liberazione, and played an important role\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 3681 (char 3680)\n",
      "../data/wikipedia_fixed/l.parquet:   4%|██                                                  | 28/701 [08:51<3:33:35, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4908 tokens (3908 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"When did Leendert Bosch obtain his title of Doctor and what was the title of his dissertation?\", \"answer\": \"A\", \"A\": \"Leendert Bosch obtained his title of Doctor at Delft University of Technology in 1955, with a dissertation titled 'Biochemische en endocrinologische onderzoekingen van normaal en neoplastisch weefsel : de stofwisseling van oestrogeen producerende ovariumtumoren' (Biochemical and endocrinological research of normal and neoplastic tissue: the metabolism of oestrogen producing ovarian tumors).\", \"B\": \"Leendert Bosch obtained his title of Doctor at Leiden University in 1964, with a dissertation titled 'Biochemische en endocrinologische onderzoekingen van normaal en neoplastisch weefsel : de stofwisseling van oestrogeen producerende ovariumtumoren' (Biochemical and endocrinological research of normal and neoplastic tissue: the metabolism of oestrogen producing ovarian tumors).\", \"C\": \"Leendert Bosch obtained his title of Doctor at Delft University of Technology in 1964, with a dissertation titled 'Biochemische en endocrinologische onderzoekingen van normaal en neoplastisch weefsel : de stofwisseling van oestrogeen producerende ovariumtumoren' (Biochemical and endocrinological research of normal and neoplastic tissue: the metabolism of oestrogen producing ovarian tumors).\", \"D\": \"Leendert Bosch obtained his title of Doctor at Leiden University in 1955, with a dissertation titled 'Biochemische en endocrinologische onderzoekingen van normaal en neoplastisch weefsel : de stofwisseling van oestrogeen producerende ovariumtumoren' (Biochemical and endocrinological research of normal and neoplastic tissue: the metabolism of oestrogen producing ovarian tumors).\", \"E\": \"Leendert Bosch obtained his title of Doctor at Delft University of Technology in 1955, with a dissertation titled 'Biochemische en endocrinologische onderzoekingen van normaal en neoplastisch weefsel : de stofwisseling van oestrogeen producerende ovariumtumoren' (Biochemical and endocrinological research of normal and neoplastic tissue: the metabolism of oestrogen producing ovarian tumors).\" }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4908 tokens (3908 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  11%|█████▍                                              | 74/701 [12:42<1:04:21,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4508 tokens (3508 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the purpose of the Leiden Open Variation Database (LOVD)?\", \"answer\": \"C\", \"A\": \"To collect and display all DNA variants found in individuals worldwide\", \"B\": \"To provide direct access to up-to-date information on sequence variation\", \"C\": \"To collect and display variants in the DNA sequence that could be causally connected to genetic diseases\", \"D\": \"To unify gene-centered, locus-specific databases (LSDBs) with each other\", \"E\": \"To create and maintain a gene sequence variation database using the internet\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4508 tokens (3508 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  11%|█████▊                                              | 78/701 [13:10<1:09:28,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4207 tokens (3207 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the list of EC numbers for the fifth group, EC 5, isomerases?\", \"answer\": \"B\", \"A\": \"The list of EC numbers for the fifth group, EC 5, isomerases, is organized alphabetically.\", \"B\": \"The list of EC numbers for the fifth group, EC 5, isomerases, is organized numerically according to the Nomenclature Committee of the International Union of Biochemistry and Molecular Biology.\", \"C\": \"The list of EC numbers for the fifth group, EC 5, isomerases, is organized based on the order of discovery.\", \"D\": \"The list of EC numbers for the fifth group, EC 5, isomerases, is organized based on the substrate specificity of the enzymes.\", \"E\": \"The list of EC numbers for the fifth group, EC 5, isomerases, is organized randomly.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4207 tokens (3207 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  12%|██████▎                                               | 82/701 [13:31<50:51,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4262 tokens (3262 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which portal provides original works on the topic of Paleontology in Iowa?\", \"answer\": \"C\", \"A\": \"Paleontology portal\", \"B\": \"Iowa portal\", \"C\": \"Wikisource\", \"D\": \"Paleobiology Database\", \"E\": \"Fossilworks\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4262 tokens (3262 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  15%|███████▊                                             | 104/701 [15:16<48:17,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 7567 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which group is named as the dihedral group of order 2n?\", \"answer\": \"B\", \"A\": \"Cyclic group of order n\", \"B\": \"Dihedral group of order 2n\", \"C\": \"Symmetric group of degree n\", \"D\": \"Alternating group of degree n\", \"E\": \"Quaternion group of order 8\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 7567 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  15%|███████▋                                           | 105/701 [15:27<1:05:17,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4931 tokens (3931 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which group is named as the dihedral group of order 2n?\", \"answer\": \"B\", \"A\": \"Cyclic group of order n\", \"B\": \"Dihedral group of order 2n\", \"C\": \"Symmetric group of degree n\", \"D\": \"Alternating group of degree n\", \"E\": \"Quaternion group of order 8\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4931 tokens (3931 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  16%|████████▍                                            | 111/701 [15:59<46:37,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which player holds the record for the most rushing yards in a single game for the Houston Texans?\", \"answer\": \"D\", \"A\": \"Arian Foster\", \"B\": \"Ben Tate\", \"C\": \"Alfred Blue\", \"D\": \"Lamar Miller\", \"E\": \"Ryan Moats\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/l.parquet:  17%|████████▉                                            | 119/701 [16:37<38:56,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4709 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"During which geological period did the Ljusdal Batholith in central Sweden form?\", \"answer\": \"B\", \"A\": \"Paleozoic\", \"B\": \"Paleoproterozoic\", \"C\": \"Mesozoic\", \"D\": \"Neoproterozoic\", \"E\": \"Cenozoic\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4709 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  20%|██████████▌                                          | 139/701 [18:27<39:34,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4205 tokens (3205 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which player made their senior debut for an Australian Football League (AFL) club in 2005?\", \"answer\": \"C\", \"A\": \"Chris Knights\", \"B\": \"Scott Thompson\", \"C\": \"Nathan van Berlo\", \"D\": \"Jayden Attard\", \"E\": \"Travis Baird\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4205 tokens (3205 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  21%|███████████▏                                         | 148/701 [19:27<55:34,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"During which tournament did Gareth Bale score in all three group matches, helping Wales reach the semi-finals?\", \"answer\": \"C\", \"A\": \"UEFA Euro 2008\", \"B\": \"UEFA Euro 2016\", \"C\": \"UEFA Euro 2020\", \"D\": \"FIFA World Cup 2022\", \"E\": \"FIFA World Cup 2018\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/l.parquet:  27%|██████████████▏                                      | 188/701 [22:50<56:44,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 30487 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes L. H. C. Tippett's career and contributions?\", \"answer\": \"C\", \"A\": \"L. H. C. Tippett was born in Cornwall and attended St Austell County Grammar School.\", \"B\": \"Tippett studied physics at Imperial College London and statistics at the Galton Laboratory, University College London.\", \"C\": \"Tippett spent his entire career at the Shirley Institute in Manchester and made significant contributions to extreme value theory.\", \"D\": \"Tippett invented the random number table and received the Shewart Medal of the American Society for Quality Control.\", \"E\": \"After retiring, Tippett became an UNIDO consultant and was active in India.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 30487 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  32%|█████████████████                                    | 225/701 [26:13<36:35,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4647 tokens (3647 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which statement accurately describes the mineral list on the mindat.org website?\", \"answer\": \"C\", \"A\": \"The mineral list on the mindat.org website is organized alphabetically by mineral name.\", \"B\": \"The mineral list on the mindat.org website is organized by the geological formation in which the minerals are found.\", \"C\": \"The mineral list on the mindat.org website is organized by the chemical composition of the minerals.\", \"D\": \"The mineral list on the mindat.org website is organized by the country in which the minerals are located.\", \"E\": \"The mineral list on the mindat.org website is organized by the physical properties of the minerals.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4647 tokens (3647 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  39%|████████████████████▊                                | 276/701 [30:36<38:03,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 10212 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Who created the Louise Hay Award and when was it first issued?\", \"answer\": \"B\", \"A\": \"The Louise Hay Award is a mathematics award planned in 1990 and first issued in 1991 by the Association for Women in Mathematics in recognition of contributions as a math educator.\", \"B\": \"The Louise Hay Award was created in 1990 and first issued in 1991 by the Association for Women in Mathematics.\", \"C\": \"The Louise Hay Award was established in 1990 and initially presented in 1991 by the Association for Women in Mathematics to acknowledge the contributions of math educators.\", \"D\": \"The Association for Women in Mathematics created the Louise Hay Award in 1990 and it was first issued in 1991.\", \"E\": \"The Louise Hay Award, which honors contributions as a math educator, was created by the Association for Women in Mathematics in 1990 and was first issued in 1991.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 10212 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  40%|█████████████████████                                | 278/701 [30:50<41:48,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 21757 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is Lawrence Maxwell Krauss known for advocating and promoting?\", \"answer\": \"C\", \"A\": \"Advancing the field of theoretical physics\", \"B\": \"Promoting scientific skepticism\", \"C\": \"Reducing the influence of superstition and religious dogma\", \"D\": \"Supporting public policy based on empirical data\", \"E\": \"Advocating for science education\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 21757 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  41%|█████████████████████▍                               | 284/701 [31:25<35:57,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4929 tokens (3929 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What are the common side effects of lithium?\", \"answer\": \"B\", \"A\": \"Increased urination and shakiness of the hands\", \"B\": \"Increased urination, shakiness of the hands, and increased thirst\", \"C\": \"Increased thirst and poor coordination\", \"D\": \"Poor coordination and ringing in the ears\", \"E\": \"Ringing in the ears and vomiting\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4929 tokens (3929 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  49%|█████████████████████████▊                           | 342/701 [36:08<37:52,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"When was the FIFA Club World Championship first contested and why was it not held between 2001 and 2004?\", \"answer\": \"B\", \"A\": \"The FIFA Club World Championship was first contested in 2000 and was not held between 2001 and 2004 due to the collapse of FIFA's marketing partner International Sport and Leisure.\", \"B\": \"The FIFA Club World Championship was first contested in 2000 and was not held between 2001 and 2004 due to a combination of factors, most importantly the collapse of FIFA's marketing partner International Sport and Leisure.\", \"C\": \"The FIFA Club World Championship was first contested in 2000 and was not held between 2001 and 2004 due to the COVID-19 pandemic.\", \"D\": \"The FIFA Club World Championship was first contested in 2000 and was not held between 2001 and 2004 due to a change in format.\", \"E\": \"The FIFA Club World Championship was first contested in 2000 and was not held between 2001 and 2004 due to the host nation's national champion not participating.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/l.parquet:  50%|██████████████████████████▌                          | 351/701 [36:55<27:43,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4127 tokens (3127 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is Linda Hong Zhao's area of specialization?\", \"answer\": \"C\", \"A\": \"Modern machine learning methods\", \"B\": \"Replicability in science\", \"C\": \"High dimensional data\", \"D\": \"Housing price prediction\", \"E\": \"Bayesian methods\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4127 tokens (3127 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  50%|██████████████████████████▌                          | 352/701 [37:05<37:21,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4305 tokens (3305 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is Linda Hong Zhao's area of specialization?\", \"answer\": \"C\", \"A\": \"Modern machine learning methods\", \"B\": \"Replicability in science\", \"C\": \"High dimensional data\", \"D\": \"Housing price prediction\", \"E\": \"Bayesian methods\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4305 tokens (3305 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  51%|███████████████████████████▏                         | 360/701 [37:48<27:33,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5215 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which haplogroup is one of the major classifications of Y-chromosome types found in human male lines and is widespread across Eurasia?\", \"answer\": \"A\", \"A\": \"Haplogroup R1a\", \"B\": \"Haplogroup R1b\", \"C\": \"Haplogroup J2\", \"D\": \"Haplogroup E1b1b\", \"E\": \"Haplogroup G2a\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5215 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  52%|███████████████████████████▌                         | 365/701 [38:13<25:38,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5405 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which conference, also known as the Woodstock of physics, took place in 1987 and was organized by the American Physical Society?\", \"answer\": \"E\", \"A\": \"1959 Richard Feynman's There's Plenty of Room at the Bottom\", \"B\": \"1947 Shelter Island Conference\", \"C\": \"1948 Pocono Conference\", \"D\": \"1949 Oldstone Conference\", \"E\": \"1987 American Physical Society meeting\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5405 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  56%|█████████████████████████████▊                       | 395/701 [40:49<28:55,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6251 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the purpose of the quinquennial Piping Plover International Census?\", \"answer\": \"B\", \"A\": \"To estimate the population of the piping plover in Canada\", \"B\": \"To count the number of piping plover nests using volunteers\", \"C\": \"To calculate the density of the piping plover and extrapolate the numbers over its range\", \"D\": \"To assess the decline in population of the piping plover over the past ten years\", \"E\": \"To confirm the continued existence of the piping plover\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6251 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  58%|██████████████████████████████▋                      | 406/701 [41:49<23:02,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5897 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the L-shell parameter used to describe?\", \"answer\": \"C\", \"A\": \"The motions of low-energy charged particles in the Earth's magnetic field\", \"B\": \"The magnitude of the magnetic field vector\", \"C\": \"A particular set of planetary magnetic field lines\", \"D\": \"The energy of charged particles moving in a magnetic field\", \"E\": \"The spatial scale for changes in the magnetic field\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5897 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  58%|██████████████████████████████▉                      | 410/701 [42:16<26:31,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4723 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which symbol is commonly used to represent the apparent radial velocity in astronomy?\", \"answer\": \"B\", \"A\": \"τ\", \"B\": \"cz\", \"C\": \"z\", \"D\": \"μ\", \"E\": \"π\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4723 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  63%|█████████████████████████████████▍                   | 442/701 [44:50<22:12,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4245 tokens (3245 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the LISICON family of solids?\", \"answer\": \"D\", \"A\": \"LISICON compounds have low ionic conductivity due to the strong bonds between lithium ions.\", \"B\": \"The crystal structure of LISICON consists of a network of Li14Zn(GeO4)4.\", \"C\": \"LISICON compounds are limited to one-dimensional diffusion due to the shape of the channels through which lithium ions can diffuse.\", \"D\": \"LISICON compounds have high ionic conductivity and allow for quick and easy diffusion of lithium ions.\", \"E\": \"LISICON compounds are unstable in contact with CO2 and atmospheric gases.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4245 tokens (3245 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  68%|████████████████████████████████████▎                | 480/701 [47:56<16:14,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6784 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"In which region is FC Ingolstadt 04 located?\", \"answer\": \"B\", \"A\": \"North Rhine-Westphalia\", \"B\": \"Bavaria\", \"C\": \"Berlin\", \"D\": \"Hamburg\", \"E\": \"Saxony\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6784 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  71%|█████████████████████████████████████▋               | 499/701 [49:25<16:38,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4927 tokens (3927 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which technique allows for the direct probing and imaging of membranes in situ with little to no chemical or physical modification?\", \"answer\": \"C\", \"A\": \"X-ray diffraction\", \"B\": \"Electrical resistance\", \"C\": \"Atomic force microscopy (AFM)\", \"D\": \"Fluorescence microscopy\", \"E\": \"Dual polarisation interferometry\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4927 tokens (3927 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  72%|██████████████████████████████████████               | 504/701 [50:09<25:41,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4149 tokens (3149 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"In which year did Miroslav Klose score his first international hat-trick?\", \"answer\": \"B\", \"A\": \"2001\", \"B\": \"2002\", \"C\": \"2006\", \"D\": \"2008\", \"E\": \"2014\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4149 tokens (3149 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  74%|███████████████████████████████████████▏             | 519/701 [51:37<16:29,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6719 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What was Leonidas Kyrkos' involvement in politics and his career?\", \"answer\": \"C\", \"A\": \"Leonidas Kyrkos was a Greek politician who served as a member of the Hellenic Parliament and the European Parliament.\", \"B\": \"Leonidas Kyrkos was a Greek journalist who worked for the leftist newspaper I Avgi.\", \"C\": \"Leonidas Kyrkos was a Greek leftist politician who was a member of the Hellenic Parliament and the European Parliament. He also held leadership positions in the Communist Party of Greece and the Coalition of the Left and Progress.\", \"D\": \"Leonidas Kyrkos was a Greek resistance fighter during World War II and later became a journalist.\", \"E\": \"Leonidas Kyrkos was a Greek politician who ran for the position of President of Greece in 2000.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6719 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/l.parquet:  77%|█████████████████████████████████████████            | 543/701 [53:51<16:20,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4113 tokens (3113 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the Littlewood–Offord problem in combinatorial geometry?\", \"answer\": \"C\", \"A\": \"The Littlewood–Offord problem is the problem of determining the number of subsums of a set of vectors that fall in a given convex set.\", \"B\": \"The Littlewood–Offord problem is the problem of determining the number of subsets of a set of vectors whose summation is in a given convex set.\", \"C\": \"The Littlewood–Offord problem is the problem of determining the number of subsets of a set of vectors whose summation is in a given convex set.\", \"D\": \"The Littlewood–Offord problem is the problem of determining the number of subsums of a set of vectors that fall outside a given convex set.\", \"E\": \"The Littlewood–Offord problem is the problem of determining the number of subsets of a set of vectors whose summation is outside a given convex set.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4113 tokens (3113 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  88%|██████████████████████████████████████████████▍      | 614/701 [59:45<06:18,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4829 tokens (3829 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which national park in the Alps was established first?\", \"answer\": \"A\", \"A\": \"Swiss National Park (1914)\", \"B\": \"Gran Paradiso National Park (1922, coupled with the Vanoise)\", \"C\": \"Stelvio National Park (1935)\", \"D\": \"Val Grande National Park (1992)\", \"E\": \"Dolomiti Bellunesi National Park (1990)\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4829 tokens (3829 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  89%|█████████████████████████████████████████████▎     | 623/701 [1:00:44<08:16,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4453 tokens (3453 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the target audience for the game L – A Mathemagical Adventure and what are the puzzles based on?\", \"answer\": \"C\", \"A\": \"Primary school children and logical thinking\", \"B\": \"University students and critical thinking\", \"C\": \"Mid-late primary (elementary) school children and critical thinking\", \"D\": \"High school students and mathematical concepts\", \"E\": \"Adults and spatial reasoning\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4453 tokens (3453 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet:  95%|████████████████████████████████████████████████▍  | 665/701 [1:04:17<02:27,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4157 tokens (3157 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"When was Lewis and Clark Cavern National Monument first established and declared by President Taft?\", \"answer\": \"C\", \"A\": \"May 11, 1908\", \"B\": \"May 16, 1911\", \"C\": \"May 11, 1908\", \"D\": \"August 24, 1937\", \"E\": \"April 22, 1938\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4157 tokens (3157 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/l.parquet: 100%|███████████████████████████████████████████████████| 701/701 [1:07:11<00:00,  5.75s/it]\n",
      "../data/wikipedia_fixed/m.parquet:   5%|██▌                                                 | 34/706 [03:14<1:09:07,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4812 tokens (3812 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the function of the mir-BART2 microRNA precursor found in Human herpesvirus 4 (Epstein–Barr virus) and Cercopithicine herpesvirus 15?\", \"answer\": \"C\", \"A\": \"It is expressed only in the lytic stage of infection.\", \"B\": \"It targets the intronic regions of the BART gene.\", \"C\": \"It is thought to target the virally encoded DNA polymerase BALF5 for degradation.\", \"D\": \"It is excised from the 3' arm of the hairpin.\", \"E\": \"It is found in the Bam HI-A region rightward transcript gene.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4812 tokens (3812 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  14%|███████▏                                            | 98/706 [08:42<1:00:00,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 1 column 133 (char 132)\n",
      "{\"prompt\": \"According to the Meyers-Serrin theorem in functional analysis, what does the equivalence W k , p ( Ω ) = H k , p ( Ω ) {\\displaystyle W^{k,p}(\\Omega )=H^{k,p}(\\Omega )} establish?\", \"answer\": \"C\", \"A\": \"The equivalence between smooth functions and weak derivatives\", \"B\": \"The relationship between Sobolev spaces and L p norms\", \"C\": \"The equivalence between two different definitions of Sobolev spaces\", \"D\": \"The requirement of smoothness for the domain Ω {\\displaystyle \\Omega }\", \"E\": \"The confusion about the relationship of these spaces in the literature\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 1 column 133 (char 132)\n",
      "../data/wikipedia_fixed/m.parquet:  17%|█████████                                            | 120/706 [11:01<50:46,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4236 tokens (3236 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What are the three phases of METIS' multilevel approach for graph partitioning?\", \"answer\": \"C\", \"A\": \"Coarsen the graph and compute a partition of GN\", \"B\": \"Coarsen the graph and project the partition back through the sequence\", \"C\": \"Coarsen the graph, compute a partition of GN, and project the partition back through the sequence\", \"D\": \"Compute a partition of GN and refine it with respect to each graph\", \"E\": \"Compute a partition of GN and project the partition back through the sequence\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4236 tokens (3236 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  26%|██████████████                                       | 187/706 [17:04<44:17,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4443 tokens (3443 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the function of microRNAs in molecular biology?\", \"answer\": \"C\", \"A\": \"MicroRNAs are short RNA molecules.\", \"B\": \"MicroRNAs regulate the expression levels of other genes.\", \"C\": \"MicroRNAs function to regulate the expression levels of other genes by several mechanisms.\", \"D\": \"MicroRNAs are involved in defense responses in Triticum aestivum L. during Puccinia graminis f.sp. tritici infection.\", \"E\": \"MicroRNAs are conserved in a large number of diverse plant species.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4443 tokens (3443 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  34%|██████████████████                                   | 240/706 [21:45<38:04,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4752 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes marinobufagenin?\", \"answer\": \"C\", \"A\": \"Marinobufagenin is a hormone secreted by the human body during kidney failure.\", \"B\": \"Marinobufagenin is a vasoconstrictor with effects similar to digitalis.\", \"C\": \"Marinobufagenin can be found in the plasma and urine of human subjects with myocardial infarction, kidney failure, and heart failure.\", \"D\": \"Marinobufagenin is a component of the skin secretion of the toad Bufo rubescens.\", \"E\": \"Marinobufagenin is an antimicrobial compound isolated from the toad Bufo rubescens.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4752 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/m.parquet:  38%|███████████████████▉                                 | 266/706 [24:03<47:35,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"According to Martin Heidegger, what is the concept of time in Western philosophy primarily based on?\", \"answer\": \"A\", \"A\": \"The privileging of Presence over Absence\", \"B\": \"The unity of three ecstases: the past, the present, and the future\", \"C\": \"The concept of time as a duration\", \"D\": \"The concept of time as an instant\", \"E\": \"The desire for immediate access to meaning\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/m.parquet:  42%|██████████████████████▏                              | 296/706 [26:48<39:26,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4646 tokens (3646 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the White cell?\", \"answer\": \"C\", \"A\": \"The White cell is made up of two opposing spherical mirrors and allows for high numerical aperture.\", \"B\": \"The White cell was first described in 1942 by John U. White and is constructed using three spherical, concave mirrors.\", \"C\": \"The White cell is still the most commonly used multipass cell and provides many advantages, including easy control of the number of traversals and high numerical aperture.\", \"D\": \"The White cell is constructed using two identical spherical mirrors, each having a hole carefully machined into its center, and is known for using multiple passes.\", \"E\": \"The White cell is a circular multipass reflection cell that relies on a circular arrangement of mirrors and provides a small detection volume.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4646 tokens (3646 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  52%|███████████████████████████▌                         | 367/706 [32:47<30:05,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4372 tokens (3372 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes metasilicic acid?\", \"answer\": \"C\", \"A\": \"Metasilicic acid is a common compound found in nature.\", \"B\": \"Metasilicic acid has the chemical formula (HO)2SiO2.\", \"C\": \"Metasilicic acid is a hypothetical chemical compound with the formula (HO)2SiO.\", \"D\": \"Metasilicic acid is a polymerized form of silicic acid.\", \"E\": \"Metasilicic acid is primarily studied in the field of organic chemistry.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4372 tokens (3372 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  53%|████████████████████████████                         | 374/706 [33:30<27:56,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is Michael T. Anderson's area of research?\", \"answer\": \"C\", \"A\": \"Differential geometry\", \"B\": \"Number theory\", \"C\": \"Ricci curvature and minimal surfaces\", \"D\": \"Algebraic topology\", \"E\": \"Quantum mechanics\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/m.parquet:  57%|█████████████████████████████▉                       | 399/706 [35:41<29:10,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4933 tokens (3933 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes Michael Genesereth?\", \"answer\": \"D\", \"A\": \"Michael Genesereth is a professor in the Physics Department at Massachusetts Institute of Technology.\", \"B\": \"Michael Genesereth is the author of the influential Game Description Language (GDL) and Knowledge Interchange Format (KIF).\", \"C\": \"Michael Genesereth received his Ph.D. in Applied Mathematics from Stanford University.\", \"D\": \"Michael Genesereth is a professor in the Computer Science Department at Stanford University and a professor by courtesy in the Stanford Law School.\", \"E\": \"Michael Genesereth is the director of the Logic Group at Harvard University.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4933 tokens (3933 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  59%|███████████████████████████████                      | 414/706 [37:07<30:18,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is a mountain pass and what role has it played throughout history?\", \"answer\": \"C\", \"A\": \"A mountain pass is a navigable route through a mountain range or over a ridge, and it has played a key role in trade, war, and both human and animal migration throughout history.\", \"B\": \"A mountain pass is a high vantage point that offers relatively easy travel between valleys, and it has been used for trade routes, communications, cultural exchange, and military expeditions.\", \"C\": \"A mountain pass is a navigable route through a mountain range or over a ridge, and it has played a key role in trade, war, and both human and animal migration throughout history. It is often found just above the source of a river and may be short or a valley many kilometers long.\", \"D\": \"A mountain pass is a topographic saddle characterized by contour lines with an hourglass shape on a topographic map. It is typically formed between two volcanic peaks or created by erosion from water or wind.\", \"E\": \"A mountain pass is a route between two mountain tops with a minimum of descent, and it is denoted by the suffix 'La' in Tibetan, Ladhakhi, and several other regional languages.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/m.parquet:  61%|████████████████████████████████▌                    | 434/706 [39:00<27:53,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the Malvern Line or Malvern Lineament?\", \"answer\": \"C\", \"A\": \"A series of faults and folds in the Malvern Hills.\", \"B\": \"A north-south aligned lineament running through the Malvern Hills.\", \"C\": \"A north-south aligned lineament consisting of faults and folds that bring old Malvernian rocks to the surface.\", \"D\": \"A line marking the edge of two terranes - the Wrekin Terrane and the Charnwood Terrane.\", \"E\": \"A line extending southwards towards Bristol and northwards past Stourport.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/m.parquet:  67%|███████████████████████████████████▎                 | 471/706 [41:59<18:20,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What was Melvin Burt Gottlieb's role at the Princeton Plasma Physics Laboratory?\", \"answer\": \"C\", \"A\": \"High-energy physicist\", \"B\": \"Director\", \"C\": \"Both a high-energy physicist and director\", \"D\": \"Assistant professor\", \"E\": \"Technical advisor\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/m.parquet:  70%|████████████████████████████████████▉                | 492/706 [49:26<16:33,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4459 tokens (3459 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Where is the Museum Vicente Pallotti located?\", \"answer\": \"A\", \"A\": \"Avenida Presidente Vargas, 115 in Santa Maria, Rio Grande do Sul, Brazil\", \"B\": \"Vale-Vêneto, Brazil\", \"C\": \"College Máximo Palotino in Santa Maria, Brazil\", \"D\": \"Paleontological Site Sanga of Alemoa, Brazil\", \"E\": \"FAPAS, Brazil\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4459 tokens (3459 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  70%|█████████████████████████████████████▎               | 497/706 [49:52<15:31,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4764 tokens (3764 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What was the cause of Elizabeth Roberts' death?\", \"answer\": \"C\", \"A\": \"She was shot multiple times\", \"B\": \"She was stabbed to death\", \"C\": \"She was strangled with a cord and shot in the head\", \"D\": \"She died from natural causes\", \"E\": \"She drowned in a lake\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4764 tokens (3764 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  74%|███████████████████████████████████████              | 520/706 [52:11<20:25,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the significance of the discovery of mud cracks in Gale Crater on Mars?\", \"answer\": \"C\", \"A\": \"The mud cracks on Mars were formed from drying mud and were buried by sediment before being exposed again by wind erosion.\", \"B\": \"The mud cracks on Mars were caused by the pressure of overlying sediments fracturing rock.\", \"C\": \"The discovery of mud cracks suggests that there were dry periods on Mars when lakes disappeared.\", \"D\": \"The mud cracks on Mars were filled with erosion resistant material, forming raised ridges.\", \"E\": \"The discovery of mud cracks provides evidence of ancient lakes in both older and younger layers of mudstone.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/m.parquet:  79%|█████████████████████████████████████████▊           | 557/706 [55:23<11:16,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4732 tokens (3732 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes multidimensional analysis (MDA)?\", \"answer\": \"C\", \"A\": \"MDA is a data analysis process that groups data into two categories: data dimensions and measurements.\", \"B\": \"MDA is a data analysis process that groups data into three or more dimensions.\", \"C\": \"MDA tends to be applied only to data sets with three or more dimensions.\", \"D\": \"MDA is a data analysis process that groups data into two dimensions.\", \"E\": \"MDA is a data analysis process that groups data into one dimension.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4732 tokens (3732 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  79%|█████████████████████████████████████████▉           | 559/706 [55:37<13:03,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4470 tokens (3470 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which compound is also known as O-methylcatechol?\", \"answer\": \"A\", \"A\": \"Guaiacol\", \"B\": \"3-Methylcatechol\", \"C\": \"4-Methylcatechol\", \"D\": \"Methylcatechol\", \"E\": \"None of the above\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4470 tokens (3470 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  84%|████████████████████████████████████████████▌        | 593/706 [58:25<07:07,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4559 tokens (3559 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Who is the recipient of the 2022 Margaret Oakley Dayhoff Award from the Biophysical Society?\", \"answer\": \"E\", \"A\": \"Diane Lidke\", \"B\": \"Lucy R. Forrest\", \"C\": \"Sarah Veatch\", \"D\": \"Antonina Roll-Mecak\", \"E\": \"Gabriela Schlau-Cohen\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4559 tokens (3559 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  88%|████████████████████████████████████████████▊      | 620/706 [1:00:48<08:39,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4232 tokens (3232 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Musgrave orogeny in Central Australia?\", \"answer\": \"A\", \"A\": \"The Musgrave orogeny was one of three Mesoproterozoic orogenic events that occurred between 1.22 and 1.12 billion years ago in the Musgrave Province of Central Australia, resulting from the convergence of the North, West, and South Australian cratons.\", \"B\": \"The Musgrave orogeny occurred 1.29 billion years ago and led to the formation of the Musgrave Province in Central Australia.\", \"C\": \"The granites of the Pitjantjatjara Supersuite were formed during the Musgrave orogeny at temperatures above 1000 degrees Celsius.\", \"D\": \"The Musgrave orogeny is considered an intercontinental orogeny due to the significant upwelling of heat and material from the mantle.\", \"E\": \"The Musgrave orogeny is extensively documented in the journal articles by Kirkland et al. (2013) and Smithies et al. (2011).\" }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4232 tokens (3232 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet:  96%|█████████████████████████████████████████████████  | 680/706 [1:05:53<02:14,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4238 tokens (3238 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"According to the magnetospheric eternally collapsing object (MECO) model proposed by Abhas Mitra, what is a key difference between MECOs and black holes?\", \"answer\": \"C\", \"A\": \"MECOs form event horizons while black holes do not.\", \"B\": \"MECOs collapse faster than black holes due to their intrinsic magnetic field.\", \"C\": \"MECOs can produce their own magnetic field while black holes cannot.\", \"D\": \"MECOs have a finite size while black holes are infinitely small.\", \"E\": \"MECOs do not carry angular momentum or rotate unlike black holes.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4238 tokens (3238 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/m.parquet: 100%|███████████████████████████████████████████████████| 706/706 [1:08:05<00:00,  5.79s/it]\n",
      "../data/wikipedia_fixed/n.parquet:  22%|████████████                                          | 72/324 [05:56<17:54,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4199 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Who jointly published the Nuclear Fusion journal since 2002?\", \"answer\": \"B\", \"A\": \"IAEA\", \"B\": \"IAEA and IOP Publishing\", \"C\": \"Max Planck Institute for Plasma Physics\", \"D\": \"Columbia University\", \"E\": \"Massachusetts Institute of Technology\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4199 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/n.parquet:  30%|████████████████                                      | 96/324 [07:44<14:56,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5551 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the shape of the G1 derivative of a cyclic voltammogram according to the provided text?\", \"answer\": \"C\", \"A\": \"Bell curve\", \"B\": \"Sigmoid\", \"C\": \"Mirrored bell curve\", \"D\": \"Gauss-sigmoid convoluted\", \"E\": \"Linear\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5551 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/n.parquet:  50%|██████████████████████████▌                          | 162/324 [13:39<14:32,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4462 tokens (3462 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the Norwegian continental shelf?\", \"answer\": \"C\", \"A\": \"The Norwegian continental shelf is an area of land that Norway exercises sovereign rights over.\", \"B\": \"The Norwegian continental shelf is an area of land that is four times the size of Norway mainland.\", \"C\": \"The Norwegian continental shelf is the continental shelf over which Norway exercises sovereign rights as defined by the United Nations Convention on the Law of the Sea.\", \"D\": \"The Norwegian continental shelf is a region in Europe that constitutes about one-third of the Europe continental shelf.\", \"E\": \"The Norwegian continental shelf is an area rich in petroleum and gas, serving as the base of Norway's petroleum economy.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4462 tokens (3462 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/n.parquet:  66%|███████████████████████████████████▏                 | 215/324 [18:16<12:28,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5320 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"During the prehistoric period, a proglacial lake called Nipissing Great Lakes formed as a result of the Labradorian Glacier's retreat. Which of the following statements accurately describes the formation and characteristics of this lake?\", \"answer\": \"C\", \"A\": \"The Nipissing Great Lakes formed from the aggregation of Glacial Lakes Houghton, Chippewa, and Hough, and Stanley.\", \"B\": \"The Nipissing Great Lakes were a single body of water encompassing Lake Michigan, Lake Huron, and Lake Superior.\", \"C\": \"The Nipissing Great Lakes were connected to Lake Huron by the Mackinac strait, with the water level being 50 feet higher.\", \"D\": \"The Nipissing Great Lakes were formed when the glacier receded completely from the Great Lakes Basin.\", \"E\": \"The Nipissing Great Lakes had a temporary two-outlet stage with one outlet at Port Huron and another at North Bay, Ontario.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5320 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/n.parquet:  85%|████████████████████████████████████████████▊        | 274/324 [23:26<03:21,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4975 tokens (3975 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"In which year did Nicole Kiil-Nielsen win her first elected office in the municipal elections in Rennes?\", \"answer\": \"A\", \"A\": \"2001\", \"B\": \"2008\", \"C\": \"2009\", \"D\": \"2014\", \"E\": \"1960s\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4975 tokens (3975 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/n.parquet:  95%|██████████████████████████████████████████████████▌  | 309/324 [26:37<01:20,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the name of the scientific journal that covers all theoretical and experimental aspects of nuclear physics and technology, including synchrotron radiation applications, beam line technology, accelerator, ray technology and applications, nuclear chemistry, radiochemistry, and radiopharmaceuticals and nuclear medicine, nuclear electronics and instrumentation, and nuclear energy science and engineering?\", \"answer\": \"C\", \"A\": \"Nuclear Science and Techniques\", \"B\": \"Science Press and Springer\", \"C\": \"Nuclear Science and Techniques\", \"D\": \"Yu-Gang Ma\", \"E\": \"Science Citation Index Expanded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/n.parquet: 100%|████████████████████████████████████████████████████▊| 323/324 [27:49<00:04,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 7460 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which complexity class is NE contained by?\", \"answer\": \"B\", \"A\": \"NEXPTIME\", \"B\": \"NEXPTIME\", \"C\": \"NP\", \"D\": \"P\", \"E\": \"EXPTIME\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 7460 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/n.parquet: 100%|█████████████████████████████████████████████████████| 324/324 [27:59<00:00,  5.18s/it]\n",
      "../data/wikipedia_fixed/o.parquet:   2%|▉                                                      | 3/186 [00:23<21:47,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which of the following statements accurately describes oxidative addition in organometallic chemistry?\", \"answer\": \"C\", \"A\": \"Oxidative addition is a process that decreases the oxidation state and coordination number of a metal centre.\", \"B\": \"Oxidative addition is favored for metals that are acidic and/or difficult to oxidize.\", \"C\": \"Oxidative addition increases both the oxidation state and coordination number of a metal centre.\", \"D\": \"Oxidative addition is most commonly seen with C–H, C–X, and X–X bonds.\", \"E\": \"Oxidative addition requires that the metal complex have a filled coordination site.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/o.parquet:  15%|████████▏                                             | 28/186 [03:05<12:36,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4135 tokens (3135 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes obligate parasites?\", \"answer\": \"D\", \"A\": \"Obligate parasites can complete their life-cycle without a suitable host.\", \"B\": \"Facultative parasites rely on their host to continue their life-cycle.\", \"C\": \"Obligate parasites do not preserve the health of their host.\", \"D\": \"Obligate parasites cannot reproduce without exploiting a suitable host.\", \"E\": \"Obligate parasites have a general invasion strategy.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4135 tokens (3135 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/o.parquet:  16%|████████▍                                             | 29/186 [03:15<17:04,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4101 tokens (3101 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes obligate parasites?\", \"answer\": \"D\", \"A\": \"Obligate parasites can complete their life-cycle without a suitable host.\", \"B\": \"Facultative parasites rely on their host to continue their life-cycle.\", \"C\": \"Obligate parasites do not preserve the health of their host.\", \"D\": \"Obligate parasites cannot reproduce without exploiting a suitable host.\", \"E\": \"Obligate parasites have a general invasion strategy.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4101 tokens (3101 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/o.parquet:  77%|█████████████████████████████████████████            | 144/186 [13:01<03:55,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6957 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the difference between osmolarity and molarity?\", \"answer\": \"C\", \"A\": \"Osmolarity measures the number of moles of solute per unit volume of solution, while molarity measures the number of osmoles of solute particles per unit volume of solution.\", \"B\": \"Osmolarity measures the number of moles of solute particles per unit volume of solution, while molarity measures the number of osmoles of solute per unit volume of solution.\", \"C\": \"Osmolarity measures the number of osmoles of solute particles per unit volume of solution, while molarity measures the number of moles of solute per unit volume of solution.\", \"D\": \"Osmolarity and molarity are synonymous terms that both measure the number of osmoles of solute particles per unit volume of solution.\", \"E\": \"Osmolarity and molarity are synonymous terms that both measure the number of moles of solute per unit volume of solution.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6957 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/o.parquet:  80%|██████████████████████████████████████████▏          | 148/186 [13:23<03:09,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5026 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is Olga Nikolaevna Bondareva best known for?\", \"answer\": \"C\", \"A\": \"Contributing to the fields of mathematical economics and game theory\", \"B\": \"Being a distinguished Soviet mathematician and economist\", \"C\": \"Being one of the two independent discoverers of the Bondareva–Shapley theorem\", \"D\": \"Publishing more than 70 scientific papers on Game Theory and Mathematics\", \"E\": \"Being a member of the editorial board of the international journal Games and Economic Behavior\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5026 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/o.parquet:  83%|███████████████████████████████████████████▉         | 154/186 [13:57<02:36,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4375 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is OpenScientist?\", \"answer\": \"C\", \"A\": \"OpenScientist is a scientific visualization and data analysis tool.\", \"B\": \"OpenScientist is an integration of open source products.\", \"C\": \"OpenScientist is an integration of open source products working together to do scientific visualization and data analysis, particularly for high energy physics.\", \"D\": \"OpenScientist is a light C++ AIDA implementation.\", \"E\": \"OpenScientist can be used to run the histogramming part of Geant4 examples.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4375 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/o.parquet: 100%|█████████████████████████████████████████████████████| 186/186 [17:00<00:00,  5.49s/it]\n",
      "../data/wikipedia_fixed/other.parquet:  32%|████████████████▍                                  | 27/84 [02:23<05:13,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4118 tokens (3118 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes 2,4,6-Tri-tert-butylpyrimidine?\", \"answer\": \"D\", \"A\": \"2,4,6-Tri-tert-butylpyrimidine is a heterocycle pyridine derivative.\", \"B\": \"2,4,6-Tri-tert-butylpyrimidine is more expensive than 2,6-di-tert-butylpyridine.\", \"C\": \"2,4,6-Tri-tert-butylpyrimidine is unable to bind protons.\", \"D\": \"2,4,6-Tri-tert-butylpyrimidine is a bulky base that does not bind boron trifluoride.\", \"E\": \"2,4,6-Tri-tert-butylpyrimidine is a substituted derivative of pyridine.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4118 tokens (3118 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/other.parquet: 100%|███████████████████████████████████████████████████| 84/84 [07:35<00:00,  5.42s/it]\n",
      "../data/wikipedia_fixed/p.parquet:   2%|█▎                                                    | 15/630 [01:18<55:54,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4994 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"During which epoch and era was the Pigeon Point Formation formed?\", \"answer\": \"C\", \"A\": \"Jurassic epoch, Mesozoic Era\", \"B\": \"Paleogene epoch, Cenozoic Era\", \"C\": \"Late Cretaceous epoch, Mesozoic Era\", \"D\": \"Paleozoic era, Permian epoch\", \"E\": \"Neogene epoch, Cenozoic Era\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4994 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet:   3%|█▉                                                    | 22/630 [01:54<42:32,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4657 tokens (3657 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which Mexican restaurant was named the best in Mexico City by the Wall Street Journal and has been consistently ranked among the World's 50 Best Restaurants?\", \"answer\": \"D\", \"A\": \"Pujol\", \"B\": \"Maido\", \"C\": \"Central\", \"D\": \"Pujol\", \"E\": \"Osteria Francescana\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4657 tokens (3657 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/p.parquet:  13%|██████▉                                               | 81/630 [07:21<48:37,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6522 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is phenotypic heterogeneity and how can it manifest in genetic mutations?\", \"answer\": \"C\", \"A\": \"Phenotypic heterogeneity refers to the presence of different mutations in the same gene, which can result in various phenotypes.\", \"B\": \"Phenotypic heterogeneity describes the occurrence of multiple mutations in different genes, leading to the same phenotype.\", \"C\": \"Phenotypic heterogeneity describes different mutations in the same gene that can sometimes give rise to strikingly different phenotypes.\", \"D\": \"Phenotypic heterogeneity is a term used to describe the variability in gene expression within a population.\", \"E\": \"Phenotypic heterogeneity refers to the presence of different genes in the same individual, resulting in diverse phenotypes.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6522 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet:  15%|████████▏                                             | 95/630 [08:35<48:15,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"According to ancient Greek philosophy, which philosopher proposed that the first principle of all things is water?\", \"answer\": \"A\", \"A\": \"Thales of Miletus claimed that the first principle of all things is water.\", \"B\": \"Anaximander claimed that none of the elements (earth, fire, air, water) could be arche for the same reason.\", \"C\": \"Anaximenes suggests that all is made from air through either rarefication or condensation (thinning or thickening).\", \"D\": \"Pythagoras of Samos taught that number, rather than matter, constitutes the true nature of things.\", \"E\": \"Heraclitus held that all is flux.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/p.parquet:  39%|████████████████████▊                                | 247/630 [25:38<27:47,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4189 tokens (3189 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the definition of psychrophiles?\", \"answer\": \"A\", \"A\": \"Psychrophiles are extremophilic organisms that are capable of growth and reproduction in low temperatures, ranging from −20 °C (−4 °F) to 20 °C (68 °F), and are found in places that are permanently cold, such as the polar regions and the deep sea.\", \"B\": \"Psychrophiles are organisms that thrive at unusually high temperatures.\", \"C\": \"Psychrophiles are organisms that can tolerate extreme cold and high salinity content.\", \"D\": \"Psychrophiles are organisms that can grow at temperatures below 7 °C (44.6 °F), but have better growth rates at higher temperatures.\", \"E\": \"Psychrophiles are organisms that can survive in snow, ice, and very cold seawater.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4189 tokens (3189 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/p.parquet:  41%|█████████████████████▉                               | 261/630 [26:39<22:11,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 7048 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the function of the piston in dinoflagellates of the Erythropsidinium and Greuetodinium genera?\", \"answer\": \"C\", \"A\": \"Locomotion\", \"B\": \"Prey capture\", \"C\": \"Unknown\", \"D\": \"Defense\", \"E\": \"All of the above\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 7048 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet:  50%|██████████████████████████▏                          | 312/630 [30:39<25:45,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4250 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the definition of the term 'phreatic'?\", \"answer\": \"C\", \"A\": \"A term used in hydrology to refer to matters relating to ground water below the water table\", \"B\": \"A term used in speleology to refer to cave passages formed by dissolving limestone in all directions\", \"C\": \"A term used in hydrology to refer to aquifers and in volcanology to refer to a type of volcanic eruption\", \"D\": \"A term used in hydrology to refer to the counterpart of the vadose zone, or unsaturated zone, above the phreatic surface\", \"E\": \"A term used in hydrology to refer to the location where the pore water pressure is under atmospheric conditions\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4250 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet:  53%|████████████████████████████▎                        | 337/630 [32:40<23:56,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4546 tokens (3546 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What are some of the features and applications of Project Chrono?\", \"answer\": \"C\", \"A\": \"It is a physics engine developed by the University of Wisconsin-Madison and University of Parma.\", \"B\": \"It supports simulating rigid and soft body dynamics, collision detection, vehicle dynamics, fluid-solid interaction, deformable terrain, and granular dynamics.\", \"C\": \"It supports sensor simulation, robotics simulation, human-autonomous agent interaction, and autonomous vehicle simulation.\", \"D\": \"It is used at tens of universities, in industry, and federal research labs.\", \"E\": \"It was initially developed for use as a multibody simulation tool for robotics and biomechanics applications.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4546 tokens (3546 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/p.parquet:  54%|████████████████████████████▍                        | 338/630 [32:50<31:40,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4123 tokens (3123 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What are some of the features and applications of Project Chrono?\", \"answer\": \"C\", \"A\": \"It is a physics engine developed by the University of Wisconsin-Madison and University of Parma.\", \"B\": \"It supports simulating rigid and soft body dynamics, collision detection, vehicle dynamics, fluid-solid interaction, deformable terrain, and granular dynamics.\", \"C\": \"It supports sensor simulation, robotics simulation, human-autonomous agent interaction, and autonomous vehicle simulation.\", \"D\": \"It is used at tens of universities, in industry, and federal research labs.\", \"E\": \"It was initially developed for use as a multibody simulation tool for robotics and biomechanics applications.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4123 tokens (3123 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/p.parquet:  57%|██████████████████████████████▎                      | 361/630 [34:55<20:07,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5757 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes Phoebe Regio on Venus?\", \"answer\": \"C\", \"A\": \"Phoebe Regio is located to the northwest of Asteria Regio.\", \"B\": \"Phoebe Regio is the largest regio on the planet Venus.\", \"C\": \"Phoebe Regio is 2,852 kilometres in diameter and is the principal feature of the V41 quadrangle.\", \"D\": \"Phoebe Regio is a region on the planet Venus that has never been explored by any spacecraft.\", \"E\": \"Phoebe Regio is a volcanic region on Venus known for its active lava flows.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5757 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet:  69%|████████████████████████████████████▎                | 432/630 [40:38<14:14,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4654 tokens (3654 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the concept of pair potentials in physics?\", \"answer\": \"B\", \"A\": \"Pair potentials are only used to describe the interaction between simple spherical objects.\", \"B\": \"Pair potentials describe the potential energy of two interacting objects solely as a function of the distance between them.\", \"C\": \"Pair potentials are only used in computational chemistry and biology, not in physics.\", \"D\": \"Pair potentials are always short-range potentials that go quickly to zero for infinite distance.\", \"E\": \"Pair potentials are not commonly used in physics and computational chemistry.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4654 tokens (3654 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/p.parquet:  70%|█████████████████████████████████████                | 440/630 [41:20<16:51,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 477 (char 476)\n",
      "{\"prompt\": \"Which of the following statements accurately describes P-chiral phosphines?\", \"answer\": \"C\", \"A\": \"P-chiral phosphines are a subset of chiral phosphines where the stereogenic center can reside at sites other than phosphorus.\", \"B\": \"P-chiral phosphines are organophosphorus compounds of the formula PRR′R″, where R, R′, R″ = H, alkyl, aryl, etc.\", \"C\": \"P-chiral phosphines exploit the high barrier for inversion of phosphines, ensuring that enantiomers of PRR'R\" do not racemize readily.\", \"D\": \"P-chiral phosphines are primarily used as ligands for asymmetric homogeneous catalysts.\", \"E\": \"P-chiral phosphines have been investigated for their applications in nucleophilic organocatalysis.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 1 column 477 (char 476)\n",
      "../data/wikipedia_fixed/p.parquet:  74%|███████████████████████████████████████▏             | 466/630 [43:31<12:10,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5707 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What are some benefits of polyculture in agriculture?\", \"answer\": \"Crops in polycultures are less likely to be affected by pests and diseases due to the diversity of plants and the presence of natural enemies.\", \"A\": \"Polyculture reduces the need for pesticides and herbicides in agriculture.\", \"B\": \"Polyculture increases crop yields by improving control of pests, weeds, and diseases.\", \"C\": \"Polyculture simplifies management as all species have similar growth rates and harvest requirements.\", \"D\": \"Polyculture is the most prevalent form of agriculture in economically developed countries.\", \"E\": \"Polyculture requires less labor and human intervention compared to monoculture.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5707 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet:  76%|████████████████████████████████████████▍            | 481/630 [44:45<13:08,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4760 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements is true about paleoceanography?\", \"answer\": \"C\", \"A\": \"Paleoceanography is the study of changes in ancient climate.\", \"B\": \"Paleoceanographic research is not related to sedimentology and paleontology.\", \"C\": \"Paleoceanographic studies use proxy methods to infer information about the past state and evolution of the world's oceans.\", \"D\": \"Paleoceanography focuses on the study of physical, chemical, and biological processes in the ocean.\", \"E\": \"Paleoceanography is a new field of study that emerged in the 1930s.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4760 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet:  77%|████████████████████████████████████████▉            | 486/630 [45:13<11:47,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4348 tokens (3348 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which tectonic thrustfront separates the high grade metamorphic rocks of the Penninic nappes from the sedimentary rocks and crystalline basement of the Helvetic nappes?\", \"answer\": \"C\", \"A\": \"The Alpine thrustfront\", \"B\": \"The Himalayan thrustfront\", \"C\": \"The Penninic thrustfront\", \"D\": \"The Appalachian thrustfront\", \"E\": \"The Andean thrustfront\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4348 tokens (3348 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/p.parquet:  83%|███████████████████████████████████████████▋         | 520/630 [47:54<08:20,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4803 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the official journal of the Australasian College of Physical Scientists and Engineers in Medicine and is recognized as an official journal of the International Organization for Medical Physics, Asia-Oceania Federation of Organizations for Medical Physics and the Biomedical College of Engineers Australia?\", \"answer\": \"C\", \"A\": \"Physical and Engineering Sciences in Medicine\", \"B\": \"Australasian Physical Sciences in Medicine\", \"C\": \"Australasian Physical & Engineering Sciences in Medicine\", \"D\": \"Australasian Bulletin of Medical Physics\", \"E\": \"Australasian Newsletter of Medical Physics\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4803 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet:  83%|████████████████████████████████████████████▏        | 525/630 [48:25<09:10,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 1 column 319 (char 318)\n",
      "{\"prompt\": \"What is the Poincaré–Lelong equation?\", \"answer\": \"C\", \"A\": \"The Poincaré–Lelong equation is a linear equation on a Kähler manifold.\", \"B\": \"The Poincaré–Lelong equation is a non-linear equation on a Kähler manifold.\", \"C\": \"The Poincaré–Lelong equation is the partial differential equation i ∂ ∂ ¯ u = ρ {\\displaystyle i\\partial {\\overline {\\partial }}u=\\rho } on a Kähler manifold, where ρ is a positive (1,1)-form.\", \"D\": \"The Poincaré–Lelong equation is a differential equation on a Riemannian manifold.\", \"E\": \"The Poincaré–Lelong equation is a polynomial equation on a Kähler manifold.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 1 column 319 (char 318)\n",
      "../data/wikipedia_fixed/p.parquet:  90%|███████████████████████████████████████████████▍     | 564/630 [52:00<05:52,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5258 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the function of the Pospiviroid RY motif stem loop found in Pospiviroids like potato spindle tuber viroid (PSTVd)?\", \"answer\": \"C\", \"A\": \"To bind with the tomato protein Virp1\", \"B\": \"To establish a viroid infection\", \"C\": \"The exact function is unknown\", \"D\": \"To interact with the RNA motif responsible for the specific interaction of PSTVd and Virp1\", \"E\": \"To characterize the RNA motif responsible for the specific interaction of PSTVd and Virp1\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5258 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet:  96%|██████████████████████████████████████████████████▊  | 604/630 [55:17<01:45,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4637 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the term used to describe a polymorphism in larval development in marine invertebrates?\", \"answer\": \"A\", \"A\": \"Poecilogony\", \"B\": \"Polychaete\", \"C\": \"Planktotrophic\", \"D\": \"Lecithotrophic\", \"E\": \"Sacoglossan\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4637 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet:  98%|████████████████████████████████████████████████████ | 619/630 [56:25<00:42,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 130 (char 129)\n",
      "{\"prompt\": \"Which term is used to describe a substance that can be converted to a vitamin within the body?\", \"answer\": \"A\", \"A\": \"Provita...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 130 (char 129)\n",
      "../data/wikipedia_fixed/p.parquet:  99%|████████████████████████████████████████████████████▍| 623/630 [56:49<00:32,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which former school building is the Penrith and Eden Museum based in?\", \"answer\": \"C\", \"A\": \"Robinson's School\", \"B\": \"Kirkby Stephen School\", \"C\": \"Newby School\", \"D\": \"Waitby School\", \"E\": \"Crosby Ravensworth School\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/p.parquet:  99%|████████████████████████████████████████████████████▍| 624/630 [56:59<00:38,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5886 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which former school building is the Penrith and Eden Museum based in?\", \"answer\": \"C\", \"A\": \"Robinson's School\", \"B\": \"Kirkby Stephen School\", \"C\": \"Newby School\", \"D\": \"Waitby School\", \"E\": \"Crosby Ravensworth School\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5886 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/p.parquet: 100%|█████████████████████████████████████████████████████| 630/630 [57:32<00:00,  5.48s/it]\n",
      "../data/wikipedia_fixed/q.parquet:  30%|████████████████▋                                      | 17/56 [01:28<03:11,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4925 tokens (3925 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is a quasi-median network?\", \"answer\": \"C\", \"A\": \"A phylogenetic network that represents multistate characters.\", \"B\": \"A split network that represents multistate characters.\", \"C\": \"A phylogenetic network where any two nodes are joined by an edge if and only if the sequences associated with the nodes differ in exactly one position.\", \"D\": \"A condensed version of a multiple sequence alignment of DNA sequences on X.\", \"E\": \"A set of all sequences that can be obtained by repeatedly taking the quasi-median of any three sequences in the set and then adding the result to the set.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4925 tokens (3925 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/q.parquet:  62%|██████████████████████████████████▍                    | 35/56 [03:02<01:39,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6277 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"According to the text, what is one of the main goals of queer ecology?\", \"answer\": \"C\", \"A\": \"To establish the superiority of heterosexuality and cisgenderedness\", \"B\": \"To reinforce the idea of human exceptionalism and anthropocentrism\", \"C\": \"To challenge traditional ideas about the value of organisms, individuals, memories, species, visions, and objects\", \"D\": \"To perpetuate heteronormative ideas about nature and society\", \"E\": \"To promote the associations between 'natural' and 'heterosexual' or 'heteronormative'\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6277 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/q.parquet:  68%|█████████████████████████████████████▎                 | 38/56 [03:22<01:38,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is a Q-star?\", \"answer\": \"C\", \"A\": \"A compact, heavy neutron star with a gravitational pull so strong that all light cannot escape.\", \"B\": \"A hypothetical type of a compact, heavy neutron star with a gravitational pull so strong that all light cannot escape.\", \"C\": \"A hypothetical type of a compact, heavy neutron star with an exotic state of matter and a gravitational pull so strong that some light, but not all light, cannot escape.\", \"D\": \"A compact, heavy neutron star with an exotic state of matter and a gravitational pull so strong that all light cannot escape.\", \"E\": \"A hypothetical type of a compact, heavy neutron star with an exotic state of matter and a gravitational pull so strong that all light cannot escape.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/q.parquet: 100%|███████████████████████████████████████████████████████| 56/56 [10:01<00:00, 10.74s/it]\n",
      "../data/wikipedia_fixed/r.parquet:   8%|████▌                                                 | 34/402 [02:36<23:51,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5215 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is one characteristic of tropical rainforests?\", \"answer\": \"C\", \"A\": \"Tropical rainforests have a dry season.\", \"B\": \"Tropical rainforests have a low humidity.\", \"C\": \"Tropical rainforests have a warm and wet climate.\", \"D\": \"Tropical rainforests have a high occurrence of wildfires.\", \"E\": \"Tropical rainforests have a low biodiversity.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5215 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/r.parquet:  17%|█████████▏                                            | 68/402 [05:30<29:37,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5675 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes a remnant natural area?\", \"answer\": \"C\", \"A\": \"Remnant natural areas are created through human intervention and restoration projects.\", \"B\": \"Remnant natural areas are characterized by the absence of native flora and fauna.\", \"C\": \"Remnant natural areas have not been significantly disturbed by destructive activities.\", \"D\": \"Remnant natural areas can only be found in the upper Midwestern United States.\", \"E\": \"Remnant natural areas are defined solely by their biological integrity.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5675 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/r.parquet:  19%|██████████▍                                           | 78/402 [06:19<22:59,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4348 tokens (3348 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is run differential in baseball and how is it calculated?\", \"answer\": \"C\", \"A\": \"Run differential is a team statistic that combines offensive and defensive scoring in baseball.\", \"B\": \"Run differential is calculated by subtracting runs scored from runs allowed in baseball.\", \"C\": \"Run differential is calculated by subtracting runs allowed from runs scored in baseball.\", \"D\": \"Run differential is positive when a team allows more runs than it scores in baseball.\", \"E\": \"Run differential is negative when a team scores more runs than it allows in baseball.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4348 tokens (3348 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/r.parquet:  37%|███████████████████▊                                 | 150/402 [11:45<18:58,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4853 tokens (3853 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is raking?\", \"answer\": \"C\", \"A\": \"Raking is a statistical process used to adjust data sample weights of a contingency table to match desired marginal totals.\", \"B\": \"Raking is a method used to estimate the ratio of rakes in a given area.\", \"C\": \"Raking, also known as raking ratio estimation or iterative proportional fitting, is the statistical process of adjusting data sample weights of a contingency table to match desired marginal totals.\", \"D\": \"Raking is a technique used to calculate the average weight of rakes in a given population.\", \"E\": \"Raking is a term used to describe the process of collecting leaves using a rake.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4853 tokens (3853 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/r.parquet:  38%|████████████████████▏                                | 153/402 [12:02<19:14,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4216 tokens (3216 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which university did Robert Ghormley Parr receive his Ph.D. from in 1947?\", \"answer\": \"B\", \"A\": \"University of North Carolina at Chapel Hill\", \"B\": \"University of Minnesota\", \"C\": \"Carnegie Institute of Technology\", \"D\": \"Johns Hopkins University\", \"E\": \"Brown University\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4216 tokens (3216 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/r.parquet:  49%|█████████████████████████▋                           | 195/402 [15:09<15:12,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4163 tokens (3163 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What are the three main reactive oxygen species?\", \"answer\": \"C\", \"A\": \"Superoxide anion, hydrogen peroxide, and hydroxyl ion\", \"B\": \"Superoxide radical, hydrogen ion, and hydroxyl radical\", \"C\": \"Superoxide anion, hydrogen peroxide, and hydroxyl radical\", \"D\": \"Superoxide radical, hydrogen peroxide, and hydroxyl ion\", \"E\": \"Superoxide anion, hydrogen ion, and hydroxyl ion\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4163 tokens (3163 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/r.parquet:  68%|███████████████████████████████████▊                 | 272/402 [20:55<08:28,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which geologic formation in West Virginia preserves fossils dating back to the Carboniferous period?\", \"answer\": \"C\", \"A\": \"The Reynolds Limestone is a geologic formation in West Virginia.\", \"B\": \"The Reynolds Limestone is a Paleozoic portal in West Virginia.\", \"C\": \"The Reynolds Limestone preserves fossils dating back to the Carboniferous period.\", \"D\": \"The Reynolds Limestone is listed in the Paleobiology Database.\", \"E\": \"The Reynolds Limestone is a fossiliferous stratigraphic unit in West Virginia.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/r.parquet:  84%|████████████████████████████████████████████▌        | 338/402 [26:07<05:51,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 8183 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes Richard Keith Ellis?\", \"answer\": \"C\", \"A\": \"Richard Keith Ellis is a British theoretical physicist who graduated from the University of Oxford and has held positions at various institutions including Imperial College, MIT, Caltech, CERN, and the University of Rome.\", \"B\": \"Richard Keith Ellis is a leading authority on perturbative quantum chromodynamics and collider phenomenology, and has made significant contributions to the interpretation of experiments performed at high energy.\", \"C\": \"Richard Keith Ellis performed the first calculation of jet structure in e+e- annihilation, allowing for the precise determination of the strong coupling, and also co-authored a book on QCD and collider physics.\", \"D\": \"Richard Keith Ellis was elected a Fellow of the American Physical Society in 1988 and a Fellow of the Royal Society of London in 2009, and has received several prestigious awards for his work in theoretical particle physics.\", \"E\": \"Richard Keith Ellis is currently a professor of Physics and Director of the Institute for Particle Physics Phenomenology at the University of Durham in the UK.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 8183 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/r.parquet:  89%|██████████████████████████████████████████████▉      | 356/402 [27:27<02:25,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 5088 tokens (4088 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which geologic formation in Texas preserves fossils dating back to the Paleogene period?\", \"answer\": \"A\", \"A\": \"The Reklaw Formation\", \"B\": \"The Paleogene Formation\", \"C\": \"The Texas Fossil Formation\", \"D\": \"The Gateway Formation\", \"E\": \"The Stratigraphic Unit Formation\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 5088 tokens (4088 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/r.parquet: 100%|█████████████████████████████████████████████████████| 402/402 [30:45<00:00,  4.59s/it]\n",
      "../data/wikipedia_fixed/s.parquet:   1%|▌                                                      | 9/841 [00:36<55:08,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4440 tokens (3440 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the primary focus of soil chemistry in the late 1960s?\", \"answer\": \"C\", \"A\": \"Chemical reactions in the soil that affect plant growth\", \"B\": \"Environmental pollution and contamination\", \"C\": \"Environmental soil science\", \"D\": \"Pedology and agricultural soil science\", \"E\": \"Soil structure and formation\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4440 tokens (3440 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:   5%|██▌                                                   | 39/841 [02:43<58:19,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4849 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which mountain range in southern Albania is protected by the Fir of Hotova National Park and has the highest peak called Maja e Papingut?\", \"answer\": \"C\", \"A\": \"Trebeshinë-Dhëmbel-Nemërçkë\", \"B\": \"Shëndelli-Lunxhëri-Bureto\", \"C\": \"Nemërçkë\", \"D\": \"Tomorr\", \"E\": \"Çika\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4849 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/s.parquet:  10%|█████▋                                                | 88/841 [06:40<55:22,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which compound is referred to as selenium sulfide?\", \"answer\": \"A\", \"A\": \"Selenium disulfide, SeS2\", \"B\": \"Selenium hexasulfide, Se2S6\", \"C\": \"Selenium monosulfide, SeS\", \"D\": \"Selenium tetrasulfide, Se2S4\", \"E\": \"Selenium pentasulfide, Se2S5\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/s.parquet:  28%|██████████████▋                                      | 234/841 [18:50<47:00,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4816 tokens (3816 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which organization collaborated with the University of Toulouse in France to develop the SuperCam suite of instruments for the Mars 2020 Perseverance rover mission?\", \"answer\": \"C\", \"A\": \"Research Institute in Astrophysics and Planetology (IRAP)\", \"B\": \"French Space Agency (CNES)\", \"C\": \"Los Alamos National Laboratory\", \"D\": \"University of Hawaii\", \"E\": \"University of Málaga\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4816 tokens (3816 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  29%|███████████████                                      | 240/841 [19:19<41:48,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4987 tokens (3987 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the primary function of sponge reefs?\", \"answer\": \"C\", \"A\": \"To provide structure on the continental shelf\", \"B\": \"To serve as a habitat for fish and invertebrates\", \"C\": \"To act as breeding and nursery areas for fish and invertebrates\", \"D\": \"To capture fine sediments carried by deep ocean currents\", \"E\": \"To compete with coral and rudist reefs\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4987 tokens (3987 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  30%|███████████████▊                                     | 251/841 [20:23<53:12,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4156 tokens (3156 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Simplified Perturbations Models (SPM)?\", \"answer\": \"C\", \"A\": \"SGP4 and SDP4 are the only models included in the SPM set.\", \"B\": \"The SPM models were first published in 1988 with no further refinements.\", \"C\": \"SGP models are used for near earth objects with an orbital period of less than 225 minutes.\", \"D\": \"The SPM models do not take into account the effects of drag and radiation.\", \"E\": \"The SPM models have a maximum error of 1 km at epoch.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4156 tokens (3156 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  35%|██████████████████▎                                  | 291/841 [24:09<46:41,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4496 tokens (3496 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the function of Small nucleolar RNA Me28S-Gm1083?\", \"answer\": \"B\", \"A\": \"It is a coding RNA molecule involved in protein synthesis.\", \"B\": \"It functions in the modification of other small nuclear RNAs.\", \"C\": \"It is a guide RNA that directs the splicing of pre-mRNA.\", \"D\": \"It is involved in the replication of DNA during cell division.\", \"E\": \"It is a regulatory RNA molecule that controls gene expression.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4496 tokens (3496 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  38%|███████████████████▉                                 | 316/841 [26:06<35:16,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 1 column 562 (char 561)\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Strichartz estimates?\", \"answer\": \"C\", \"A\": \"The Strichartz estimates establish the size and decay of solutions in mixed norm Lebesgue spaces.\", \"B\": \"The Strichartz estimates are a family of inequalities for linear dispersive partial differential equations.\", \"C\": \"The Strichartz estimates are first noted by Robert Strichartz and arose out of connections to the Fourier restriction problem.\", \"D\": \"The Strichartz estimates are applicable only to the linear Schrödinger equation in R d {\\displaystyle \\mathbb {R} ^{d}} with h = m = 1.\", \"E\": \"The Strichartz estimates are independent of the values of q, r, and d.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 1 column 562 (char 561)\n",
      "../data/wikipedia_fixed/s.parquet:  42%|██████████████████████                               | 350/841 [28:34<30:26,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4100 tokens (3100 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What was the purpose of Anton Dohrn in establishing the Stazione Zoologica Anton Dohrn?\", \"answer\": \"C\", \"A\": \"To create a research institute for basic biology\", \"B\": \"To establish an international scientific community\", \"C\": \"To provide laboratory space, equipment, research material, and a library\", \"D\": \"To generate income from the bench system and the sale of scientific journals and specimens\", \"E\": \"To supervise and control the research institute\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4100 tokens (3100 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  42%|██████████████████████▏                              | 353/841 [28:56<47:43,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5021 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the meaning of the name Sidera Lodoicea?\", \"answer\": \"C\", \"A\": \"The name Sidera Lodoicea refers to the four moons of Saturn discovered by Giovanni Domenico Cassini.\", \"B\": \"The name Sidera Lodoicea is derived from the Latin words for 'star' and 'Louisian Stars'.\", \"C\": \"The name Sidera Lodoicea means 'Louisian Stars' and was chosen by Cassini to honor King Louis XIV of France.\", \"D\": \"The name Sidera Lodoicea was inspired by the Latin name used by Galileo to name the four Galilean satellites of Jupiter.\", \"E\": \"The name Sidera Lodoicea was given to the four moons of Saturn published by Cassini in his Découverte de deux nouvelles planètes autour de Saturne.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5021 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/s.parquet:  45%|███████████████████████▊                             | 377/841 [31:05<39:20,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4110 tokens (3110 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements is true about Smith spaces?\", \"answer\": \"B\", \"A\": \"Smith spaces are named after Marianne Ruth Freundlich Smith.\", \"B\": \"All Smith spaces are stereotype and are in the stereotype duality relations with Banach spaces.\", \"C\": \"The polar K = B ∘ of the unit ball B in X is the universal compact set in X⋆.\", \"D\": \"For any Banach space X, its stereotype dual space X⋆ is a Smith space.\", \"E\": \"If K is a convex balanced compact set in a locally convex space Y, then its linear span CK possesses a unique structure of a Smith space with K as the universal compact set.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4110 tokens (3110 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  46%|████████████████████████▍                            | 388/841 [31:58<29:21,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4425 tokens (3425 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the purpose of the Stawell Underground Physics Laboratory (SUPL)?\", \"answer\": \"C\", \"A\": \"To conduct research on neutrinos\", \"B\": \"To study the constellation Cygnus\", \"C\": \"To conduct research on dark matter\", \"D\": \"To study the effects of seasonal environmental changes\", \"E\": \"To investigate the stopping rate of self-interacting dark matter\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4425 tokens (3425 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  48%|█████████████████████████▎                           | 402/841 [33:11<34:40,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 1 column 306 (char 305)\n",
      "{\"prompt\": \"Which of the following statements accurately describes the Scorer's functions?\", \"answer\": \"C\", \"A\": \"The Scorer's functions are special functions studied by Scorer (1950) and denoted Gi(x) and Hi(x).\", \"B\": \"The Scorer's functions are solutions to the equation y ″ ( x ) − x   y ( x ) = 1 π {\\displaystyle y''(x)-x\\ y(x)={\\frac {1}{\\pi }}}.\", \"C\": \"The Scorer's functions can be defined in terms of Airy functions.\", \"D\": \"The Scorer's functions are represented by complex plots in the complex plane.\", \"E\": \"The Scorer's functions were tabulated by Scorer (1950) for numerical evaluation of integrals.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 1 column 306 (char 305)\n",
      "../data/wikipedia_fixed/s.parquet:  53%|████████████████████████████▏                        | 447/841 [36:51<33:13,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4121 tokens (3121 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes sclerobionts?\", \"answer\": \"C\", \"A\": \"Sclerobionts are organisms that live exclusively on soft substrates.\", \"B\": \"Sclerobionts are organisms that live in or on any kind of soft substrate.\", \"C\": \"Sclerobionts are organisms that live in or on any kind of hard substrate.\", \"D\": \"Sclerobionts are organisms that live exclusively on hard substrates.\", \"E\": \"Sclerobionts are organisms that live in or on any kind of substrate, regardless of its hardness.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4121 tokens (3121 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  57%|██████████████████████████████▍                      | 483/841 [39:32<25:26,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 1 column 548 (char 547)\n",
      "{\"prompt\": \"What is the Susskind-Glogower operator and how is it defined?\", \"answer\": \"B\", \"A\": \"The Susskind-Glogower operator is a phase operator that displaces the number operator in the same fashion as the momentum operator acts as the generator of translations in quantum mechanics.\", \"B\": \"The Susskind-Glogower operator, first proposed by Leonard Susskind and J. Glogower, refers to the operator where the phase is introduced as an approximate polar decomposition of the creation and annihilation operators. It is defined as V = 1 a a † a {\\displaystyle V={\\frac {1}{\\sqrt {aa^{\\dagger }}}}a} , and its adjoint V † = a † 1 a a † {\\displaystyle V^{\\dagger }=a^{\\dagger }{\\frac {1}{\\sqrt {aa^{\\dagger }}}}} .\", \"C\": \"The Susskind-Glogower operator is a solution to the Landau-Zener problem via Susskind-Glogower operators.\", \"D\": \"The Susskind-Glogower operator is an exact solution of generalized Dicke models via Susskind-Glogower operators.\", \"E\": \"The Susskind-Glogower operator is a nonlinear coherent state for the Susskind-Glogower operators.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid \\escape: line 1 column 548 (char 547)\n",
      "../data/wikipedia_fixed/s.parquet:  59%|███████████████████████████████▏                     | 494/841 [40:45<34:02,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4673 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the Southeast Bathurst Fault Zone and how did it form?\", \"answer\": \"C\", \"A\": \"The Southeast Bathurst Fault Zone is a north–south trending system of extensional faults forming a graben-like structure on southeastern Bathurst Island in the Canadian Arctic Archipelago.\", \"B\": \"The Southeast Bathurst Fault Zone is a south–north trending system of compressional faults forming a rift-like structure on southeastern Bathurst Island in the Canadian Arctic Archipelago.\", \"C\": \"The Southeast Bathurst Fault Zone is a north–south trending system of extensional faults forming a graben-like structure on southeastern Bathurst Island in the Canadian Arctic Archipelago. It formed during the latest Cretaceous to early Tertiary when extension of the Boreal Rifting Episode penetrated the North American continent from the Canada Basin.\", \"D\": \"The Southeast Bathurst Fault Zone is a north–south trending system of extensional faults forming a graben-like structure on southeastern Bathurst Island in the Canadian Arctic Archipelago. It formed during the Eocene when the Eurekan Rifting Episode uplifted and compressed the region.\", \"E\": \"The Southeast Bathurst Fault Zone is a north–south trending system of extensional faults forming a graben-like structure on southeastern Bathurst Island in the Canadian Arctic Archipelago. It formed during the latest Cretaceous to early Tertiary when compression of the Boreal Rifting Episode penetrated the North American continent from the Canada Basin.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4673 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/s.parquet:  59%|███████████████████████████████▍                     | 498/841 [41:08<30:35,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4250 tokens (3250 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"When was the Société astronomique de France established and where was its headquarters located in 1887?\", \"answer\": \"B\", \"A\": \"28 January 1887, 16 rue Cassini, 75014 Paris\", \"B\": \"28 January 1887, 28 rue Serpente, 6th arrondissement of Paris\", \"C\": \"4 April 1887, 16 rue Cassini, 75014 Paris\", \"D\": \"4 April 1887, 28 rue Serpente, 6th arrondissement of Paris\", \"E\": \"17 October 1966, 28 rue Saint-Dominique, Paris 75007\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4250 tokens (3250 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  63%|█████████████████████████████████▌                   | 533/841 [44:13<20:22,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"Which research group cloned the snoRNA ACA53 from a HeLa cell extract immunoprecipitated with an anti-GAR1 antibody?\", \"answer\": \"A\", \"A\": \"Kiss et al. (2004)\", \"B\": \"Jády et al. (2004)\", \"C\": \"Bertrand et al. (2004)\", \"D\": \"Kiss et al. (2004) and Jády et al. (2004)\", \"E\": \"Kiss et al. (2004) and Bertrand et al. (2004)\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/s.parquet:  68%|████████████████████████████████████▎                | 576/841 [52:59<17:53,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4805 tokens (3805 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which instrumental separation techniques can be used to select ligands with desired parameters from large combinatorial libraries of biopolymers?\", \"answer\": \"C\", \"A\": \"Kinetic capillary electrophoresis (KCE)\", \"B\": \"Surface plasmon resonance (SPR)\", \"C\": \"Microscale thermophoresis (MST)\", \"D\": \"DNA smart aptamers\", \"E\": \"RNA and peptide smart aptamers\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4805 tokens (3805 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  69%|████████████████████████████████████▌                | 580/841 [53:27<26:15,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4160 tokens (3160 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which statement accurately describes the Schaeffer-Fulton stain technique for isolating endospores?\", \n",
      "\"answer\": \"C\", \n",
      "\"A\": \"The primary stain used in the Schaeffer-Fulton stain technique is safranin.\", \n",
      "\"B\": \"Endospores can be stained by normal staining procedures due to their permeable walls.\", \n",
      "\"C\": \"The Schaeffer-Fulton stain technique uses heat to drive the primary stain into the endospores.\", \n",
      "\"D\": \"The slide is decolorized with malachite green after cooling in the Schaeffer-Fulton stain technique.\", \n",
      "\"E\": \"The Schaeffer-Fulton stain technique was developed by bacteriologists Wirtz and Conklin.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4160 tokens (3160 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  71%|█████████████████████████████████████▋               | 598/841 [55:05<22:03,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"According to the text, which of the following statements accurately describes the viewpoint of structuralists in evolutionary biology?\", \"answer\": \"C\", \"A\": \"Structuralists believe that natural selection is the sole driving force of evolution.\", \"B\": \"Structuralists argue that physical forces supersede natural selection in shaping the development of an animal's body.\", \"C\": \"Structuralists propose that evolution is guided by physical forces that shape an animal's body and sometimes supersede natural selection.\", \"D\": \"Structuralists deny the existence of deep homologies between different groups of organisms.\", \"E\": \"Structuralists believe that biological complexity can be solely attributed to natural selection.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/s.parquet:  72%|███████████████████████████████████▏             | 603/841 [1:00:56<1:53:53, 28.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the purpose of shotgun lipidomics?\", \"answer\": \"C\", \"A\": \"To investigate the biological function, significance, and sequelae of alterations in lipids and protein constituents mediating lipid metabolism\", \"B\": \"To examine the structures, functions, interactions, and dynamics of cellular lipids and the dynamic changes that occur during pathophysiologic perturbations\", \"C\": \"To identify and quantitate the thousands of cellular lipid molecular species and their interactions with other lipids, proteins, and other moieties in vivo\", \"D\": \"To define the biochemical mechanisms of lipid-related disease processes through identifying alterations in cellular lipid metabolism, trafficking and homeostasis\", \"E\": \"To fingerprint and quantitate individual molecular species of most major and many minor lipid classes directly from biological lipid extracts without the need for chromatographic purification\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/s.parquet:  74%|█████████████████████████████████████▊             | 623/841 [1:02:26<16:16,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5257 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"According to structuralism in the philosophy of mathematics, how are mathematical objects defined?\", \"answer\": \"C\", \"A\": \"Mathematical objects are defined by their intrinsic properties.\", \"B\": \"Mathematical objects are defined by their empirical existence.\", \"C\": \"Mathematical objects are defined by their external relations in a system.\", \"D\": \"Mathematical objects are defined by their spatial and temporal characteristics.\", \"E\": \"Mathematical objects are defined by their subjective interpretations.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5257 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/s.parquet:  76%|██████████████████████████████████████▌            | 635/841 [1:03:21<12:46,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What was the focus of Suzanne Amador Kane's thesis?\", \"answer\": \"C\", \"A\": \"Utilizing video to understand bird behavior\", \"B\": \"Studying the structures of multilayer films and biological membranes\", \"C\": \"Utilizing x-ray and light scattering techniques to study biological membranes and low-dimensional soft matter systems\", \"D\": \"Analyzing the biomechanics of peacock courtship rituals\", \"E\": \"Investigating the behavior of raptors while hunting\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/s.parquet:  76%|██████████████████████████████████████▊            | 639/841 [1:04:03<32:58,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5428 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the staggered extension process (StEP) and how is it used in biotechnology and molecular biology?\", \"answer\": \"C\", \"A\": \"The staggered extension process (StEP) is a technique used in biotechnology and molecular biology to create new, mutated genes with qualities of one or more initial genes. It involves the use of a modified polymerase chain reaction with short cycles to quickly elongate DNA and anneal synthesized fragments with complementary fragments of other strands, resulting in shuffled mutations and amplified genes with new combinations of mutations.\", \"B\": \"The staggered extension process (StEP) is a technique used in biotechnology and molecular biology to create new, mutated genes with qualities of one or more initial genes. It involves the use of a modified polymerase chain reaction with long cycles to slowly elongate DNA and anneal synthesized fragments with complementary fragments of other strands, resulting in shuffled mutations and amplified genes with new combinations of mutations.\", \"C\": \"The staggered extension process (StEP) is a technique used in biotechnology and molecular biology to create new, mutated genes with qualities of one or more initial genes. It involves the use of a modified polymerase chain reaction with short cycles to quickly elongate DNA and anneal synthesized fragments with complementary fragments of other strands, resulting in shuffled mutations and amplified genes with new combinations of mutations. The StEP protocol has been found to be useful as a method of directed evolution for the discovery of enzymes useful to industry.\", \"D\": \"The staggered extension process (StEP) is a technique used in biotechnology and molecular biology to create new, mutated genes with qualities of one or more initial genes. It involves the use of a modified polymerase chain reaction with long cycles to slowly elongate DNA and anneal synthesized fragments with complementary fragments of other strands, resulting in shuffled mutations and amplified genes with new combinations of mutations. The StEP protocol has been found to be useful as a method of directed evolution for the discovery of enzymes useful to industry.\", \"E\": \"The staggered extension process (StEP) is a technique used in biotechnology and molecular biology to create new, mutated genes with qualities of one or more initial genes. It involves the use of a modified polymerase chain reaction with short cycles to quickly elongate DNA and anneal synthesized fragments with complementary fragments of other strands, resulting in shuffled mutations and amplified genes with new combinations of mutations. The StEP protocol has been found to be useful as a method of directed evolution for the discovery of enzymes useful to industry. The StEP protocol was first described in a paper published in Nature Biotechnology in 1998.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5428 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/s.parquet:  87%|████████████████████████████████████████████▍      | 732/841 [1:11:27<08:43,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4211 tokens (3211 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the role of neutrinos in a core-collapse supernova?\", \"answer\": \"D\", \"A\": \"Neutrinos are responsible for the formation of a neutron star during a supernova.\", \"B\": \"Neutrinos carry away about 99% of the gravitational energy of a dying star in the form of kinetic energy.\", \"C\": \"Neutrinos interact only through weak interaction and gravity.\", \"D\": \"Neutrinos play a crucial role in the collapse and explosion of a supernova.\", \"E\": \"Neutrinos are the primary source of cosmic neutrinos in the MeV energy range.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4211 tokens (3211 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  94%|███████████████████████████████████████████████▉   | 791/841 [1:16:09<03:50,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the main function of sperm chemotaxis?\", \"answer\": \"D\", \"A\": \"To guide spermatozoa towards the oocyte through a concentration gradient of a chemoattractant\", \"B\": \"To prevent large numbers of spermatozoa from reaching the oocyte\", \"C\": \"To compete with other spermatozoa for fertilization of the oocyte\", \"D\": \"To assist in the guidance and selection of spermatozoa for fertilization\", \"E\": \"To activate motility in spermatozoa before chemotaxis\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/s.parquet:  95%|████████████████████████████████████████████████▍  | 798/841 [1:22:08<11:14, 15.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4260 tokens (3260 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which term is most often used to refer to a DNA sequence?\", \"answer\": \"A\", \"A\": \"Protein sequence\", \"B\": \"DNA sequence\", \"C\": \"Genotype\", \"D\": \"Self-incompatibility in plants\", \"E\": \"List of geneticists\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4260 tokens (3260 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet:  96%|█████████████████████████████████████████████████  | 810/841 [1:23:11<02:43,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4808 tokens (3808 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What are the main factors that contribute to the formation of sea foam?\", \"answer\": \"C\", \"A\": \"The presence of high concentrations of dissolved organic matter from macrophytes and phytoplankton\", \"B\": \"The breaking of algal cells in times of heavy swells\", \"C\": \"Breaking surface waves, bubble entrainment, and whitecap formation\", \"D\": \"Falling rain drops on the sea surface\", \"E\": \"Turbulence in the surface mixed layer\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4808 tokens (3808 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/s.parquet: 100%|███████████████████████████████████████████████████| 841/841 [1:26:17<00:00,  6.16s/it]\n",
      "../data/wikipedia_fixed/t.parquet:   0%|                                                               | 0/477 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4220 tokens (3220 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the definition of strangeness in particle physics?\", \"answer\": \"A\", \"A\": \"Strangeness is a property of particles expressed as a quantum number, defined as the difference between the number of strange quarks and the number of strange antiquarks.\", \"B\": \"Strangeness is a property of particles expressed as a quantum number, defined as the sum of the number of strange quarks and the number of strange antiquarks.\", \"C\": \"Strangeness is a property of particles expressed as a quantum number, defined as the product of the number of strange quarks and the number of strange antiquarks.\", \"D\": \"Strangeness is a property of particles expressed as a quantum number, defined as the ratio of the number of strange quarks to the number of strange antiquarks.\", \"E\": \"Strangeness is a property of particles expressed as a quantum number, defined as the square root of the sum of the squares of the number of strange quarks and the number of strange antiquarks.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4220 tokens (3220 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/t.parquet:   3%|█▊                                                    | 16/477 [01:35<45:00,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4119 tokens (3119 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes tin iodide?\", \"answer\": \"B\", \"A\": \"Tin iodide refers to two different ionic compounds, tin(II) iodide and tin(IV) iodide.\", \"B\": \"Tin iodide may refer to two different ionic compounds, tin(II) iodide or stannous iodide, and tin(IV) iodide or stannic iodide.\", \"C\": \"Tin iodide is a single compound with two different names, tin(II) iodide and tin(IV) iodide.\", \"D\": \"Tin iodide is a compound that can exist in two different forms, tin(II) iodide and tin(IV) iodide.\", \"E\": \"Tin iodide is a compound that can be either tin(II) iodide or tin(IV) iodide.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4119 tokens (3119 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/t.parquet:  12%|██████▋                                               | 59/477 [05:08<24:53,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5627 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Who is the main character in The Orbital Children?\", \"answer\": \"A\", \"A\": \"Touya Sagami\", \"B\": \"Konoha Nanase\", \"C\": \"Taiyo Tsukuba\", \"D\": \"Mina Misasa\", \"E\": \"Hiroshi Tanegashima\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5627 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/t.parquet:  19%|██████████▌                                           | 93/477 [07:54<29:26,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4557 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which statement accurately describes the Tang Shan Hot Springs in China?\", \"answer\": \"B\", \"A\": \"The Tang Shan Hot Springs are located near Nanjing.\", \"B\": \"There are two Tang Shan Hot Springs, one near Beijing and the other near Nanjing.\", \"C\": \"The Tang Shan Hot Springs are only three meters apart.\", \"D\": \"The spring waters of the Tang Shan Hot Springs are not beneficial for treating skin diseases.\", \"E\": \"The Tang Shan Hot Springs are not a popular tourist attraction.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4557 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/t.parquet:  27%|██████████████▏                                      | 128/477 [10:52<28:15,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4167 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"According to the System of Nature, what is the relationship between the mind and the brain?\", \"answer\": \"A\", \"A\": \"The mind is identified with the brain.\", \"B\": \"The mind and the brain are separate entities.\", \"C\": \"The mind is a product of the brain.\", \"D\": \"The mind controls the brain.\", \"E\": \"The brain is a product of the mind.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4167 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/t.parquet:  30%|████████████████                                     | 144/477 [12:10<26:25,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4270 tokens (3270 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the relationship between total boron and alkalinity in a solution?\", \"answer\": \"C\", \"A\": \"Total boron is not a factor in determining alkalinity.\", \"B\": \"Total boron concentration can only be calculated by knowing the salinity.\", \"C\": \"Total boron is an important quantity when determining alkalinity due to borate's contribution to a solution's acid neutralizing capacity.\", \"D\": \"Total boron is a non-conservative element in seawater.\", \"E\": \"Total boron concentration is solely determined by the concentration of boric acid.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4270 tokens (3270 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/t.parquet:  36%|███████████████████▏                                 | 173/477 [14:30<24:12,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4895 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What was Theodor Svedberg's area of research and for what did he receive the Nobel Prize?\", \"answer\": \"C\", \"A\": \"Theodor Svedberg was a Swedish chemist who conducted research on colloids and proteins using the ultracentrifuge.\", \"B\": \"Theodor Svedberg was a Swedish chemist who conducted research on the theories of Brownian motion put forward by Albert Einstein and the Polish geophysicist Marian Smoluchowski.\", \"C\": \"Theodor Svedberg's area of research was colloids and proteins, and he received the Nobel Prize for his work using the ultracentrifuge.\", \"D\": \"Theodor Svedberg's area of research was physical and colloid chemistry, and he received the Nobel Prize for his contributions to science in Sweden.\", \"E\": \"Theodor Svedberg's area of research was physical and colloid chemistry, and he received the Nobel Prize for his work with the ultracentrifuge.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4895 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/t.parquet:  42%|██████████████████████▎                              | 201/477 [16:54<28:54,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4345 tokens (3345 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the length of the Terceira Rift and how does it compare to other slow-spreading ridges?\", \"answer\": \"B\", \"A\": \"The Terceira Rift is 550 km long, which is shorter than other slow-spreading ridges.\", \"B\": \"The Terceira Rift is 550 km long, and represents the world's slowest spreading center, with plate divergence of 2–4 mm/year.\", \"C\": \"The Terceira Rift is 550 km long, and represents the world's fastest spreading center, with plate divergence of 2–4 mm/year.\", \"D\": \"The Terceira Rift is 550 km long, which is longer than other slow-spreading ridges.\", \"E\": \"The Terceira Rift is 550 km long, and represents the world's average spreading center, with plate divergence of 2–4 mm/year.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4345 tokens (3345 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/t.parquet:  48%|█████████████████████████▋                           | 231/477 [19:27<18:03,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4340 tokens (3340 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which bowler holds the record for achieving a ten-wicket haul the most number of times in Test cricket?\", \"answer\": \"C\", \"A\": \"Muttiah Muralitharan\", \"B\": \"Ajaz Patel\", \"C\": \"Muttiah Muralitharan\", \"D\": \"Jim Laker\", \"E\": \"Tim Southee\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4340 tokens (3340 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/t.parquet:  52%|███████████████████████████▊                         | 250/477 [21:03<17:13,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the formalized dimensionless quantity called rotation defined as?\", \"answer\": \"C\", \"A\": \"The angular measure subtended by a complete circle at its center\", \"B\": \"The ratio of a given angle and the full turn\", \"C\": \"The numerical value of the angle in units of turns\", \"D\": \"The number of revolutions of a rotating body about a given axis\", \"E\": \"The measure of rotational displacement\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/t.parquet:  57%|██████████████████████████████▎                      | 273/477 [28:15<15:22,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 8636 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is Tihomir Novakov known for in the field of science?\", \"answer\": \"C\", \"A\": \"Building X-ray tubes and radios in high school\", \"B\": \"Teaching at the University of Belgrade\", \"C\": \"Black carbon, air quality, and climate change research\", \"D\": \"Developing new analytical techniques for measuring black carbon\", \"E\": \"Hosting the first International Conference on Carbonaceous Particles in the Atmosphere\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 8636 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/t.parquet:  58%|██████████████████████████████▊                      | 277/477 [28:40<18:37,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the significance of Thrupe Lane Swallet in the Mendip Hills?\", \"answer\": \"C\", \"A\": \"Thrupe Lane Swallet is a small, single pothole cave system that is dominated by a series of deep and mainly vertical passages.\", \"B\": \"Thrupe Lane Swallet is a 0.5 hectares geological Site of Special Scientific Interest in Somerset.\", \"C\": \"Thrupe Lane Swallet shows a form of cave development not seen elsewhere in the Mendips and contains the tallest vertical shaft in any known cave on the Mendip Hills.\", \"D\": \"Thrupe Lane Swallet was first entered in 1974 following digging by three caving groups.\", \"E\": \"The stream that flows through Thrupe Lane Swallet feeds St Andrew's Wells in the grounds of the Bishop's Palace in Wells.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/t.parquet:  59%|███████████████████████████████                      | 280/477 [28:55<15:26,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4948 tokens (3948 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the name of the large shield volcano on Venus?\", \"answer\": \"A\", \"A\": \"Theia Mons\", \"B\": \"Beta Regio\", \"C\": \"6000 m\", \"D\": \"extinct\", \"E\": \"caldera\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4948 tokens (3948 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/t.parquet:  75%|████████████████████████████████████████             | 360/477 [35:26<08:10,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4115 tokens (3115 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"How many European basketball clubs have achieved the Triple Crown?\", \"answer\": \"C\", \"A\": \"2\", \"B\": \"5\", \"C\": \"12\", \"D\": \"7\", \"E\": \"4\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4115 tokens (3115 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/t.parquet:  77%|█████████████████████████████████████████            | 369/477 [36:14<07:55,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4276 tokens (3276 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Who is the current editor-in-chief of The Journal of Molecular Diagnostics?\", \"answer\": \"E\", \"A\": \"Nelson Fausto\", \"B\": \"Karen L. Kaul\", \"C\": \"Timothy J. O'Leary\", \"D\": \"Barbara Zehnbauer\", \"E\": \"Ronald M. Przygodzki\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4276 tokens (3276 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/t.parquet:  91%|████████████████████████████████████████████████▎    | 435/477 [41:51<03:42,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 7392 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"When did the Tree of Life Web Project begin and what is its purpose?\", \"answer\": \"B\", \"A\": \"The Tree of Life Web Project began in 1995 and its purpose is to provide information about the diversity and phylogeny of life on Earth.\", \"B\": \"The Tree of Life Web Project began in 1995 and its purpose is to provide information about the diversity and phylogeny of life on Earth.\", \"C\": \"The Tree of Life Web Project began in 1995 and its purpose is to provide information about the history of the World Wide Web.\", \"D\": \"The Tree of Life Web Project began in 1995 and its purpose is to provide information about the development of computer programs.\", \"E\": \"The Tree of Life Web Project began in 1995 and its purpose is to provide information about the funding problems faced by the University of Arizona.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 7392 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/t.parquet:  95%|██████████████████████████████████████████████████   | 451/477 [43:08<01:49,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"Which type of liquid crystal phase is determined by temperature and is insensitive to concentration?\", \"answer\": \"C\", \"A\": \"Nematic\", \"B\": \"Smectic\", \"C\": \"Thermotropic\", \"D\": \"Cholesterolic\", \"E\": \"Isotropic\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/t.parquet:  98%|████████████████████████████████████████████████████ | 468/477 [49:32<00:43,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the purpose of transient expression in cells?\", \"answer\": \"C\", \"A\": \"To study the effects of long-term gene expression\", \"B\": \"To perform RNA interference (RNAi)-mediated gene silencing\", \"C\": \"To study the effects of short-term gene expression\", \"D\": \"To generate large-scale recombinant proteins\", \"E\": \"To optimize selected parameters before scale-up in stably transfected cells\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/t.parquet: 100%|█████████████████████████████████████████████████████| 477/477 [50:25<00:00,  6.34s/it]\n",
      "../data/wikipedia_fixed/u.parquet:  10%|█████▍                                                  | 9/93 [00:40<06:41,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4341 tokens (3341 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which of the following statements accurately describes unsuccessful transfer or abortive transfer in bacterial DNA transfer?\", \"answer\": \"C\", \"A\": \"Unsuccessful transfer or abortive transfer occurs when the transferred DNA fails to survive transduction and conjugation.\", \"B\": \"Unsuccessful transfer or abortive transfer occurs when the transferred DNA fails to integrate into the genetic material of the recipient cells.\", \"C\": \"Unsuccessful transfer or abortive transfer occurs when the transferred DNA fails to establish itself as part of the hereditary material of the recipient cells.\", \"D\": \"Unsuccessful transfer or abortive transfer occurs when the transferred DNA fails to form circular molecules.\", \"E\": \"Unsuccessful transfer or abortive transfer occurs when the transferred DNA fails to express genes in the recipient cells.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4341 tokens (3341 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/u.parquet:  27%|██████████████▊                                        | 25/93 [02:11<06:43,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4729 tokens (3729 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"Which statement accurately describes the role of uridine diphosphate galactose (UDP-galactose) in the production of polysaccharides?\", \"answer\": \"C\", \"A\": \"UDP-galactose is the final product in the production of polysaccharides.\", \"B\": \"UDP-galactose is not involved in nucleotide sugars metabolism.\", \"C\": \"UDP-galactose is an intermediate in the production of polysaccharides.\", \"D\": \"UDP-galactose is the substrate for the transferase B4GALT5.\", \"E\": \"UDP-galactose is not related to the deficiency of Galactose 1 Phosphate Uridyltransferase.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4729 tokens (3729 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/u.parquet:  72%|███████████████████████████████████████▌               | 67/93 [05:43<02:03,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What was the subject of Urtė Neniškytė's doctoral thesis?\", \"answer\": \"D\", \"A\": \"Investigation of retrograde transport of heat shock protein Hsp47\", \"B\": \"Application of microRNAs for fluorescent microscopy-based functional analysis\", \"C\": \"Amyloid β and microglial phagocytosis\", \"D\": \"Role of microglial phagocytic function in neuronal loss induced by Alzheimer's disease peptide Aβ\", \"E\": \"Risk genes of late-onset Alzheimer's disease and how they modulate microglial activation\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/u.parquet:  78%|███████████████████████████████████████████▏           | 73/93 [06:11<01:28,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4698 tokens (3698 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the function of an upstream activation sequence (UAS) in transcription activation?\", \"answer\": \"C\", \"A\": \"To serve as a binding site for transactivators\", \"B\": \"To recruit RNA polymerase II to the pre-initiation complex\", \"C\": \"To increase the expression of a neighboring gene\", \"D\": \"To create an ordered array of nucleosomes\", \"E\": \"To regulate the expression of genes encoding phospholipid biosynthetic enzymes\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4698 tokens (3698 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/u.parquet:  80%|███████████████████████████████████████████▊           | 74/93 [06:22<01:57,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What is the function of an upstream activation sequence (UAS) in transcription activation?\", \"answer\": \"C\", \"A\": \"To serve as a binding site for transactivators\", \"B\": \"To recruit RNA polymerase II to the pre-initiation complex\", \"C\": \"To increase the expression of a neighboring gene\", \"D\": \"To create an ordered array of nucleosomes\", \"E\": \"To regulate the expression of genes encoding phospholipid biosynthetic enzymes\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/u.parquet:  94%|███████████████████████████████████████████████████▍   | 87/93 [12:52<00:38,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4989 tokens (3989 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the purpose of uncomputation in reversible circuits?\", \"answer\": \"C\", \"A\": \"To measure the intermediate effects of ancilla bits\", \"B\": \"To discard garbage registers during computation\", \"C\": \"To clean up temporary effects on ancilla bits for re-use\", \"D\": \"To collapse the wave-function before the program terminates\", \"E\": \"To apply further operations to garbage registers\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4989 tokens (3989 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/u.parquet: 100%|███████████████████████████████████████████████████████| 93/93 [13:39<00:00,  8.81s/it]\n",
      "../data/wikipedia_fixed/v.parquet:  11%|█████▊                                                | 16/149 [01:16<12:47,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which of the following statements accurately describes volume-weighted average price (VWAP)?\", \"answer\": \"C\", \"A\": \"VWAP is the total value of a security or financial asset traded during a trading session.\", \"B\": \"VWAP is a measure of the highest trading price for a given time period.\", \"C\": \"VWAP is the ratio of the value of a security or financial asset traded to the total volume of transactions during a trading session.\", \"D\": \"VWAP is a trading benchmark used by active investors to maximize their execution.\", \"E\": \"VWAP is a calculation used exclusively in algorithmic trading.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/v.parquet:  18%|█████████▊                                            | 27/149 [02:21<10:42,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4393 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Why is the modern constellation Volans not included in the Three Enclosures and Twenty-Eight Mansions system of traditional Chinese uranography?\", \"answer\": \"B\", \"A\": \"Because its stars are too dim to be seen by observers in China\", \"B\": \"Because its stars are too far south for observers in China to know about them\", \"C\": \"Because it was discovered after the introduction of Western star charts\", \"D\": \"Because it is not considered a legitimate constellation in Chinese uranography\", \"E\": \"Because it is classified as one of the 23 Southern Asterisms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4393 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/v.parquet:  24%|█████████████                                         | 36/149 [03:09<08:26,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4228 tokens (3228 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the goal of the Global Virome Project (GVP)?\", \"answer\": \"D\", \"A\": \"To identify and prevent future virus outbreaks\", \"B\": \"To sequence the whole genome of all known viral species\", \"C\": \"To develop low-cost sequencing methods for high-risk countries\", \"D\": \"To collect and sequence unknown viruses and prevent disease outbreaks\", \"E\": \"To study the effects of viruses on ecosystems and the microbiome\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4228 tokens (3228 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/v.parquet:  54%|█████████████████████████████▎                        | 81/149 [06:58<05:10,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 5094 tokens (4094 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is a vector in mathematics and physics?\", \"answer\": \"D\", \"A\": \"A scalar quantity that represents a single number.\", \"B\": \"A tuple of numbers of a fixed length.\", \"C\": \"A geometric object with magnitude and direction.\", \"D\": \"A term used to refer to quantities with both magnitude and direction.\", \"E\": \"An element of a vector space that can be added and scaled.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 5094 tokens (4094 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/v.parquet:  55%|█████████████████████████████▋                        | 82/149 [07:08<07:04,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 7856 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is a vector in mathematics and physics?\", \"answer\": \"D\", \"A\": \"A scalar quantity that represents a single number.\", \"B\": \"A tuple of numbers of a fixed length.\", \"C\": \"A geometric object with magnitude and direction.\", \"D\": \"A term used to refer to quantities with both magnitude and direction.\", \"E\": \"An element of a vector space that can be added and scaled.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 7856 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/v.parquet:  56%|██████████████████████████████▍                       | 84/149 [07:23<07:12,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6242 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What is the purpose of vaporized hydrogen peroxide (VHP) in the context of decontamination?\", \"answer\": \"C\", \"A\": \"To eliminate all forms of microbial life in the inanimate environment\", \"B\": \"To produce a low-temperature antimicrobial vapor\", \"C\": \"To decontaminate enclosed and sealed areas\", \"D\": \"To remove exotic animal viruses from equipment and surfaces\", \"E\": \"To sterilize buildings contaminated with anthrax spores\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6242 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/v.parquet:  63%|██████████████████████████████████                    | 94/149 [08:09<03:37,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4426 tokens (3426 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is Valery Leonidovich Pokrovsky best known for?\", \"answer\": \"C\", \"A\": \"His contributions to the theory of optimal linear arrays\", \"B\": \"His research on quantum mechanics and condensed matter theory\", \"C\": \"His pioneering contributions to the modern theory of phase transitions\", \"D\": \"His work on commensurate-incommensurate phase transitions in two-dimensional systems\", \"E\": \"His analysis of transformations between commensurate and incommensurate superstructures\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4426 tokens (3426 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/v.parquet:  67%|███████████████████████████████████▌                 | 100/149 [08:58<06:25,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4656 tokens (3656 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the purpose of suturing the edges of the surgical site in the vaginal cuff after a hysterectomy?\", \"answer\": \"C\", \"A\": \"To prevent infection and hematoma in the vaginal cuff\", \"B\": \"To reduce the risk of pelvic organ prolapse\", \"C\": \"To prevent the vaginal cuff from partially or completely dehiscing\", \"D\": \"To promote wound healing in the peritoneum\", \"E\": \"To prevent evisceration of the intestines into the vagina\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4656 tokens (3656 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/v.parquet:  81%|██████████████████████████████████████████▋          | 120/149 [10:45<02:10,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6331 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What does the list of the Venezuela national football team's competitive records and statistics show?\", \"answer\": \"C\", \"A\": \"The current active players in the Venezuela national football team.\", \"B\": \"The number of championships won by the Venezuela national football team.\", \"C\": \"The all-time international record of the Venezuela national football team against opposing nations.\", \"D\": \"The matches played by the Venezuela national football team against the Netherlands Antilles.\", \"E\": \"The historical results of the Venezuela national football team in international competitions.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6331 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/v.parquet:  90%|███████████████████████████████████████████████▋     | 134/149 [12:06<01:28,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5776 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the compound viniferin?\", \"answer\": \"A\", \"A\": \"Viniferin refers to a group of chemical compounds, including alpha-Viniferin, beta-Viniferin, delta-Viniferin, epsilon-Viniferin, gamma-Viniferin, R-Viniferin, and R2-Viniferin.\", \"B\": \"Viniferin is a resveratrol trimer that is highly polymerized and is also known as gamma-Viniferin.\", \"C\": \"Viniferin is a resveratrol cyclic tetramer that is commonly referred to as beta-Viniferin.\", \"D\": \"Viniferin is a resveratrol dehydrodimer and is also known as delta-Viniferin.\", \"E\": \"Viniferin is a resveratrol dimer and is commonly referred to as epsilon-Viniferin.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5776 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/v.parquet:  93%|█████████████████████████████████████████████████▍   | 139/149 [12:29<00:41,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 10221 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"Who is the Vinogradov Fracture Zone named after?\", \"answer\": \"A\", \"A\": \"Alexandr Vinogradov\", \"B\": \"Galina Agapova\", \"C\": \"Dr. Galina Agapova\", \"D\": \"Russian Academy of Sciences\", \"E\": \"Vernadsky Institute of Geochemistry\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 10221 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/v.parquet: 100%|█████████████████████████████████████████████████████| 149/149 [13:35<00:00,  5.47s/it]\n",
      "../data/wikipedia_fixed/w.parquet:  10%|█████▍                                                | 20/199 [01:45<16:32,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 1 column 3382 (char 3381)\n",
      "{\"prompt\": \"What is William Shaw's educational background and professional experience?\", \"answer\": \"C\", \"A\": \"William Shaw studied mathematics at King's College, Cambridge and earned a B.A. in 1980. He received a D.Phil. (PhD) in mathematical physics from Wolfson College, Oxford in 1984. From 1984 to 1987, he was a research fellow at Clare College, Cambridge and C.L.E. Moore Instructor at the Massachusetts Institute of Technology. From 1987 to 1990, he worked for Smith Associates in Guildford, and ECL in Henley-on Thames. From 1991 to 2002, he was a lecturer in mathematics at Balliol College, Oxford. In 2002, he moved to St Catherine's College, Oxford, where he was University Lecturer in financial mathematics. In 2006, he moved to a Professorship at King's College London and in 2011 to a Professorship at UCL. He returned to the financial industry in 2012 and remained a visiting professor at UCL until 2017.\", \"B\": \"William Shaw studied mathematics at King's College, Cambridge and earned a B.A. in 1980. He received a D.Phil. (PhD) in mathematical physics from Wolfson College, Oxford in 1984. From 1984 to 1987, he was a research fellow at Clare College, Cambridge and C.L.E. Moore Instructor at the Massachusetts Institute of Technology. From 1987 to 1990, he worked for Smith Associates in Guildford, and ECL in Henley-on Thames. From 1991 to 2002, he was a lecturer in mathematics at Balliol College, Oxford. In 2002, he moved to St Catherine's College, Oxford, where he was University Lecturer in financial mathematics. In 2006, he moved to a Professorship at King's College London and in 2011 to a Professorship at UCL. He returned to the financial industry in 2012 and remained a visiting professor at UCL until 2017.\", \"C\": \"William Shaw studied mathematics at King's College, Cambridge and earned a B.A. in 1980. He received a D.Phil. (PhD) in mathematical physics from Wolfson College, Oxford in 1984. From 1984 to 1987, he was a research fellow at Clare College, Cambridge and C.L.E. Moore Instructor at the Massachusetts Institute of Technology. From 1987 to 1990, he worked for Smith Associates in Guildford, and ECL in Henley-on Thames. From 1991 to 2002, he was a lecturer in mathematics at Balliol College, Oxford. In 2002, he moved to St Catherine's College, Oxford, where he was University Lecturer in financial mathematics. In 2006, he moved to a Professorship at King's College London and in 2011 to a Professorship at UCL. He returned to the financial industry in 2012 and remained a visiting professor at UCL until 2017.\", \"D\": \"William Shaw studied mathematics at King's College, Cambridge and earned a B.A. in 1980. He received a D.Phil. (PhD) in mathematical physics from Wolfson College, Oxford in 1984. From 1984 to 1987, he was a research fellow at Clare College, Cambridge and C.L.E. Moore Instructor at the Massachusetts Institute of Technology. From 1987 to 1990, he worked for Smith Associates in Guildford, and ECL in Henley-on Thames. From 1991 to 2002, he was a lecturer in mathematics at Balliol College, Oxford. In 2002, he moved to St Catherine's College, Oxford, where he was University Lecturer in financial mathematics. In 2006, he moved to a Professorship at King's College London and in 2011 to a Professorship at UCL. He returned to the financial industry in 2012 and remained a visiting professor at UCL until 2017.\", \"E\": \"William Shaw studied mathematics at King's College, Cambridge and earned a B.A. in 1980. He received a D.Phil. (PhD) in mathematical physics from Wolfson College, Oxford in 1984. From 1984 to 1987, he was a research fellow at Clare College, Cambridge and C.L.E. Moore Instructor at the Massachusetts Institute of Technology. From 1987 to 1990, he worked for Smith Associates in Guildford, and ECL in Henley-on Thames. From 1991 to 2002, he was a lecturer in mathematics at Balliol College, Oxford. In 2002, he moved to St Catherine's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 47, in <module>\n",
      "    texts_json = json.loads(text)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 3382 (char 3381)\n",
      "../data/wikipedia_fixed/w.parquet:  38%|████████████████████▎                                 | 75/199 [06:47<10:40,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 5003 tokens (4003 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the main cause of the dispersion of whistlers?\", \"answer\": \"C\", \"A\": \"Lightning strikes in the ionosphere\", \"B\": \"Interactions with free electrons in the magnetosphere\", \"C\": \"Slower velocity of lower frequencies through the plasma environments of the ionosphere and magnetosphere\", \"D\": \"Modulated heating of the lower ionosphere with an HF heater array\", \"E\": \"Transmitting high power HF waves with a VLF modulated power envelope into the D-region ionosphere\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 5003 tokens (4003 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/w.parquet:  50%|██████████████████████████▊                           | 99/199 [08:39<07:13,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4721 tokens (3721 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What was William Fogg Osgood's occupation at Harvard after he graduated?\", \"answer\": \"B\", \"A\": \"Instructor\", \"B\": \"Professor of mathematics\", \"C\": \"Assistant professor\", \"D\": \"Chairman of the department of mathematics\", \"E\": \"Visiting professor of mathematics\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4721 tokens (3721 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/w.parquet:  54%|████████████████████████████▍                        | 107/199 [09:23<08:26,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4236 tokens (3236 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the Woods-Saxon potential and how is it used in the nuclear shell model?\", \"answer\": \"C\", \"A\": \"The Woods-Saxon potential is a mean field potential for the nucleons inside the atomic nucleus, which describes the forces applied on each nucleon in the nuclear shell model.\", \"B\": \"The Woods-Saxon potential is a potential well that attracts nucleons towards the center of the nucleus and rapidly approaches zero as the distance from the center increases.\", \"C\": \"The Woods-Saxon potential is a mean field potential for the nucleons inside the atomic nucleus, which is used to describe approximately the forces applied on each nucleon in the nuclear shell model for the structure of the nucleus.\", \"D\": \"The Woods-Saxon potential is a potential well that is monotonically increasing with distance and approximately flat in the center for large atomic numbers.\", \"E\": \"The Woods-Saxon potential is a mean field potential for the nucleons inside the atomic nucleus, which is solved analytically using the Schrödinger equation and the hypergeometric differential equation.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4236 tokens (3236 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/w.parquet:  68%|████████████████████████████████████▏                | 136/199 [11:50<04:05,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which university did Wayne Snyder earn his Ph.D. from?\", \"answer\": \"D\", \"A\": \"Yardley University\", \"B\": \"Berklee School of Music\", \"C\": \"Tufts University\", \"D\": \"University of Pennsylvania\", \"E\": \"Boston University\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/w.parquet:  73%|██████████████████████████████████████▌              | 145/199 [12:32<03:32,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"What was Willard Sterling Boyle's contribution to the field of physics?\", \"answer\": \"C\", \"A\": \"He invented the first continuously operating ruby laser.\", \"B\": \"He helped select lunar landing sites for the Apollo space program.\", \"C\": \"He co-invented the charge-coupled device (CCD) sensor.\", \"D\": \"He developed integrated circuits.\", \"E\": \"He invented the technology that powers digital cameras today.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/w.parquet:  80%|██████████████████████████████████████████▌          | 160/199 [18:52<02:48,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What is the subject of the documentary film 'Wild Ocean'?\", \"answer\": \"C\", \"A\": \"The migration of billions of sardines\", \"B\": \"The impact of human activities on the ocean\", \"C\": \"The annual sardine run up South Africa's KwaZulu-Natal coast\", \"D\": \"The biodiversity of South Africa's coast\", \"E\": \"The 3D IMAX technology used in the film\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/w.parquet:  81%|███████████████████████████████████████████▏         | 162/199 [19:09<03:46,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4286 tokens (3286 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"What is the overall goal of normalization in Western blot data?\", \"answer\": \"C\", \"A\": \"To compare the relative abundance of a specific protein across different tissues or developmental stages\", \"B\": \"To measure the signal intensity of the loading internal control detected on the same gel or blot\", \"C\": \"To minimize effects arising from variations in experimental errors that can compromise the conclusions from Western blot data\", \"D\": \"To define the upper and lower limits of quantitation and characterize the linear relationship between signal intensity and sample mass volume for each antigen\", \"E\": \"To ensure reproducibility and accuracy of the results by validating normalization controls for each experiment\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4286 tokens (3286 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/w.parquet:  92%|████████████████████████████████████████████████▋    | 183/199 [20:50<00:55,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, you requested 4116 tokens (3116 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "{\"prompt\": \"When was the Worldwide Center of Mathematics founded?\", \"answer\": \"B\", \"A\": \"In 2010\", \"B\": \"In 2008\", \"C\": \"In 2019\", \"D\": \"In 2006\", \"E\": \"In 2015\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, you requested 4116 tokens (3116 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "../data/wikipedia_fixed/w.parquet: 100%|█████████████████████████████████████████████████████| 199/199 [22:03<00:00,  6.65s/it]\n",
      "../data/wikipedia_fixed/x.parquet: 100%|███████████████████████████████████████████████████████| 20/20 [01:34<00:00,  4.75s/it]\n",
      "../data/wikipedia_fixed/y.parquet:  33%|██████████████████▎                                    | 15/45 [01:05<01:49,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5541 tokens. Please reduce the length of the messages.\n",
      "{\"prompt\": \"What was Yuri Mikhailovich Baturin's role in his second spaceflight mission?\", \"answer\": \"C\", \"A\": \"Research Cosmonaut\", \"B\": \"Flight Engineer\", \"C\": \"Assistant to the President on National Security and Secretary of the Defense Council\", \"D\": \"Paying Space Tourist\", \"E\": \"None of the above\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 5541 tokens. Please reduce the length of the messages.\n",
      "../data/wikipedia_fixed/y.parquet:  38%|████████████████████▊                                  | 17/45 [01:19<02:24,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"What was Yuri Anatolievich Ovchinnikov's political career and its relationship to his scientific achievements?\", \"answer\": \"C\", \"A\": \"He had no political career and solely focused on his scientific achievements.\", \"B\": \"His political career hindered his scientific achievements.\", \"C\": \"His political career played a significant role in his scientific achievements.\", \"D\": \"His political career had no impact on his scientific achievements.\", \"E\": \"His political career was unrelated to his scientific achievements.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/y.parquet:  91%|██████████████████████████████████████████████████     | 41/45 [03:37<00:27,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server is overloaded or not ready yet.\n",
      "{\"prompt\": \"Which of the following statements accurately describes the compounds Ytterbium(II) iodide and Ytterbium(III) iodide?\", \"answer\": \"C\", \"A\": \"Ytterbium(II) iodide is also known as ytterbium diiodide, while Ytterbium(III) iodide is also known as ytterbium triiodide.\", \"B\": \"Ytterbium(II) iodide is a compound with the chemical formula YbI3, while Ytterbium(III) iodide is a compound with the chemical formula YbI2.\", \"C\": \"Ytterbium(II) iodide is also known as ytterbium diiodide, while Ytterbium(III) iodide is also known as ytterbium triiodide.\", \"D\": \"Ytterbium(II) iodide and Ytterbium(III) iodide are both compounds with the chemical formula YbI2.\", \"E\": \"Ytterbium(II) iodide and Ytterbium(III) iodide are both compounds with the chemical formula YbI3.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 745, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "../data/wikipedia_fixed/y.parquet: 100%|███████████████████████████████████████████████████████| 45/45 [03:59<00:00,  5.32s/it]\n",
      "../data/wikipedia_fixed/z.parquet:  20%|██████████▉                                             | 8/41 [00:31<02:02,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "{\"prompt\": \"Which model is a helix-coil transition model that describes helix-coil transitions of macromolecules?\", \"answer\": \"A\", \"A\": \"Zimm–Bragg model\", \"B\": \"Alpha helix\", \"C\": \"Lifson–Roig model\", \"D\": \"Random coil\", \"E\": \"Statistical mechanics\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 287, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18466/658324387.py\", line 46, in <module>\n",
      "    text = query_prompt(prompt)\n",
      "  File \"/tmp/ipykernel_18466/2719080132.py\", line 2, in query_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 609, in request_raw\n",
      "    raise error.APIConnectionError(\n",
      "openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "../data/wikipedia_fixed/z.parquet: 100%|███████████████████████████████████████████████████████| 41/41 [08:56<00:00, 13.09s/it]\n"
     ]
    }
   ],
   "source": [
    "import traceback \n",
    "batch_size = 1\n",
    "\n",
    "def make_prompt(series):\n",
    "    prompt = f\"\"\"\n",
    "You are a professor at a science university and are creating a test for your students.\n",
    "Please create one exam based on the provided text, where you choose the most accurately statement from five options for the question.\n",
    "The output should json format below:\n",
    "{{\"prompt\": <the question text>, \"answer\" <answer(one of A through E)>, \"A\": <option A>, \"B\": <option B>, \"C\": <option C>, \"D\": <option D>, \"E\": <option E>}}\n",
    "\n",
    "Context:\n",
    "{series['text']}\n",
    "\n",
    "Attention:\n",
    "- The question should be LONG sentences.\n",
    "\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def f(series):\n",
    "    try:\n",
    "        if series[\"A\"] != series[\"A\"]:\n",
    "            if type(series[\"answer\"]) == dict:\n",
    "                for key in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "                    series[key] = series[\"choices\"][key]\n",
    "            elif type(series[\"answer\"] == list):\n",
    "                for i, key in enumerate([\"A\", \"B\", \"C\", \"D\", \"E\"]):\n",
    "                    series[key] = series[\"choices\"][i]\n",
    "    except:\n",
    "        return series\n",
    "    return series\n",
    "\n",
    "now_date = dt.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "first = True\n",
    "for file in files:\n",
    "    if os.path.basename(file) in [\"all.parquet\"]:\n",
    "        print(f\"pass: {file}\")\n",
    "        continue\n",
    "    df_science = get_df(file)\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(df_science)), desc=file):\n",
    "        try:\n",
    "            series = df_science.iloc[i]\n",
    "            prompt = make_prompt(series)\n",
    "            text = query_prompt(prompt)\n",
    "            texts_json = json.loads(text)\n",
    "            if first:\n",
    "                print(\"--------------- text ------------------: \")\n",
    "                print(series[\"text\"])\n",
    "                print(\"--------------- json ------------------: \")\n",
    "                print(texts_json)\n",
    "                first = False\n",
    "            if type(texts_json) == dict:\n",
    "                texts_json[\"wiki_id\"] = series[\"id\"]\n",
    "                texts_json[\"original_text\"] = series[\"text\"]\n",
    "                texts.append(texts_json)\n",
    "                for col in [\"A\", \"B\", \"C\", \"D\", \"E\", \"answer\", \"prompt\"]:\n",
    "                    if col not in texts_json:\n",
    "                        print(f\"{col} not existed: {texts_json}\")\n",
    "            else:\n",
    "                for text_json in texts_json:\n",
    "                    text_json[\"wiki_id\"] = series[\"id\"]\n",
    "                    text_json[\"original_text\"] = series[\"text\"]\n",
    "                    texts.append(text_json)\n",
    "                    if col not in text_json:\n",
    "                        print(f\"{col} not existed: {texts_json}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            traceback.print_exc()\n",
    "            print(text)\n",
    "            time.sleep(10)\n",
    "        if i % 20 == 0:\n",
    "            df_texts = pd.DataFrame(texts)\n",
    "            df_texts = df_texts.apply(f, axis=1)\n",
    "\n",
    "            df_texts.to_parquet(f\"output_gpt3.5_generate/{now_date}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = pd.DataFrame(texts)\n",
    "df_texts = df_texts.apply(f, axis=1)\n",
    "\n",
    "df_texts.to_parquet(f\"output_gpt3.5_generate/{now_date}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
